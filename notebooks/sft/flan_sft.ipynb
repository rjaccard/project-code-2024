{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from tqdm import tqdm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_metamathqa(examples, prefix=\"Please answer the following question: \"):\n",
    "    \"\"\"Add a prefix to the inputs and tokenize the inputs and targets.\n",
    "\n",
    "    Args:\n",
    "        examples: dataset examples.\n",
    "        tokenizer: tokenizer.\n",
    "        prefix (str, optional): Prefix to add to each inputs. Defaults to \"Please answer the following question: \".\n",
    "\n",
    "    Returns:\n",
    "        dataset: processed examples.\n",
    "    \"\"\"\n",
    "    # The \"inputs\" are the tokenized answer:\n",
    "    inputs = [prefix + doc for doc in examples[\"query\"]]\n",
    "\n",
    "    # The \"labels\" are the tokenized outputs:\n",
    "    labels = examples[\"response\"]\n",
    "    return {\"question\": inputs, \"answer\": labels}\n",
    "\n",
    "def load_metamathqa(dev_mode=False):\n",
    "    \"\"\"Load the metamathqa dataset.\n",
    "\n",
    "    Returns:\n",
    "        Dataset: metamathqa dataset.\n",
    "    \"\"\"\n",
    "    dataset = load_dataset(\"meta-math/MetaMathQA\")\n",
    "    if dev_mode:\n",
    "        dataset[\"train\"] = dataset[\"train\"].select(range(20))  # For development\n",
    "    else:\n",
    "        dataset[\"train\"] = dataset[\"train\"].select(range(50000))  # For fine-tuning\n",
    "    dataset = dataset[\"train\"].train_test_split(test_size=0.2)\n",
    "    tokenized_dataset = dataset.map(\n",
    "        process_metamathqa,\n",
    "        batched=True,\n",
    "        remove_columns=[\"query\", \"response\", \"type\", \"original_question\"],\n",
    "    )\n",
    "    return tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mmlu_stem(examples):\n",
    "    alphabet = list(map(chr, range(65, 91)))\n",
    "    labels = [alphabet[ans] for ans in examples[\"answer\"]]\n",
    "    inputs = [\"Question: \" + question +\"\\n\\nOptions:\\n\" + \"\\n\".join(f\"{index}. {value}\" for index, value in zip(alphabet[:len(choices)], choices)) + \"\\n\\nAnswer:\" for question, choices in zip(examples[\"question\"], examples[\"choices\"])]\n",
    "    data = {\"question\": inputs, \"answer\": labels}\n",
    "    \n",
    "    return data\n",
    "\n",
    "def load_mmlu_stem(dev_mode=False):\n",
    "    \"\"\"Load the mmlu_stem dataset.\n",
    "\n",
    "    Returns:\n",
    "        Dataset: mmlu_stem dataset.\n",
    "    \"\"\"\n",
    "    dataset = load_dataset(\"TIGER-Lab/MMLU-STEM\")\n",
    "    dataset['train'] = dataset['train'].shuffle(seed=42)\n",
    "    if dev_mode:\n",
    "        dataset[\"train\"] = dataset[\"train\"].select(range(1000))  # For development\n",
    "    dataset = dataset[\"train\"].train_test_split(test_size=0.2)\n",
    "    tokenized_dataset = dataset.map(\n",
    "        process_mmlu_stem,\n",
    "        remove_columns=[\"question\", \"subject\", \"choices\", \"answer\"],\n",
    "        batched=True,\n",
    "\n",
    "    )\n",
    "    return tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 800/800 [00:00<00:00, 33915.63 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 19584.91 examples/s]\n"
     ]
    }
   ],
   "source": [
    "mmlu = load_mmlu_stem(dev_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'answer'],\n",
       "        num_rows: 800\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'answer'],\n",
       "        num_rows: 200\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmlu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpt_wrapper\n",
    "from gpt_wrapper.chat import Chat\n",
    "gpt_wrapper.api_base = \"http://mnlp-backend-938795011.eu-central-1.elb.amazonaws.com\"\n",
    "gpt_wrapper.api_key = \"a216b70d-88dc-4400-b6ee-b7e3f33da050\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = Chat.create(\"mmlu-stem\")\n",
    "def generate_gpt_answers(examples):\n",
    "    gpt_answer = []\n",
    "    for question in tqdm(examples[\"question\"]):\n",
    "        try:\n",
    "            answer = chat.ask(question).to_dict()['content']\n",
    "            gpt_answer.append(answer)\n",
    "        except:\n",
    "            gpt_answer.append(\"I don't know.\")\n",
    "    print(chat.budget())\n",
    "    return {\"question\": examples[\"question\"], \"answer\": gpt_answer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'limit': 10000000, 'usage': 2001048}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.budget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [21:38<00:00,  1.62s/it]s]\n",
      "Map: 100%|██████████| 800/800 [21:38<00:00,  1.62s/ examples]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'limit': 10000000, 'usage': 3466833}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server timeout.\n",
      "Retrying in 10 seconds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [05:35<00:00,  1.68s/it]\n",
      "Map: 100%|██████████| 200/200 [05:35<00:00,  1.68s/ examples]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'limit': 10000000, 'usage': 3466833}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 800/800 [00:00<00:00, 177734.16 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 200/200 [00:00<00:00, 59684.16 examples/s]\n"
     ]
    }
   ],
   "source": [
    "mmlu_gpt = mmlu.map(generate_gpt_answers, batched=True)\n",
    "mmlu_gpt.save_to_disk(\"mmlu_gpt.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmlu_gpt_filter = load_from_disk(\"mmlu_gpt.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "627"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter = 0\n",
    "for example in test['train']:\n",
    "    if example['answer'] == \"I don't know.\":\n",
    "        counter += 1\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetDict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'limit': 10000000, 'usage': 3466833}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.budget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(\"../../checkpoints/checkpoints/sft/full_model/\", local_files_only=True)\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"../../checkpoints/checkpoints/sft/full_model/\", local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_t5_answers(examples):\n",
    "    inputs = tokenizer(examples[\"question\"], return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    outputs = model.generate(**inputs)\n",
    "    answers = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return {\"question\": examples[\"question\"], \"answer\": answers}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pythagorean theorem states that the sum of the roots of a polynomial $ax2 + bx + c = 0$ is equal to $-fracba$. In this case, the roots are $a = 1$ and $b = -2$, so the pythagorean theorem states that the sum of the roots of a polynomial $ax2 + bx + c = 0$ is equal to $-fracba$. Therefore, the pythagorean theorem states that the sum of the roots of a polynomial $ax2 + bx + c = 0$ is equal to $-fracba$. So, the pythagorean theorem states that the sum of the roots of a polynomial $ax2 + bx + c = 0$ is equal to $-fracba$. In this case, the roots are $a = 1$ and $b = -2$, so the pythagorean theorem states that the sum of the roots of a polynomial $ax2 + bx + c = 0$ is equal to $-fracba$. In this case, the roots are $a = 1$ and $b = -2$, so the pythagorean theorem states that the sum of the roots of a polynomial $ax2 + bx + c = 0$ is equal to $-fracba$. Therefore, the pythagorean theorem states that the sum of the roots of a polynomial $ax2 + bx + c = 0$ is equal to $-fracba$. Therefore, the pythagorean theorem states that the sum of the roots of a polynomial $ax2 + bx + c =\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.batch_decode(model.generate(**tokenizer(\"Answer the following question: Explain the pythagorean theorem?\",\n",
    "                                                  return_tensors=\"pt\",\n",
    "                                                  padding=True,\n",
    "                                                  truncation=True), max_length=512, num_beams=4, num_return_sequences=1),\n",
    "                       skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
