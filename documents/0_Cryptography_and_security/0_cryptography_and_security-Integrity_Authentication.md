
## Chapter 6 Integrity And Authentication

This chapter mostly discusses on two cryptographic primitives: message authentication codes and hash functions. Some related primitives will be covered as well. Message authentication codes use a symmetric secret key to authenticate messages: the sender appends a tag to the message, computed based on the message and the key, and the verifier checks that the received message and tag are consistent with the key. This shall protect against an adversary trying to *forge* messages without being detected. *Hash functions* map an arbitrary bitstring to a *digest* (or *message hash*)
of fixed length. There are many security properties which may be required on hash functions, depending on the application.

## 6.1 Commitment Scheme

In a *commitment scheme*, there are two participants, the sender and the receiver, running a protocol in two phases: the commitment phase and the opening phase.

The sender wants to commit on a message X without revealing it. Typically, he picks some random r and computes
(*c, k*) = Commit(X; r).

He then reveals c to the receiver.

In the opening phase, the sender reveals k and the receiver can compute Open(*c, k*) = X. The correctness requirement implies that Open(Commit(X; r)) = X for any X and r. As we will see, the security consists of two properties:
hiding and *binding*.

The commitment must be *hiding*: the receiver shall not retrieve any information about X
during the commitment phase (i.e., from c). This is similar to encryption.

Compared to encryption, there is a second security property which is required: the commitment must be *binding*: the sender shall not be able to construct c, k, k′ such that Open(*c, k*) ̸=
Open(*c, k*′).

In many cases, the commitment scheme is based on a single function H: we have Commit(X; r) =
(*c, k*) with c = H(X∥r) and k = (*X, r*) and Open(c, (*X, r*)) = X if c = H(X∥r) and ⊥ otherwise.

As we can see, H behaves like an encryption function to *hide* X, but no decryption algorithm is needed in the scheme. In this construction, the binding property means that we cannot find X, X′, r, and r′ such that H(X∥r) = H(X′∥r′) and X ̸= X′.

This primitive can be used to make two participants flip a coin over a communication channel in a fair way (see Fig. 6.1): Alice would flip a bit b and commit to it to Bob. Then, Bob would flip a bit b′ and send it to Alice. Then, Alice would open her commitment and the outcome of the protocol would be b⊕b′. If Alice is honest, since the commitment is hiding, b′ must be independent from b, so b⊕b′ shall be uniformly distributed. If Bob is honest, since the commitment is binding, the opened b must be independent from b′, so b ⊕ b′ shall be uniformly distributed. This can generalize to rolling a die.

There are many kinds of commitments: those requiring an interactive protocol, and those which are non-interactive (the specification in terms of Commit and Open algorithms as above is essentially non-interactive). As for the security notions, there are several degrees: perfect, statisti-

Alice Bob pick b ∈ {0, 1} pick r, (*c, k*) ← Commit(X; r) c −−−−−−−−−−−−−−→ b′ ←−−−−−−−−−−−−−− pick b′ ∈ {0, 1} k −−−−−−−−−−−−−−→ b ← Open(*c, k*) z ← b ⊕ b′ z ← b ⊕ b′ output: z output: z
cal, and computational. For instance, in a perfectly hiding commitment, X and c are statistically independent. For statistically hiding commitments, the distributions of (c|X = x0) and (c|X = x1)
would be indistinguishable for any x0 and x1, up to some "negligible" probability. For computationally hiding commitments, this would only hold for computationally bounded adversaries. Finally, some commitments may require a global setup such as a common reference string, which is supposed to be set up once for all.

As we will see, the most practical commitment schemes are based on a hash function.

## 6.2 Key Derivation Function And Pseudorandom Generator

Pseudorandom generator.

A *pseudorandom generator (PRNG)* is typically an automaton, initialized with a seed, which updates its state and outputs a number at every generation. Cryptographic pseudorandom generators must be such that the generated sequence of numbers must be indistinguishable from a sequence of random numbers. For instance, the stream ciphers that we have seen, with a secret key playing the role of the seed can be considered as a PRNG, since they generate a pseudorandom key stream.

There are famous failure cases related to PRNG. For instance, we can often see implementations of cryptographic schemes in which the secret keys are generated by a PRNG using a seed of too low entropy, e.g., 16 bits. In such cases, even though the secret key may be as large as 128 bits, we can do an exhaustive search on the seed to recover it. In another failure case, some random number in Zq was generated by reducing an ℓ-bit random number modulo q, where ℓ = ⌈log2 q⌉.

Although it looks reasonable, it introduces an enormous bias in the distribution of the generated numbers, as the ones between 0 and 2ℓ − q − 1 would appear twice more often than others. This led to an attack against the signature algorithm DSA [4]. To defeat it, a better way consisted in reducing modulo q a 2ℓ-bit random number. This way, there are 22ℓ mod q numbers appearing with a slightly larger probability, but the gap is of 2−2ℓ instead of 2−ℓ.

Pseudorandom function.

Quite often, we may need a function fK set up with a key K. For instance, a block cipher is a permutation over a block space which is set up by a key. Sometimes, we just need fK to produce values looking like random. We say that f is a pseudorandom function
(PRF) if by playing with a black-box function, we cannot say whether the function in the black box is just fK set up with a random K or a truly random function.

Key derivation function.

A *key derivation function (KDF)* typically maps some random value with imperfect distribution into a symmetric key which has a distribution close to uniform.

## 6.3 Cryptographic Hash Function

A hash function maps a bitstring of arbitrary length to a bitstring of fixed length (e.g., 160 bits for the SHA1 hash function). There are three main uses of hash functions: domain expansion, commitment, and pseudorandom generation.

Domain expanders are used in digital signatures: we design signature algorithms to be able to sign bitstrings of fixed length but we also want to sign arbitrary bitstrings. So, we first hash then sign. But for that to be secure, it must be *impossible in practice* to exhibit two messages producing the same value after hashing. This is called a *collision*. So, we often require hash functions to be collision-resistant: it must be impossible to find x and y such that H(x) = H(y). For this reason, H(x) is often called the *digest*, or *fingerprint*, or *hash* of x.

We can define Commit(X; r) = (H(X∥r), X∥r) and Open(*c, X*∥r) = X if H(X∥r) = c and fail otherwise, to obtain a commitment scheme. So, H(X∥r) must hide X and also bind to X.

In access control, we have also seen that H(password) can be used to verify a password without disclosing it. So, we often require that given h, it is infeasible to find x such that H(x) = h. We say that H is *preimage-resistant*.

We can define a PRNG by H(seed∥counter). We can thus define a KDF function by

$$s\mapsto\mathsf{true}_{\ell}(H(s\|1)\|H(s\|2)\|\cdots)$$

to obtain an $\ell$-bit key from a seed $s$.

The notion of preimage-resistance in hash functions has two variants: in the _first preimage resistance_, we require that an adversary who is given a digest $h$ cannot find $x$ such that $H(x)=h$. In the _second preimage resistance_, we require that an adversary who is given $y$ cannot find $x\neq y$ such that $H(x)=H(y)$ (so, a second preimage makes a collision). Clearly, a buteforce attack can do a preimage take $($a$ that per image attack or a second preimage attack) with complexity $\mathcal{O}(N)$, where $N$ is the target object. Each selection produces a random object, independent from the others. Assuming a uniform distribution, the probability to match the target is $\frac{1}{N}$. So, the expected number of iterations is

$$\sum_{i=1}^{+\infty}i\left(1-\frac{1}{N}\right)^{i-1}\frac{1}{N}=N$$
The MD hash functions and follow ups.

In 1990, Ronald Rivest proposed a hash function called MD4 [69]. ("MD" stands for "Message Digest".) It was quickly replaced by MD5, in 1991, which became a famous standard (RFC 1321 [70] in 1992). Both produce digests of 128 bits.

A variant of MD4 and MD5 was proposed as a US standard in 1993 [1].

It was called SHA
(Secure Hash Algorithm) but is now called SHA0 because it is became obsolete quickly after. It was replaced by SHA1 in 1995 [2]. SHA0 and SHA1 produce digests of 160 bits. Some new standard algorithms were proposed in 2002: the SHA2 family, which includes SHA256, SHA384, and SHA512 [3]. (These names indicate the bitlength of the produced digest.) The SHA3 standard appeared in 2015 [36]. So far, MD4, MD5, and SHA0 are badly broken [29, 33, 84, 86, 88]. There was a theoretical attack on SHA1 in 2005 [85]. Since then, a collision on SHA1 has been found [81]. Consequently, SHA1 is not recommended any more.

The general design of these hash functions consists of making a *compression function* from a kind of block cipher, then a hash function by iterating the compression on message blocks. To hash a message, the message is first padded, then split into blocks. Then, compression starts with an initial value and the first block, producing a chaining value. Each block is processed by compressing the previous chaining value and the block. The final chaining value is the digest.

Following the *Merkle-Damg˚ard extension* [31, 59], the message X is transformed into a sequence of blocks Xi by X1∥ *. . .* ∥Xn = X∥pad(X)
where pad(X) consists of a bit 1 followed by a variable number of 0 bits and the encoding of the message length, so that the length of X∥pad(X) is multiple of the block length. It is also assumed that the padding is not larger than a block. Then, using a compression function C, we define H0 = IV, the initial value, and Hi = C(Hi−1, Xi), i = 1*, . . . , n*. Finally, Hn is the message digest.

This construction comes with the following theorem:
Theorem 6.1. If C is collision-resistant, then the constructed hash function is collision-resistant.

To construct a compression function from a block cipher, we use the _Davies-Meyer construction_[91]

$$C(H,X)=\mathsf{Enc}_{X}(H)+H$$
where + is a group operation, H is the chaining value, and X is the message block.

The SHA1 compression function [2] is similar:
Input: an initial hash *a, b, c, d, e*, a message block x0*, . . . , x*15
Output: a hash a, b, c, d, e
1: ainitial ← a 2: binitial ← b

3: cinitial ← c
4: dinitial ← d
5: for i = 16 to 79 do
6:
        xi ← ROTL1 (xi−3 XOR xi−8 XOR xi−14
                                                     XOR xi−16)

7: end for
8: for i = 1 to 4 do

9:
        for j = 0 to 19 do

10:
            t ← ROTL5(a) + fi(b, c, d) + e + x20(i−1)+j + ki

11:
            e ← d 12:
            d ← c

13:
            c ← ROTL30(b)

14:
            b ← a 15:
            a ← t

16:
        end for

17: end for
18: a ← a + ainitial
19: b ← b + binitial
20: c ← c + cinitial
21: d ← d + dinitial
22: e ← e + einitial
Here, ki is a constant and the fi functions are bitwise Boolean functions defined by

f1(b, c, d)
          =
             if b then c else d

f2(b, c, d)
          =
             b XOR c XOR d

f3(b, c, d)
          =
             majority(b, c, d) f4(b, c, d)
          =
             b XOR c XOR d

Note that

if $x$ then $y$ else $z$ = ($x$ AND $y$) OR ((NOT $x$) AND $z$)

$$=(x$$ AND $y$) XOR ((NOT $x$) AND $z$)

$$=(x$$ AND $y$) OR ($y$ AND $z$) OR ($z$ AND $x$)

$$=(x$$ AND $y$) XOR ($y$ AND $z$) XOR ($z$ AND $x$)
SHA3.

The SHA3 standard is no longer based on the MD family. It comes from the Keccak algorithm which is based on a *sponge construction* [19]. The standard appeared in 2015 [36]. Keccak was designed in Belgium by Bertoni, Daemen, Peeters, and Van Assche from STMicroelectronics and NXP Semiconductors.

Keccak is quite flexible as it includes several tunable parameters r, c, and d. However, SHA3
only kept four vectors of parameters, defining this way the four functions in the following table:

| algo     | r     | c     |   d |
|----------|-------|-------|-----|
| SHA3-224 | 1 152 | 448   | 224 |
| SHA3-256 | 1 088 | 512   | 256 |
| SHA3-384 | 832   | 768   | 384 |
| SHA3-512 | 576   | 1 024 | 512 |

The parameter d (giving the name of the function) is the bitlength of the output. The functions are based on a construction using a *state* of b = r + c bits. This construction is called *sponge*. It consists of padding the input message, then processing it by chunks of r bits. The sponge processes each chunk of message through an *absorbing* phase. At each step, one chunk is XORed into the state and the state is updated using an invertible function f. Then, the sponge produce outputs through a *squeezing* phase. Each step produces r bits and it stops as soon as we reach d bits of output. This is illustrated on Fig. 6.2.

The function f (which is formally called Keccak-f[b] in Keccak) operates on a state of b bits.

This state is represented as a 3-dimensional array of b = 5×5×2ℓ bits. The function is an iteration nr = 12 + 2ℓ times of a round R defined by

$$R=\iota\circ\chi\circ\pi\circ\rho\circ\theta$$
All these functions are fixed and defined in the design. The operation θ is linear. The operations
ρ and π only permute the bits in the state. The operation ι add constants (depending on the round index) modulo 2 to a few bits of the state. The operation χ is the only non-linear one. It is actually an invertible quadratic operation.

## 6.4 Message Authentication Codes

As already mentioned, a *message authentication code (MAC)* typically appends a *tag* to a message.

This tag is computed based on a secret key and the message. The message is authenticated if it comes with a correct tag, based on the secret key. So, the primitive shall avoid that an adversary forges a message/tag pair not issued by the legitimate sender but still passing the authentication. Attacks scenario includes *key recovery* or just *forgery*. They can be run in a *known message attack*, in which the adversary collects messages which are randomly authenticated by the legitimate sender, or a *chosen message attack*, in which the adversary can select messages of his choice to be authenticated. (But, in the case of forgery attack, the forgery is valid if it was not authenticated by the legitimate sender.)
HMAC.

One of the most popular MACs is the HMAC algorithm, which is a standard (RFC 2104 [52]).

It is notably used in TLS and SSH.1 It is based on a Merkle-Damg˚ard hash function H. Roughly speaking, the tag of the message X with key K is computed by

$\mathrm{HMAC}_{K}(X)=\mathrm{trunc}(H((K\oplus\mathrm{opad})||H((K\oplus\mathrm{ipad})||X)))$
1See Section A.8.

where opad and ipad are constants defined by the standard. (See Fig. 6.3.) If the key K is too long (longer than a message block), it is first replaced by H(K). Then and in any case, the key is concatenated with enough zero bytes so that the final length matches the one of a block.

In 2006, Bellare [14] proved that if the compression function defines a pseudorandom function
(PRF) (in a sense to be defined in this chapter), then HMAC is also a PRF, which implies a MAC
which is secure against existential forgeries. In the same year, Kim et al. [49] proved that HMAC
based on several known hash functions (such as MD4 and SHA0) is not a PRF. In 2009, Wang et al. [87] have shown an attack on the PRF property of HMAC based on MD5 (with a quite high complexity though). CBCMAC.

Another popular construction (to be used with care) is based on a block cipher.

The tag of a message (assumed to be a sequence of blocks) is the last ciphertext block of the CBC
encryption of the message. This algorithm can be secure in some applications in two cases:

- the application makes sure that all messages have exactly the same length; - the tag is only available to the adversary in some encrypted form.
Otherwise, CBCMAC alone is insecure as we can easily make forgeries. To see this, we can observe that if c is the CBCMAC of a message X1, then the CBCMAC of X1∥B, the concatenation of X1
with a new block B, is the encryption CK(c ⊕ B). Hence, if c′ is the CBCMAC of a message X2, with the knowledge of X1, X2, B, c, c′, CK(c ⊕ B), we can forge the CBCMAC of a new message X2∥B′ for B′ = B ⊕ (c ⊕ c′) as it is CK(c′ ⊕ B′) = CK(c ⊕ B) (see Fig. 6.4).

There are several variants, including CMAC, which is the RFC 4493 standard [79].2 RFC 4493
uses AES-128. So, we assume that the block length is of 128 bits and that the key has 128 bits. In CMAC, the message is split into blocks. The last block may be incomplete, in which case we pad it with the bitstring 10 *· · ·* 0 to have a complete block. We obtain a sequence of blocks x1*, . . . , x*n. We set y1 = CK(x1) where CK is AES using the key K of CMAC. Then, we set yi = CK(xi⊕yi−1)
for i = 2*, . . . , n* − 1. Finally, yn = CK(xn ⊕ yn−1 ⊕ kb), where b *∈ {*1, 2} and k1 and k2 are two values defined below. If xn was not padded, we use b = 1. Otherwise, we use b = 2. We define L = CK(0), the encryption of the all-0 block. Now, we consider 128-bit blocks as elements of

$\mathsf{GF}(2^{128})$. For instance, $L=\sum_{i=1}^{128}L_{i}X^{128-i}$, where $L_{1}$ is the most significant bit of $L$ and $L_{128}$ is its least significant bit and $X$ is a free variable. We set $k_{1}=X\times L$ mod $P(X)$ where

$$P(X)=X^{128}+X^{7}+X^{2}+X+1$$
is the irreducible polynomial used to define the representation of GF(2128). We further set k2 =
X × k1 mod P(X). This fully defines k1 and k2. The result of CMAC is yn (or a truncation of it if we need a shorter tag than 128 bits). In 2003, Iwata and Kurosawa [45] (the original designers of CMAC) proved that if C is a pseudorandom permutation, then CMAC is secure. PMAC.

Another block-cipher based construction, which is not based on CBCMAC, was proposed by Black and Rogaway [20]. It is also proven secure if the block cipher is a pseudorandom permutation. It works by first computing the subkey L which is the encryption of the zero-block:
L = CK(0). Then, the message is cut into blocks x1*, . . . , x*n (the last one could be incomplete but non-empty). We compute Σ = Ln i=i CK(xi ⊕ (2i · L)), where · is the GF multiplication and
2 is a shorthand for the variable of the polynomial defining the GF structure.3 If the last block is complete, Σ is replaced by Σ ⊕ (2−1 · L). The MAC is then the first t bits of CK(Σ). The construction is depicted on Fig. 6.5.

WC-MAC.

There is an analog to the Vernam cipher [83] for authentication which provides unconditional security. The construction was proposed by Wegman and Carter in 1981 [28, 89].

To authenticate a message X, we essentially encrypt a value hK(X) using the Vernam cipher, where h is an ε-XOR-universal hash function.4
Definition 6.2. A family (hK)K *of functions is called a* ε-XOR-universal hash function if for any a, x, y, with x ̸= y, we have

$$\operatorname*{Pr}[h_{K}(x)\oplus h_{K}(y)=a]\leq\varepsilon$$
over the uniform selection of the random key K. The construction WC-MAC is validated by the following theorem:
Theorem 6.3 (Krawczyk 1994 [51]). If h is a ε-XOR-universal hash function, any chosen message attack against WC-MAC has a success probability bounded by ε. Proof. We consider an adversary making a chosen message forgery attack. Let d be the number of messages that the adversary made. We denote by x1*, . . . , x*d the messages queries by the adversary and by c1*, . . . , c*d the corresponding authentication tags. We have ci = hK(xi)⊕Ki. Let (*x, j, c*) be the output of the adversary. It is a forgery if x is different from all xi and if we have c = hK(x)⊕Kj.

In the first case, we have j ̸∈ {1*, . . . , d*}. So, Kj is uniform and independent from the entire experiment. Hence, c = hK(x) ⊕ Kj occurs with probability exactly 2−m, where m is the output bitlength of the hash function. We note that the sum over all a of Pr[hK(x) ⊕ hK(y) = a] is equal to 1. Since the function is ε-XOR-universal, we deduce that 1 ≤ 2mε. So, this first case succeeds with probability bounded by ε.

In the second case, we have 1 ≤ j ≤ d. The *view* of the adversary is characterized by the event that ci = hK(xi) ⊕ Ki happens for all i = 1*, . . . , d*. We let V denote this event. We let V ′ denote the same event for all i except i = j. We have

$$V=V^{\prime}\cup[c_{j}=h_{K}(x_{j})\oplus K_{j}]$$
c1*,...,c*d Pr[c = hK(x) ⊕ Kj|V ] Pr[V ]. For each (c1*, . . . , c*d), we have

We assume without loss of generality that the adversary is deterministic. So, $x_{k}$ is a function of $c_{1},\ldots,c_{i-1}$ and $(x,j,c)$ is a function of $c_{1},\ldots,c_{i}$. The probability of success in this case is $p=\sum_{c_{1},\ldots,c_{k}}\Pr[c=h_{K}(x)\oplus K_{j}|V]\Pr[V]$. For each $(c_{1},\ldots,c_{i})$, we have

$$\Pr[c=h_{K}(x)\oplus K_{j}|V]=\Pr[c=h_{K}(x)\oplus K_{j}|c_{j}=h_{K}(x_{j})\oplus K_{j}]$$ $$=\Pr[h_{K}(x)\oplus h_{K}(x_{j})=c\oplus c_{j}|c_{j}=h_{K}(x_{j})\oplus K_{j}]$$
due to the independence between (K1, . . . , Kj−1, Kj+1*, . . . , K*d) and (*K, K*j). We use the Bayes formula to obtain

$\Pr[c=h_{K}(x)\oplus K_{j}|V]$

$=\frac{\Pr[K_{j}=h_{K}(x_{j})\oplus c_{j}|h_{K}(x)\oplus h_{K}(x_{j})=c\oplus c_{j}]}{\Pr[K_{j}=h_{K}(x_{j})\oplus c_{j}]}\times\Pr[h_{K}(x)\oplus h_{K}(x_{j})=c\oplus c_{j}]$
and, thanks to the uniform distribution of Kj and its independence to K, we deduce

$\Pr[c=h_{K}(x)\oplus K_{j}|V]=\Pr[h_{K}(x)\oplus h_{K}(x_{j})=c\oplus c_{j}]\leq\varepsilon$.

This holds for all (c1*, . . . , c*d), so the probability of success in this case is bounded by ε multiplied by P
c1*,...,c*d Pr[V ] = 1.

In both cases, the probability of success is bounded by ε, so the overall probability of success of the attack is bounded by ε.

⊓⊔
In 1994, Krawczyk proposed a ε-XOR-universal hash function based on an LFSR [51]: each key K defines an irreducible polynomial of degree m over Z2 and the initial state of the LFSR defined by this polynomial. Then, hK(x) is the XOR of the states of the LFSR at time t corresponding to a bit xt = 1 in the message x.

In the GCM mode (see below), each key defines a Galois field element H, the message x defines a polynomial with binary coefficients, and hK(x) is simply the evaluation of this polynomial on H.

Just like the Vernam cipher can be replaced (at the cost of loosing the unconditional security)
by a stream cipher, we can just encrypt hK(X) with a stream cipher as well. This may require synchronization or to transmit a nonce.

This type of construction has severe drawbacks. If, for one reason or another, the user uses a nonce value for two different messages X and X′, it means that it applies the Vernam cipher with the same key to hK(X) and hK(X′). Clearly, this leaks the value of hK(X) ⊕ hK(X′). In many constructions, this could leak some information about K which allows to build forgeries. We will see it is the case for GCM.

Poly1305.

One variant of the WC-mac is the Poly1305 one-time authenticator. It is used to authenticate a message using a key to be used only once. A key consists of a pair (*r, s*) of numbers between 0 and 2128 − 1 (hence, 128-bit numbers). In applications, (*r, s*) is typically obtained by encrypting a nonce. A message to authenticate is a sequence of 128-bit numbers m1*, . . . , m*ℓ. The tag is computed by

$(m_{1}+2^{128})r^{\ell}+\cdots+(m_{\ell}+2^{128})r+s$ mod $(2^{130}-5)$
2130 − 5 is a prime number.

Authenticated modes of operation.

There exist some all-in-one mode of operation for block ciphers so that it integrates a MAC. These are *authenticated modes of operation*.

In the *CCM mode* (RFC 3610 [90]), the message is concatenated with its CBCMAC, then encrypted in CTR mode (CCM stands for "counter with CBCMAC"). More precisely, the algorithm is defined by two parameters M ∈ {4*, . . . ,* 16} (the size of the tag in bytes) and L ∈ {2, . . . , 8}
(the size of the length field in bytes). The encryption takes the key K, the message X (a stream of bytes), and a nonce N. The message is padded into X∥pad where pad consists of enough zero bytes to complete the last block. Then, we split X∥pad = B1*∥ · · · ∥*Bn into blocks Bi. We define a byte byte1 which encodes M and L. Then, we set B0 = byte1∥N∥length(X) to define an initial block. Then, we compute the CBCMAC (with key K) of B0∥B1*∥ · · · ∥*Bn and truncate it to M
bytes to obtain the tag T. I.e., we encrypt the block sequence in CBC mode with a zero initial vector and the last ciphertext block is truncated to define T. We define a byte byte2 which encodes L. We define some blocks Ai = byte2∥N∥i which will play the role of counters. Then, we encrypt T by T ⊕ truncM(CK(A0)) and X by X ⊕ trunclength(X)(CK(A1)*∥ · · · ∥*CK(An)) (see Fig. 6.6). The decryption is straightforward.

Interestingly, the CCM mode can also authenticate some *associated data* a at the same time as the encryption. For instance, secure messaging may like to bind a confidential message to the IP header in the protocol transporting the encrypting message. This information is not confidential but may be authenticated together with the message. For this, one special bit in byte1 is flipped to indicate that a is used and a new sequence of blocks length(a)∥a∥pad′ is inserted between B0
and B1 in the CBCMAC computation. Then, the associated data a must be provided for the decryption.

In the *GCM mode* [35], the message is concatenated with its universal hash (see above), then encrypted in CTR mode. The encryption algorithm GCMAE takes a key K, some initial vector IV, the message X, and some associated data A. It returns a ciphertext C and a tag T. If there is no X, the algorithm is a MAC algorithm GMAC and the returned ciphertext is also empty. The decryption takes K, IV, C, T, and A, and returns X or an error message. The encryption is based

on GCTR, which encrypts X in CTR mode using a provided counter ct (see Fig. 6.7). I.e.,

$\mathsf{GCTR}(\mathsf{ct},X)=X\oplus\mathsf{trunc}_{\mathsf{length}(X)}(C_{K}(\mathsf{ct})\|C_{K}(\mathsf{ct}+1)\|\cdots)$
We also define a universal hash function GHASHH(X1*, . . . , X*m) for a sequence of blocks X1, . . . , Xm and a key H which is another block. Each block is taken as an element of GF(2128) and we define

$\mathrm{GHASH}_{H}(X_{1},\ldots,X_{m})=X_{1}H^{m}+\cdots+X_{m}H$
in GF(2128). Then, we can define how GCMAE works: we set H to be the CK-encryption of the all-zero block. We define a counter block J0 = IV∥0311 in binary (i.e., IV padded with 31 zero bits and a last bit set to 1). The ciphertext C is the GCTR-encryption of X with counter J0 + 1.

To compute T, we first determine the number u of zero bits to add to C to have an integral number of blocks (i.e., C padded with u zero bits has a length multiple of the block length), and the number v of zero bits to add to A to have an integral number of blocks. Then, we compute S = GHASHH(A∥0v∥C∥0u∥length(A)∥length(C)). Finally, we GCTR-encrypt S with counter J0
and truncate the result into T.

To see the connection with the WC-MAC construction, we should see why GHASH is ε-XOR-
universal. We can see that any message X defined a polynomial PX whose degree is the blocklength of X.

We have GHASHH(X) = PX(H).

Hence, GHASHH(X) ⊕ GHASHH(Y ) = a is equivalent to (PX + PY )(H) = a. When X and Y have a number of blocks bounded by m, this is a polynomial equation in H of degree bounded by m. Hence, it has no more than m roots. We deduce that the probability over H that GHASHH(X) ⊕ GHASHH(Y ) = a holds is bounded by m2−128.

Reusing a nonce in GCM is a disaster. If an adversary gets T = GMACK(IV, A) and T ′ =
GMACK(IV, A′) with A ̸= A′, then T ⊕ T ′ = GHASHH(A) ⊕ GHASHH(A′) which is a polynomial equation with unknown H. We can solve it over the Galois field (the number of solutions we obtain is bounded by the size of A and A′ in blocks, and we can later isolate the right solution).

Once we know H, it is enough to authenticate anything. As we can XOR anything to a received ciphertext, we can easily forge the encryption of any message.

## 6.5 Formalism

We formalize a bit the notion of hash function.

Definition 6.4. A **hash function** is a tuple (D, {0, 1}τ, h) with a message domain D ⊆ {0, 1}∗, an output domain {0, 1}τ, and one efficient deterministic algorithm h implementing a function

$$\begin{array}{r l r l}{h:}&{{}{\mathcal{D}}}&{\longrightarrow}&{\{0,1\}^{\tau}}\\ {}&{X}&{\longmapsto}&{h(X)}\end{array}$$
The notion of one-wayness (or resistance to the first preimage attack) is formalized as follows.

Definition 6.5. A hash function (D, {0, 1}τ, h) is (*t, ε*)-one-way if for any probabilistic algorithm A limited to a time complexity and a code size of t*, the advantage* Adv is bounded by ε, where Adv = Pr[*game returns* 1]
Game
1: x
$←− D
2: y ← h(x)
3: A(y) → x′
4: **return** 1x=x′
We could easily define the notion of resistance to the second preimage attack. However, it must be relative to the distribution of the first preimage which is selected as there is no notion of uniform distribution when D is infinite.

To define collision-resistance is more tricky. We could try to formalize it as follows.

Definition 6.6 (bad definition!). A hash function (D, {0, 1}τ, h) is (*t, ε*)-secure against collision attacks if for any probabilistic algorithm A limited to a time complexity t*, the advantage* Adv is bounded by ε, where Adv = Pr[*game returns* 1]
Game
1: A(y) → x, x′
2: **return** 1h(x)=h(x′),x̸=x′
However, no hash function would be secure following this definition. Indeed, whenever #D > 2τ, we know that a collision (*x, x*′) exists. We could define an algorithm A which just prints this collision. This algorithm *exists* and works with very low complexity. Making a correct definition for collision-resistance is actually more difficult, and beyond the scope of this lecture.

We also formalize a bit the notion of message authentication code. For this, we limit ourselves to the most common construction of a symmetric message authentication scheme.

Definition 6.7. A **message authentication code** *is a tuple* ({0, 1}k, D, {0, 1}τ, MAC) with a key domain {0, 1}k, a message domain D ⊆ {0, 1}∗, an output domain {0, 1}τ, and one efficient deterministic algorithm MAC implementing a function from {0, 1}k × D to {0, 1}τ.

Write MACK(·) = MAC(K, .).

So, k is the key length and τ is the tag (output) length. It is important that MAC is deterministic so that computing twice on the same input produce the same output. So, verifying if a tag is correct can be done by recomputing it.

Just like for symmetric encryption, we first define security against key recovery. We could consider *known message attacks* (KMA) or *chosen message attacks* (CMA). For simplicity, we formalize only CMA.

Definition 6.8. *A message authentication code* ({0, 1}k, D, {0, 1}τ, MAC) is (*q, t, ε*)-secure against key recovery under chosen message attacks if for any probabilistic algorithm A limited to a time complexity t and to q *queries, the advantage* Adv is bounded by ε, where

Adv = Pr[*game returns* 1]
Game
Oracle OMac(X):
4: *return* MAC(*K, X*)
1: K
$←− {0, 1}k
2: AOMac → K′
3: **return** 1K=K′

A MAC always producing the same tag no matter the input is secure against key recovery (it does not use the key) but is clearly unusable to authenticate messages. So, we define the stronger notion of forgery attacks.

Definition 6.9. *A message authentication code* ({0, 1}k, D, {0, 1}τ, MAC) is (*q, t, ε*)-secure against forgery under chosen message attacks if for any probabilistic algorithm A limited to a time complexity t and to q *queries, the advantage* Adv is bounded by ε, where

Adv = Pr[*game returns* 1]
Game
Oracle OMac(X):
1: K
$←− {0, 1}k
6: Queried ← Queried ∪ {X}
7: *return* MAC(*K, X*)

2: Queried ← ∅
3: AOMac → (X, t)
4: if X ∈ Queried then return 0
5: return 1MAC(K,X)=t

Indeed, if A queries X to the authentication oracle, the oracle authenticates X, so we do not
consider X as being forged.
   If we have an algorithm A making a key recovery, we can transform it into an algorithm making
a forgery as follows:

1: run A → K
2: pick a fresh X arbitrarily

 3: compute c = MAC(K, X)
 4: return (X, c)
So, security against forgeries is stronger than security against key recovery.
    Finally, we have an even stronger security notion which is the notion of pseudorandom function.

Definition 6.10. A message authentication code ({0, 1}k, D, {0, 1}τ, MAC) is a (q, t, ε)-pseudorandom
function (PRF) if for any probabilistic algorithm A limited to a time complexity t and to q queries,
the advantage Adv is bounded by ε, where

$$\mathrm{Adv}=\mathrm{Pr}[\Gamma_{1}\,\,\,{\mathrm{_returns}}\,\,1]-\mathrm{Pr}[\Gamma_{0}\,\,\,{\mathrm{_returns}}\,\,1]$$
Oracle O(N, X):
Game Γb
1: K
$←− {0, 1}k
5: if b = 0 *then return* F(X)
6: *return* MAC(*K, X*)
2: pick F : D → {0, 1}τ
3: AO → z
4: **return** z Intuitively, an adversary who can make a forgery can check his forgery with an extra query. With the function MAC(*K, .*), it will say the forgery is correct. But with F(·), it will only confirm the forgery with probability 2−τ as F is random. So, if we can make a forgery, MAC cannot be pseudorandom.

More formally, given an adversary A making a forgery, we construct a distinguisher D as follows:
1: run AO → (*X, c*)
2: if X was queried by A, output 0 and abort
3: query X to O and get c′

4: output $1_{\varepsilon=\varepsilon^{\prime}}$

With the oracle $\mathcal{O}=\mathsf{MAC}(K,.)$, we have $\Pr[\mathcal{D}^{\mathsf{MAC}(K,.)}\to1]=\Pr[\mathcal{A}\text{wins}]$. With the oracle $\mathcal{O}=F(\cdot)$, we have $\Pr[\mathcal{D}^{F(\cdot)}\to1]\leq2^{-\tau}$. So,

$$\Pr[\mathcal{A}\text{wins}]\leq\mathsf{Adv}(\mathcal{D})+2^{-\tau}$$

Where $\mathsf{Adv}(\mathcal{D})=\Pr[\mathsf{P}^{\mathsf{MAC}(K,.)}\to1]-\Pr[\mathcal{D}^{F(\cdot)}\to1]$ is bounded by $\varepsilon$. Therefore, $(q,t,\varepsilon)$-PRF-regularity implies $(q-1,t-t_{0},\varepsilon+2^{-\tau})$-unforgeability, where $t_{0}$ is the computational overhead of $\mathcal{D}$ compared to $\mathcal{A}$.

## 6.6 Bruteforce Collision Search Algorithms

If we pick in a set of N elements some independent and uniformly distributed samples, and if the number of samples is asymptotically θ
√
N, the probability that at least one value is selected twice tends towards 1 − e− θ2
2 . For instance, if N = 365 is the calendar and if we look at the birthday of random people in the calendar, we need less than 25 samples to find two persons with the same birthday with good probability. This may look paradoxical. This is actually called the *birthday paradox*. Nevertheless, the idea is that with O(
√
N) we can find collisions within a constant probability.

Theorem 6.11. Let θ > 0 be a real number. If we pick n independent and uniformly distributed elements X1, . . . , Xn in a set of cardinality N, if n = o(N) as N goes to infinity, then the probability that at least two elements are equal is

$\Pr[\exists i<j\ X_{i}=X_{j}]=1-\frac{N!}{(N-n)!N^{n}}=1-e^{-\frac{n^{2}}{2N}+o(1)}$
Before we give the formal proof, we provide here an informal computation which is easier to remember:

$ \begin{array}{ccc}p&\approx&1-\left(1-\frac{1}{N}\right)^{\left(\frac{n}{2}\right)}\\ \\ &\approx&1-\left(1-\frac{1}{N}\right)^{\frac{n^2}{2}}\\ \\ &=&1-e^{\frac{n^2}{2}\ln\left(1-\frac{1}{N}\right)}\\ \\ &\approx&1-e^{-\frac{n^2}{2N}}\\ \\ &=&1-e^{-\frac{\theta^2}{2}}\end{array}$

4. 
N .

where we used that
                   n
                    2
                     
                      ≈ n2

$\frac{\textit{i}^2}{2}\:\text{and}\:\ln\left(1-\frac{1}{N}\right)\approx-\frac{1}{N}$

$$\textit{75}$$
Proof. Clearly, we have N n possible sequences of n values. The number of sequences of pairwise different values is N!

(N−n)!. So, we have

$$\mathrm{Pr}[\exists i<j\ X_{i}=X_{j}]=1-\frac{N!}{(N-n)!N^{n}}$$
Now, we use the Stirling formula

2πme−mmm m! ∼ m→+∞ √
and obtain

$$\Pr[\exists i<j\ X_{i}=X_{j}]=1-{\frac{N!}{(N-n)!N^{n}}}\mathop{\sim}_{N\to+\infty}^{\sim}\left(1-{\frac{n}{N}}\right)^{-N+n}e^{-n}=e^{-(N-n)\ln\left(1-{\frac{n}{N}}\right)-n}$$
2 + o(ε2) and obtain We now use ln(1 − ε) = −ε − ε2

$$\Pr[\exists i<j\ X_{i}=X_{j}]=1-{\frac{N!}{(N-n)!N^{n}}}\mathop{\sim}\limits_{N\to+\infty}^{\sim}e^{-{\frac{n^{2}}{2N}}+o(1)}$$
⊓⊔
This can be used to break hash functions: if we have digests of m bits, we take N = 2m and each random message x produces a random digest h(x) in a set of size N. So, with θ2
m
2 trials, we find a collision with probability 1 − e− θ2
2 ×
√
N). The algorithm runs as follows:
N.

In any case, it is O(
√
2 .

In another variant of the above result, we can show that if we repeatedly pick samples until we find a collision, the expected number of samples before we stop with a collision is p π
Input: a cryptographic hash function h onto a domain of size N
Output: a pair (*x, x*′) such that x ̸= x′ and h(x) = h(x′)
1: repeat
2:
pick a (new) random x

3:
compute y = h(x)
4:
insert (*y, x*) in the hash table
N) as well.

5: **until** there is already another (*y, x*′) pair in the hash table 6: yield (*x, x*′)
Note that this algorithm has a memory complexity of O(
√
N.

2 ×
√
Theorem 6.12. In the above algorithm, assuming that the obtained y values are independent and uniformly distributed, the expected number of iterations is asymptotically equivalent to p π
As an example, we can show an attack against a variant of CBCMAC called EMAC. Following EMAC, we use two keys K1 and K2. The MAC is the encryption under K2 of the CBCMAC under K1 of the message. Let us say that the block length is ℓ. To break EMAC, we submit 2
ℓ
2
chosen random message to get their tags and eventually get two messages X1 ̸= X2 such that their tags t are equal. This means that their CBCMAC under key K1 must be equal. We can now pick a random block B and ask for the tag t′ of X1∥B. Finally, the forged tag for X2∥B is also t′. This comes from t′ being the t′ = CK2(B ⊕ C−1
K2(t)), due to the properties of the CBCMAC.

CBCMAC (and variants) are not the only constructions which are vulnerable to collisions at the birthday bound. We can also have a similar attack against PMAC. Indeed, assuming that 128 is the bitlength of blocks, we can get 264 pairs (Bi, PMACK(x∥Bi)) and 264 pairs (B′
j, PMACK(x∥B′
j)), where x is an arbitrary sequence of blocks, each Bi is an incomplete block, and each B′
i is a complete block. We expect to find a collision PMACK(x∥Bi) = PMACK(x∥B′
j). Due to the structure of PMAC, we deduce (assuming t = 128) CK(Bi) = CK(B′
j ⊕(2−1 ·L)) so 2·(Bi ⊕Bj) = L. With L, we can mount forgeries. For *t <* 128 we obtain candidates for L which can be filtered. Eventually, we recover the right L.

There exist also constant-memory algorithms to find collisions with complexity O(
√
8 ×
√
N). For instance, the Floyd cycle algorithm can be used [38]. The idea is that by picking x0 at random and a permutation σ over the digest space, then iterating the function F = σ ◦H on x0 will eventually cycle. The graph of the obtained values will have a "ρ shape", with a tail and a loop. The point at which the tail connects to the loop is a collision. The magic is that, for this random function, the expected length of the tail λ and the expected length of the loop τ are both p π
N. So, the collision is found by visiting the graph in a clever way.

The Floyd algorithm is also called the tortoise and the hare algorithm. There are two animals
(a tortoise and a hare) running over the trail with the form of a ρ, but one (the hare) goes twice faster than the other. They start from the tail of the ρ and will meet again inside the loop. The trick is that the number of steps until they meet again is necessarily a multiple of the length of the loop. So, by making and a new tortoise start from the tail as the other tortoise is also continuing, the two tortoises will meet at the collision point. The algorithm works as follows:
1: pick x0, a permutation σ, and define F = σ ◦ H

2: a ← x0 (tortoise)
3: b ← x0 (hare)
4: repeat
5:
        a ← F(a)

6:
        b ← F(F(b))

7: until a = b

8: a ← x0
9: aold ← ⊥

10: bold ← ⊥
11: while a ̸= b do
12:
        aold ← a

13:
        bold ← b

14:
        a ← F(a) 15:
        b ← F(b)

16: end while
17: print aold and bold The number i of iterations of the repeat loop is such that λ ≤ i ≤ λ+τ. The number of iterations of the while loop is λ. So, the number of F computations is 3i + 2λ. This is O(
√
N).

As an example, we plotted the graph of a function over Z128 that we took arbitrarily by x = uv 7→ first byte of SHA256("3.1415927-uv") mod 128
where uv is x in hexadecimal. More precisely, the function was implemented by the following bash script:
#! /bin/bash string="3.1415927"

for i in {0..127}
do
  j='printf "$string-%02x" $i | sha256sum'
  j='echo $j | tr "abcdef" "ABCDEF"'
  j='echo $j | sed "s/^\(..\).*$/ibase=16;obase=2;\1/g" | bc'
  j='echo $j | sed "s/.$//g"'
  j='echo $j | sed "s/^/ibase=2;obase=A;/g" | bc'
  echo "$i -> $j"
done

128π

8
   ≈ 7. Hence, we should expect tails and

To apply the previous result, we first note that q loops to have length 7 on average. We plot the graph of the function on Fig. 6.8 and put in red all vertices which have a tail of at most 7.

## 6.7 How To Select Security Parameters?

Symmetric encryption must face the generic attacks of complexity 2n, when n is the bitlength of the key. We take this as a reference for a security: a symmetric encryption scheme is secure if this is the best attack we can mount on it. So, the keylength is the security parameter. In general, we say that the bitlength-equivalent security is n if the best attack needs 2n operations.

For hash functions with digests of n bits, as far as preimage attacks are concerned, the bitlength equivalent is n. But *if we care* for collisions, the bitlength equivalent is n
2 . So, n shall be doubled compared to the key length in symmetric encryption to obtain a corresponding security.

We should note that it is dangerous to underestimate the impact of academic attacks, specially about collisions. People think that "academic collisions" which are found are not so dangerous because they result after incredible efforts into colliding messages which have no meaning.

It was however demonstrated that these attacks can be transformed into collision forgeries for media content with real meaning. Indeed, most of media format look like programming. There are several techniques to "hide" random-looking parts (which make the collision) into a content which makes sense. For instance, we can make a media document of three parts: a common prefix, a collision block, and a common suffix. The common prefix could be a program taking some data at a target address inside the collision block and interpreting it as another address, then the command to jump a this address. Common prefix:
1: take addr at address target 2: jump to addr If we build two (random-looking) collision blocks inside which this address becomes either addr1
and addr2, we can then build a common suffix which will have form Common suffix:
1: [filler]
2: [starting at addr1] content 1
3: [filler]
4: [starting at addr2] content 2
The two documents formed this way will show either content 1 or content 2. Of course, the inner structure will show the trick, but the attack could be devastating.

## 6.8 Other Reasons Why Security Collapses

Note that the security of symmetric encryption, MAC, and hashing, is often purely heuristic. We are not able to prove the security based on some well established hardness assumption like in public-key cryptography. So, we rather consider the primitives as secure until we have a proof that it is not the case.

This assumes that discovered attacks become public.

Assuming that the academic research is ahead of hidden research on cryptanalysis, this may be a reasonable assumption when the primitive gets enough exposure.

Security may collapse if the academic research discovers an attack on these primitives. There is also a quantum threat on symmetric cryptography: Grover has shown that using quantum computer, the bitlength equivalent security is divided by two, compared to classical computers [43].

Symmetric primitives also suffer from side channel attacks: sometimes, implementation may leak some information, although the mathematical model of the primitive does not consider it.

It is also unfortunate that many security arguments in the literature happen to be incorrect.

Sometimes, even security models are inappropriate, if not irrelevant at all.

Finally, the stand-alone security of primitives rarely offers any guaranty on the security when the primitive is used together with others in a complicated infrastructure.

This is the unfortunate current state of research in cryptography.