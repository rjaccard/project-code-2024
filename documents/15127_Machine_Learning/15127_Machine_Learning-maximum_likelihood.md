# Maximum Likelihood 

## Motivation

In the previous lecture 3a we arrived at the least-squares problem in the following way: we postulated a particular cost function (square loss) and then, given data, found that model that minimizes this cost function. In the current lecture we will take an alternative route. The final answer will be the same, but our starting point will be probabilistic. In this way we find a second interpretation of the least-squares problem.

## Gaussian Distribution And Independence

Recall the definition of a Gaussian random variable in R with mean µ and variance σ2. It has a density of

$$p(y\,|\,\mu,\sigma^{2})={\mathcal{N}}(y\,|\,\mu,\sigma^{2})={\frac{1}{\sqrt{2\pi\sigma^{2}}}}\exp\left[-{\frac{(y-\mu)^{2}}{2\sigma^{2}}}\right].$$

In a similar manner, the density of a Gaussian random vector with mean µ and covariance Σ (which must be a positive semi-definite matrix) is

$$\mathcal{N}(\mathbf{y}\mid\boldsymbol{\mu},\boldsymbol{\Sigma})=\frac{1}{\sqrt{(2\pi)^{D}\det(\boldsymbol{\Sigma})}}\exp\left[-\frac{1}{2}(\mathbf{y}-\boldsymbol{\mu})^{\top}\boldsymbol{\Sigma}^{-1}(\mathbf{y}-\boldsymbol{\mu})\right].$$

Also recall that two random variables $X$ and $Y$ are called _independent_ when $p(x,y)=p(x)p(y)$.

## A Probabilistic Model For Least-Squares

We assume that our data is generated by the model,
$$y_{n}={\bf x}_{n}^{\top}{\bf w}+\epsilon_{n},$$
where the ϵn (the noise) is a zeromean Gaussian random variable with variance σ2 and the noise that is added to the various samples is independent of each other, and independent of the input. Note that the model w is unknown.

Therefore, given N samples, the likelihood of the data vector y = (y1, · · ·, yN) given the input X
(each row is one input) and the model w is equal to
$$p(\mathbf{y}\mid\mathbf{X},\mathbf{w})=\prod_{n=1}^{N}p(y_{n}\mid\mathbf{x}_{n},\mathbf{w})=\prod_{n=1}^{N}{\mathcal{N}}(y_{n}\mid\mathbf{x}_{n}^{\top}\mathbf{w},\sigma^{2}).$$

The probabilistic view point is that we should maximize this likelihood over the choice of model w. I.e., the
"best" model is the one that maximizes this likelihood.

## Defining Cost With Log-Likelihood

Instead of maximizing the likelihood, we can take the logarithm of the likelihood and maximize it instead. Expression is called the loglikelihood (LL).

$${\mathcal{L}}_{\mathrm{LL}}(\mathbf{w}):=\log\,p(\mathbf{y}\,|\,\mathbf{X},\mathbf{w})=-{\frac{1}{2\sigma^{2}}}\sum_{n=1}^{N}(y_{n}-\mathbf{x}_{n}^{\top}\mathbf{w})^{2}+\mathrm{cnst}.$$

Compare the LL to the MSE (mean squared error)

$$\begin{array}{c}{{{\mathcal{L}}_{\mathrm{LL}}(\mathbf{w})=-\frac{1}{2\sigma^{2}}\sum_{n=1}^{N}(y_{n}-\mathbf{x}_{n}^{\top}\mathbf{w})^{2}+\mathrm{cnst}}}\\ {{{\mathcal{L}}_{\mathrm{MSE}}(\mathbf{w})=\frac{1}{2N}\sum_{n=1}^{N}(y_{n}-\mathbf{x}_{n}^{\top}\mathbf{w})^{2}}}\end{array}$$

## Maximum-Likelihood Estimator (Mle)

It is clear that maximizing the LL is equivalent to minimizing the MSE:

$$\operatorname*{arg\,min}_{\bf w}\;{\mathcal L}_{\mathrm{MSE}}({\bf w})=\arg\operatorname*{max}_{\bf w}\;{\mathcal L}_{\mathrm{LL}}({\bf w}).$$

This gives us another way to design cost functions.

MLE can also be interpreted as finding the model under which the observed data is most likely to have been generated from (probabilistically). This interpretation has some advantages that we discuss now.

## Properties Of Mle

MLE is a *sample* approximation to the *expected log-likelihood*:

$${\mathcal{L}}_{\mathrm{LL}}(\mathbf{w})\approx\mathbb{E}_{p(y,\mathbf{x})}\left[\,\log p(y\,|\,\mathbf{x},\mathbf{w})\right]$$

MLE is consistent, i.e., it will give us the correct model assuming that we have a sufficient amount of data.

$${\bf W}_{\mathrm{MLE}}\longrightarrow^{p}{\bf W}_{\mathrm{true}} \mathrm{in~probability}$$

The MLE is asymptotically normal, i.e.,
$$\begin{array}{r c l}{{\left(\mathbf{w}_{\mathrm{MLE}}-\mathbf{w}_{\mathrm{true}}\right)}}&{{\longrightarrow^{d}}}&{{\frac{1}{\sqrt{N}}\mathcal{N}\left(\mathbf{w}_{\mathrm{MLE}}\mid\mathbf{0},\mathbf{F}^{-1}\left(\mathbf{w}_{\mathrm{true}}\right)\right)}}\end{array}$$

where $\mathbf{F}(\mathbf{w})=-\mathbb{E}_{p(\mathbf{y})}\left[\frac{\partial^{2}\mathcal{L}}{\partial\mathbf{w}\partial\mathbf{w}^{\top}}\right]$ is the Fisher information.

MLE is efficient, i.e. it achieves the Cramer-Rao lower bound.
$Covariance(w_{MLE}) = F^{−1}(w_{true})$