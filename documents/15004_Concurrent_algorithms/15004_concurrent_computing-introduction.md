# 1. Introduction 

### 1.1. A broad picture: the concurrency revolution

The field of concurrent computing has gained a huge importance after major chip manufacturers have switched their focus from increasing the speed of individual processors to increasing the number of processors on a chip. The good old times where nothing needed to be done to boost the performance of programs, besides changing the underlying processors, are over. To exploit multicore architectures, programs have to be executed in a concurrent manner. In other words, the programmer has to design a program with more and more threads and make sure that concurrent accesses to shared data do not create inconsistencies. A single-threaded application can for instance exploit at most $1 / 100$ of the potential throughput of a 100 -core chip.

The computer industry is thus calling for a software revolution: the concurrency revolution. This might look surprising at first glance for the very idea of concurrency is almost as old as computer science. In fact, the revolution is more than about concurrency alone: it is about concurrency for everyone.

Concurrency is going out of the small box of specialized programmers and is conquering the masses now. Somehow, the very term "concurrency" itself captures this democratization: we used to talk about "parallelism". Specific kinds of programs designed by specialized experts to clearly involve independent tasks were deployed on parallel architectures. The term "concurrency" better reflects a wider range of programs where the very facts that the tasks executing in parallel compete for shared data is the norm rather than the exception. But designing and implementing such programs in a correct and efficient manner is not trivial.

A major challenge underlying the concurrency revolution is to come up with a library of abstractions that programmers can use for general purpose concurrent programming. Ideally, such library should both be usable by programmers with little expertise in concurrent programming as well as by advanced programmers who master how to leverage multicore architectures. The ability of these abstractions to be composed is of key importance, because an application could be the result of assembling independently devised pieces of code.

The aim of this book is to study how to define and build such abstractions. We will focus on those that are considered (a) the most difficult to get right and (b) having the highest impact on the overall performance of a program: synchronization abstractions, also called shared objects or sometimes concurrent data structures.

### 1.2. The topic: shared objects

In concurrent computing, a problem is solved through several processes that execute a set of tasks. In general, and except in so called "embarrassingly parallel" programs, i.e., programs that solve problems that can easily and regularly be decomposed into independent parts, the tasks usually need to synchronize their activities by accessing shared constructs, i.e., these tasks depend on each other. These typically serialize the threads and reduce parallelism. According to Amdahl's law [4], the cost of accessing these constructs significantly impacts the overall performance of concurrent computations. Devising, implementing and making good usage of such synchronization elements usually lead to intricate schemes that are very fragile and sometimes error prone.

Every multicore architecture provides synchronization constructs in hardware. Usually, these constructs are "low-level" and making good usage of them is far from trivial. Also, the synchronization constructs that are provided in hardware differ from architecture to architecture, making concurrent programs hard to port. Even if these constructs look the same, their exact semantics on different machines may also be different, and some subtle details can have important consequences on the performance or the correctness of the concurrent program. Clearly, coming up with a high-level library of synchronization abstractions that could be used across multicore architectures is crucial to the success of the multicore revolution. Such a library could only be implemented in software for it is simply not realistic to require multicore manufacturers to agree on the same high-level library to offer to their programmers.

We assume a small set of low-level synchronization primitives provided in hardware, and we use these to implement higher level synchronization abstractions. As pointed out, these abstractions are supposed to be used by programmers of various skills to build application pieces that could themselves be used within a higher-level application framework.

The quest for synchronization abstractions, i.e., the topic of this book, can be viewed as a continuation of one of the most important quests in computing: programming abstractions. Indeed, the History of computing is largely about devising abstractions that encapsulate the specifities of underlying hardware and help programmers focus on higher level aspects of software applications. A file, a stack, a record, a list, a queue and a set, are well-known examples of abstractions that have proved to be valuable in traditional sequential and centralized computing. Their definitions and effective implementations have
enabled programming to become a high-level activity and made it possible to reason about algorithms without specific mention of hardware primitives.

In modern computing, an abstraction is usually captured by an object representing a server program that offers a set of operations to its users. These operations and their specification define the behavior of the object, also called the type of the object.

The way an abstraction (object) is implemented is usually hidden to its users who can only rely on its operations and their specification to design and produce upper layer software, i.e., software using that object. The only visible part of an object is the set of values in can return when its operations are invoked. Such a modular approach is key to implementing provably correct software that can be reused by programmers in different applications.

The abstractions we study in this book are shared objects, i.e., objects that can be accessed by concurrent processes, typically running on independent processors. That is, the operations exported by the shared object can be accessed by concurrent processes. Each individual process accesses however the shared object in a sequential manner. Roughly speaking, sequentiality means here that, after it has invoked an operation on an object, a process waits to receive a reply indicating that the operation has terminated, and only then is allowed to invoke another operation on the same or a different object. The fact that a process $p$ is executing an operation on a shared object $X$ does not however preclude other processes $q$ from invoking an operation on the same object $X$.

The objects considered have a precise sequential specification. called also its sequential type, which specifies how the object behaves when accessed sequentially by the processes. That is, if executed in a sequential context (without concurrency), their behavior is known. This behavior might be deterministic in the sense that the final state and response is uniquely defined given every operation, input parameters and initial state. But this behavior could also be non-deterministic, in the sense that given an initial state of the object, and operation and an input parameter, there can be several possibilities for a new state and response.

To summarize, this book studies how to implement, in the algorithmic sense, objects that are shared by concurrent processes. Strictly speaking, the objective is to implement object types but when there is no ambiguity, we simply say objects. In a sense, a process represents a sequential Turing machine, and the system we consider represents a set of sequential Turing machines. These Turing machines communicate and synchronize their activities through low-level shared objects. The activities they seek to achieve consist themselves in implementing higher-level shared objects. Such implementations need to be correct in the sense that they typically need to satisfy two properties: linearizability and waitfreedom. We now overview these two properties before detailing them later.

### 1.3. Linearizability

This property says that, despite concurrency among operations of an object, these should appear as if they were executed sequentially. Two concepts are important here. The first is the notion of appearance, which, as we already pointed out, is related to the values returned by an operation: these values are the only way through which the behavior of an object is visible to the users of that object, i.e., the applications using that object. The second is the notion of sequentiality which we also discussed earlier. Namely, The operations issued by the processes on the shared objects should appear, according to the values they return, as if they were executing one after the other. Each operation invocation op on an object $X$ should appear to take effect at some indivisible instant, called the linearization point of that invocation, between the invocation and the reply times of $o p$.

In short, linearizabiliy delimits the scope of an object operation, namely what it could respond in a concurrent context, given the sequential specification of that object. This property, also sometimes
called atomicity, transforms the difficult problem of reasoning about a concurrent system into the simpler problem of reasoning about a sequential one where the processes access each object one after the other. Linearizability constraints the implementation of the object but simplifies its usage on the other hand. To program with linearizable objects, also called atomic objects, the developer simply needs the sequential specification of each object, i.e., its sequential type.

Most interesting synchronization problems are best described as linearizable shared objects. Examples of popular synchronization problems are the reader-writer and the producer-consumer problems. In the reader-writer problem, the processes need to read or write a shared data structure such that the value read by a process at a given point in time $t$ is the last value written before $t$. Solving this problem boils down to implementing a linearizable object exporting read() and write() operations. Such an object type is usually called a linearizable, an atomic read-write variable or a register. It abstracts the very notions of shared file and disk storage.

In the producer-consumer problem, the processes are usually split into two camps: the producers which create items and the consumers which use the items. It is typical to require that the first item produced is the first to be consumed. Solving the producer-consumer problem boils down to implementing a linearizable object type, called a FIFO queue (or simply a queue) that exports two operations: enqueue() (invoked by a producer) and dequeue() (invoked by a consumer).

Other exemples include for instance counting, where the problem consists in implementing a shared counter, called FAI Fetch - and - Increment. Processes invoque this object to increment the value of the counter and get the current value.

### 1.4. Wait-freedom

This property basically says that processes should not prevent each other from obtaining values to their operations. More specifically, no process $p$ should ever prevent any other process $q$ from making progress, i.e., obtaining responses to $q$ 's operations, provided $q$ remains alive and kicking. A process $q$ should be able to terminate each of its operations on a shared object $X$ despite speed variations or the failure of any other process $p$. Process $p$ could be very fast and might be permanently accessing shared object $X$, or could have been swapped out by the operating system while accessing $X$. None of these situations should prevent $q$ from completing its operation. Wait-freedom conveys the robustness of an implementation. It transforms the difficult problem of reasoning about a failure-prone system where processes can be arbitrarily delayed or speeded up, into the simpler problem of reasoning about a system where every process progresses at its own pace and runs to completion.

In other words, wait-freedom says that the process invoking the operation on the object should obtain a response for the operation, in a finite number of its own steps, independently of concurrent steps from other processes. The notion of step, as we will discuss later, means here a local instruction of the process, say updating a local variable, or an operation invocation on a base object (low-level object) used in the implementation.

### 1.5. Combining linearizability and wait-freedom

Ensuring linearizability alone or wait-freedom alone is simple. A trivial wait-free implementation could return arbitrary responses to each operation, say some value corresponding to some initial state of the object. This would satisfy wait-freedom as no process would prevent other processes from progressing. However, the responses would no satisfy linearizability.

Also, one could ensure linearizability using a basic mutual exclusion mechanism so that every operation on the implemented object is performed in an indivisible critical section. Some traditional synchro-
nization schemes rely indeed on mutual exclusion (usually based on some locking primitives): critical shared objects (or critical sections of code within shared objects) are accessed by processes one at a time. No process can enter a critical section if some other process is in that critical section. We also say that a process has acquired a lock on that object (resp., critical section). Linearizability is then automatically ensured if all related variables are protected by the same critical section. This however significantly limits the parallelism and thus the performance of the program, unless the program is devised with minimal interference among processes. Mutual exclusion hampers progress since a process delayed in a critical section prevents all other processes from entering that critical section. In other words, it violates wait-freedom. Delays could be significant and especially when caused by crashes, preemptions and memory paging. For instance, a process paged-out might be delayed for millions of instructions, and this would mean delaying many other processes if these want to enter the critical section held by the delayed process. With modern architectures, we might be talking about one process delaying hundreds of processors, making them completely idle and useless. We will study other, weaker lock-free implementations, which also provide an alternative to mutual exclusion-based implementations.

### 1.6. Object implementation

As explained, this book studies how to wait-free implement high-level atomic objects out of more primitive base objects. The notions of high and primitive being of course relative as we will see. It is also important to notice that the term implement is to be considered in an abstract manner; we will describe the algorithms in pseudo-code. There will not be any C or Java code in this book. A concrete execution of these algorithms would need to go through a translation into some programming language.

An object to be implemented is typically called high-level, in comparison with the objects used in the implementation, considered at a lower-level. It is common to talk about emulations of the high-level object using the low-level ones. Unless explicitly stated otherwise, we will by default mean wait-free implementation when we write implementation, and atomic object when we write object.

It is often assumed that the underlying system model provides some form of registers as base objects. These provide the abstraction of read-write storage elements. Message-passing systems can also, under certain conditions, emulate such registers. Sometimes the base registers that are supported are atomic but sometimes not. As we will see in this book, there are algorithms that implement atomic registers out of non-atomic base registers that might be provided in hardware.

Some multiprocessor machines also provide objects that are more powerful than registers like test \&set objects or compare\&swap objects. Intuitively, these are more powerful in the sense that the writer process does not systematically overwrite the state of the object, but specifies the conditions under which this can be done. Roughly speaking, this enables more powerful synchronization schemes than with a simple register object. We will capture the notion of "more powerful" more precisely later in the book.

Not surprisingly, a lot of work has been devoted over the last decades to figure out whether certain objects can wait-free implement other objects. As we have seen, focusing on wait-free implementations clearly excludes mutual exclusion (locking) based approaches, with all its drawbacks. From the application perspective, there is a clear gain because relying on wait-free implementations makes it less vulnerable to failures and dead-locks. However, the desire for wait-freedom makes the design of atomic object implementations subtle and difficult. This is particularly so when we assume that processes have no a priori information about the interleaving of their steps: this is the model we will assume by default in this book to seek general algorithms.

### 1.7. Reducibility

In its abstract form, the question we address in this book, namely of implementing high-level objects using lower level objects, can be stated as a general reducibility question. Given two object types $X 1$ and $X 2$, can we implement $X 2$ using any number of instances of $X 1$ (we simply say "using $X 1$ ")? In other words, is there an algorithm that implements $X 2$ using $X 1$ ? In the case of concurrent computing, "implementing" typically assumes providing linearizability and wait-freedom. These notions encapsulate the smooth handling of concurrency and failures.

When the answer to the reducibility question is negative, and it will be for some values of $X 1$ and $X 2$, it is also interesting to ask what is needed (under some minimality metric) to add to the low-level objects $(X 1)$ in order to implement the desired high-level object (X2). For instance, if the base objects provided by a given multiprocessor machine are not enough to implement a particular object in software, knowing that extending the base objects with another specific object (or many of such objects) is sufficient, might give some useful information to the designers of the new version of the multiprocessor machine in question. We will see examples of these situations.

### 1.8. Organization

The book is organized in an incremental way, starting from very basic objects, then going step by step to implementing more and more sophisticated and powerful objects. After precisely defining the notions of linearizability and wait-freedom, we proceed through the following steps.

1. We first study how to implement linearizable read-write registers out of non-linearizable base registers, i.e., registers that provide weaker guarantees than linearizability. Furthermore, we show how to implement registers that can contain values from an arbitrary large range, and be read and written by any process in the system, starting from single-bit (containing only 0 or 1 ) base registers, where each base register can be accessed by only one writer process and only one reader process.
2. We then discuss how to use registers to implement seemingly more sophisticated objects than registers, like counters and snapshot objects. We contrast this with the inherent limitation of linearizable registers in implementing more powerful objects like queues. This limitation is highlighted through the seminal consensus impossibility result.
3. We then discuss the importance of consensus as an object type, by proving its universality. In particular, we describe a simple algorithm that uses registers and consensus objects to implement any other object. We then turn to the question on how to implement a consensus object from other objects. We describe an algorithm to implement a consensus object in a system of two processes, using registers and either a test\&set or a queue objects, as well as an algorithm that implements a consensus object using a compare\&swap object in a system with an arbitrary number of processes. The difference between these implementations is highlighted to introduce the notion of consensus number.
4. We then study a complementary way of implementing consensus: using registers and specific oracles that reveal certain information about the operational status of the processes. Such oracles can be viewed as failure detectors providing information about which process are operational and which processes are not. We discuss how even an oracle that is unreliable most of time can help devise a consensus algorithm. We also discuss the implementation of such an oracle assuming that the computing environment satisfies additional assumptions about the scheduling
of the processes. This may be viewed as a slight weakening of the wait-freedom requirement which requires progress no matter how processes interleave their steps.
