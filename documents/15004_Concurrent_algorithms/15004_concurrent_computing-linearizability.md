# 2. Linearizability 

### 2.1. Introduction

Linearizabiliy is a metric of the correctness of a shared object implementation. It addresses the question of what values can be returned by an object that is shared by concurrent processes. If an object returns a response, linearizability says whether this response is correct or not.

The notion of correctness, as captured by linearizability, is defined with respect to how the object is expected to react when accessed sequentially: this is called the sequential specification of the object. In this sense, the notion of correctness of an object, as captured by linearizability, is relative to how the object is supposed to behave in a sequential world.

It is important to notice that linearizability does not say when an object is expected to return a response. As we will see later, the complementary to linearizability is the wait-freedom property, another correctness metric that captures the fact that an object operation should eventually return a response (if certain conditions are met).

To illustrate the notion of linearizability, and the actual relation to a sequential specification, consider a FIFO (first-in-first-out) queue. This is an object of the type queue that contains an ordered set of elements and exhibits the following two operations to manipulate this set.

- $\operatorname{Enq}(a)$ : Insert element $a$ at the end of the queue;
- $\operatorname{Deq}():$ Return the first element inserted in the queue that was not already removed; Then remove this element from the queue; if the queue is empty, return the default element $\perp$.

Figure 2.1 conveys a sequential execution of a system made up of a single process accessing the queue (here the time line goes from left to right). There is only a single object and a single process so we omit their identifiers here. The process first enqueues element $a$, then element $b$, and finally element c. According to the expected semantics of a queue (first-in-first-out), and as depicted by the figure, the first dequeue invocation returns element $a$ whereas the second returns element $b$.

Figure 2.2 depicts a concurrent execution of a system made up of two processes sharing the same queue: $p_{1}$ and $p_{2}$. Process $p_{2}$, acting as a producer, enqueues elements $a, b, c, d$, and then $e$. On the other hand, process $p_{1}$, acting as a consumer, seeks to de dequeue two elements. On Figure 2.2, the execution of $\operatorname{Enq}(a), \operatorname{Enq}(b)$ and $\operatorname{Enq}(c)$ by $p_{2}$ overlaps with the first $\operatorname{Deq}()$ of $p_{1}$ whereas the execution of $\operatorname{Enq}(c), \operatorname{Enq}(d)$ and $E n q(e)$ by $p_{2}$ overlaps with the second $\operatorname{Deq}()$ of $p_{1}$. The questions raised in the figure are what elements can be dequeued by $p 1$. The role of linearizability is precisely to address such questions.

Linearizability does so by relying on how the queue is supposed to behave if accessed sequentially. In other words, what should happen in Figure 2.2 depends on what happens in Figure 2.1. Intuitively, linearizability says that, when accessed concurrently, an object should return the same values that it could have returned in some sequential execution. Before defining linearizability however, and the very concept of "value that could have been returned in some sequential execution", we first define more precisely some important underlying elements, namely processes and objects, and then the very notion of a sequential specification.

### 2.2. The Players

### 2.2.1. Processes

We consider a system consisting of a finite set of $n$ processes, denoted $p_{1}, \ldots, p_{n}$. Besides accessing local variables, processes may execute operations on shared objects (we will sometimes simply say objects. Through these objects, the processes synchronize their computations. In the context of this chapter, which aims at defining linearizability of the objects, we will omit the local variables accessed by the processes.

An execution by a process of an operation on a object $X$ is denoted $X . o p(\arg )(\mathrm{res})$ where $\arg$ and res denote, respectively, the input and output parameters of the invocation. The output corresponds to the response to the invocation. It is common to write $X . o p$ when the input and output parameters are not important.

The execution of an operation $o p()$ on an object $X$ by a process $p_{i}$ is modeled by two events, namely, the events denoted $\operatorname{inv}\left[X . o p(\arg )\right.$ by $\left.p_{i}\right]$ that occurs when $p_{i}$ invokes the operation (invocation event), and the event denoted $\operatorname{resp}\left[\right.$ X.op(res) by $\left.p_{i}\right]$ that occurs when the operation terminates (response event). We say that these events are generated by process $p_{i}$ and associated with object $X$. Given an operation $X . o p(\arg )(\mathrm{res})$, the event $\operatorname{resp}\left[\right.$ X.op(res) by $\left.p_{i}\right]$ is called the response event matching the invocation event $i n v\left[X . o p\left(\arg\right.\right.$ by $\left.p_{i}\right]$. Sometimes, when there is no ambiguity, we talk about operations where we should be talking about operation executions. We also say sometimes that the object returns a response to the process. This is by language abuse because it is actually the process executing the operation on the object that actually computes the response.

Every interaction between a process and an object corresponds to a computation step and is represented by an event: the visible part of a step, i.e., the invocation or the reply of an operation. A sequence of such events is called a history and this is precisely how we model executions of processes on shared objects. Basically, a history depicts the sequence of observable events of the execution of a concurrent system. We will detail the very notion of history later in this chapter.

While we assume that the system of processes is concurrent, we assume that each process is individually sequential: a process executes (at most) one operation on an object at a time. That is, the algorithm of a sequential process stipulates that, after an operation is invoked on an object, and until a matching response is returned, the process does not invoke any other operation. As pointed out, the fact
that processes are (individually) sequential does not preclude them from concurrently invoking operations on the same shared object. Sometimes however, we will focus on sequential executions (modeled by sequential histories) which precisely preclude such concurrency; that is, only one process at a time invokes an operation on an object.

### 2.2.2. Objects

An object has a unique identity and is of a unique type. Multiple objects can be of the same type however: we talk about instances of the type. In our context, we consider a type as defined by (1) the set of possible values for (the states of) objects of that type, including the initial state; (2) a finite set of operations through which the (state of the ) objects of that type can be manipulated; and (3) a sequential specification describing, for each operation of the type, the effect this operation produces when it executes alone on the object, i.e., in the absence of concurrency. The effect is measured in terms of the response that the object returns and the new state that the object gets to after the operation executes.

We assume here that every operation of an object type can be applied on each of its states. This sometimes requires specific care when defining the objects. For instance, if a dequeue operation is invoked on a queue which is in an empty state, a specific response nil is returned.

We say that an object operation is deterministic if, given any state of the object and input parameters, the response and the resulting state of the object are uniquely defined. An object type is deterministic if it has only deterministic operations. We assume here finite non-determinism, i.e., for each state and operation, the set of possible outcomes (response and resulting state) is finite. Otherwise the object is said to be non-deterministic: several outputs and resulting states are possible. The pair composed of (a) the output returned and (b) the resulting state, is chosen randomly from the set of such possible pairs (or from an infinite set).

A sequential specification is generally modeled as a set of sequences of invocations immediately followed by matching responses that, starting from an initial state of an object, are allowed by the object (type) when it is accessed sequentially. Indeed the resulting state obtained after each operation execution is not directly conveyed, but it is indirectly reflected through the responses returned in the subsequence operations of the sequence.

To illustrate the notion of a sequential specification, consider the following two object types:

Example 1: a FIFO queue The first example is the unbounded (FIFO) queue described earlier. The producer enqueues items in a queue that the consumers dequeues. The type has the following sequential specification: every dequeue returns the first element enqueued and not dequeued yet. If there is not such element (i.e., the queue is empty), a specific default value nil is returned. As pointed out earlier this specification never prevents an enqueue or a dequeue operation to be executed. One could consider a variant of the specification where the dequeue could not be executed if the queue is empty - it would have to wait for an enqueue - we preclude such specifications.

Designing algorithms that implement this object correctly in a concurrent context captures the classical producer/consumer synchronization problem.

Example 2: a read/write object (register) The second example (called register) is a simple read/write abstraction that models objects such as a shared memory word, a shared file or a shared disk. Designing algorithms that implement this object correctly in a concurrent context captures the classical reader/writer synchronization problem.

The type exports two operations:

- The operation $\operatorname{read}()$ has no input parameter. It returns the value stored in the object.
- The operation write $(v)$ has an input parameter, $v$, representing the new value of the object. This operation returns value $o k$ indicating to the calling process that the operation has terminated.

The sequential specification of the object is defined by all the sequences of read and write operations in which each read operation returns the input parameter of the last preceding write operation (i.e., the last value written). We will study implementations of this object in the next chapters.

### 2.2.3. Histories

Processes interact with shared objects via invocation and response events. Such events are totally ordered. (Simultaneous events are arbitrarly ordered).

The interaction between processes and objects is thus modeled as a totally ordered set of events $H$, called a history (sometimes also called a trace). The total order relation on $H$, denoted $<_{H}$, abstracts out the real-time order in which the events actually occur.

Recall that an event includes (a) the name of an object, (b) the name of a process, (c) the name of an operation as well as the corresponding input or output parameters.

A local history of $p_{i}$, denoted $H \mid p_{i}$, is a projection of $H$ on process $p_{i}$ : the subsequence $H$ consisting of the events generated by $p_{i}$.

Two histories $H$ and $H^{\prime}$ are said to be equivalent if they have the same local histories, i.e., for each process $p_{i}, H\left|p_{i}=H^{\prime}\right| p_{i}$.

As we consider sequential processes, we focus on histories $H$ such that, for each process $p_{i}, H \mid p_{i}$ (the local history generated by $p_{i}$ ) is sequential: the history starts with an invocation, followed by a response, (the matching response associated with the same object) followed by another invocation, etc. We say in this case that the global history $H$ is well-formed.

An operation is said to be complete in a history if the history includes both the event corresponding to the invocation of the operation and its response. If the history contains only the invocation, we say that the operation is pending in that history. A history without pending operations is said to be complete. A history with pending operations is said to be incomplete. Incomplete histories are important to study as they typically model the situation where a process invokes an operation and stops, e.g., crashes, before obtaining a response. Note that, being sequential, a process can have at most one pending operation in a given history.

A history $H$ induces an irreflexive partial order on its operations. Let $o p=X . o p 1()$ by $p_{i}$ and $o p^{\prime}=Y . o p 2()$ by $p_{j}$ be two any operations. Informally, operation op precedes operation $o p^{\prime}$, if op terminates before op starts, where "terminates" and "starts" refer to the time-line abstracted by the $<_{H}$ total order relation. More precisely:

$$
\left(o p \rightarrow_{H} o p^{\prime}\right) \stackrel{\text { def }}{=}\left(r e s p[o p]<_{H} i n v\left[o p^{\prime}\right]\right)
$$

Two operations $o p$ and $o p^{\prime}$ are said to overlap (we also say they are concurrent) in a history $H$ if neither $\operatorname{resp}[o p]<_{H} i n v\left[o p^{\prime}\right]$, nor resp $\left[o p^{\prime}\right]<_{H} i n v[o p]$ (neither precedes the other one). Notice that two overlapping operations are such that $\neg\left(o p \rightarrow_{H} o p^{\prime}\right)$ and $\neg\left(o p^{\prime} \rightarrow_{H} o p\right)$. As sequential histories have no overlapping operations, $\rightarrow_{H}$ is a total order if $H$ is a sequential history.

Figure 2.3 highlights the events involved in the history depicting the execution of Figure 2.2 above. The history contains events $e_{1} \ldots e_{14}$. As all events in $H$ involve the same object, the identity of this object is omitted. The history has no pending operations, and is consequently complete.

If we restrict the history to the sequence of events $e_{1} \ldots e_{12}$, we will obtain an incomplete one: the last dequeue operation of $p_{1}$ as well as the last enqueue of $p_{2}$ are now pending operations in the resulting history.

### 2.2.4. Sequential histories

Definition 1 A history is sequential if its first event is an invocation, and then (1) each invocation event, except possibly the last, is immediately followed by the matching response event, (2) each response event, except possibly the last, is immediately followed by an invocation event.

The precision "except possibly the last" is due to the fact that a history can be incomplete as we discussed earlier. A history that is not sequential is said to be concurrent.

Given that a sequential history $S$ has no overlapping operations, the associated partial order $\rightarrow_{S}$ defined on its operations is actually a total order. Strictly speaking, the sequential specification of an object is a set of sequential histories involving solely that object. Basically, the sequential specification represents all possible sequential accesses to the object.

Figure 2.4 depicts a complete sequential history. This history has no overlapping operations. The operations are totally ordered.

### 2.2.5. Legal histories

As we pointed out, the definition of a linearizable history refers to the sequential specifications of the objects involved in the history. The notion of a legal history captures this idea.

Given a sequential history $H$ and an object $X$, let $H \mid X$ denote the subsequence of $H$ made up of all the events involving only object $X$. We say that $H$ is legal if, for each object $X$ involved in $H, H \mid X$ belongs to the sequential specification of $X$. Figure 2.4 for instance depicts a legal history. It belongs to the sequential specification of the queue. The first dequeue by $p_{1}$ returns a $a$ whereas the second returns a $b$.

### 2.3. Linearizability

Intuitively, linearizability states that a history is correct if the response returned to its invocations could have been obtained by a sequential execution, i.e., according to the sequential specifications of the
objects. More specifically, we say that a history is linearizable if each operation appears as if it has been executed instantaneously at some indivisible point between its invocation event and its response event. This point is called the linearization point of the operation. We define below more precisely linearizability as well as some of its main characteristics.

### 2.3.1. The case of complete histories

For pedagogical reasons, it is easier to first define linearizability for complete histories $H$, i.e., histories without pending operations, and then extend this definition to incomplete histories.

Definition 2 A complete history $H$ is linearizable if there is a history $L$ such that:

1. $H$ and $L$ are equivalent,
2. $L$ is sequential,
3. L is legal, and
4. $\rightarrow_{H} \subseteq \rightarrow_{L}$.

The definition above says that a history $H$ is linearizable if there exist a permutation of $H, L$, which satisfies the following requirements. First, $L$ has to be indistinguishable from $H$ to any process: this is the meaning of equivalence. Second, $L$ should not have any overlapping operations: it has to be sequential. Third, the restriction of $L$ to every object involved in it should belong to the sequential specification of that object: it has to be legal. Finally, $L$ has to respect the real-time occurrence order of the operations in $H$.

In short, $L$ represents a history that could have been obtained by executing all the operations of $H$, one after the other, while respecting the occurrence order of non-overlapping operations in $H$. Such a sequential history $L$ is called a linearization of $H$ or a sequential witness of $H$.

An algorithm implementing some shared object is said to be linearizable if all histories generated by the processes accessing the object are linearizable. Proving linearizability boils down to exhibiting, for every such history, a linearization of the history that respects the "real-time" occurrence order of the operations in the history, and that belongs to the sequential specification of the object. This consists in determining for every operation of the history, its linearization point in the corresponding sequential witness history. To respect the real time occurrence order, the linearization point associated with an operation has always to appear within the interval defined by the invocation event and the response event associated with that operation. It is also important to notice that a history $H$, may have multiple possible linearizations.

Example with a queue. Consider history $H$ depicted on Figure 2.3. Whether $H$ is linearizable or not depends on the values returned by the dequeue invocations of $p_{1}$, i.e., in events $e_{7}$ and $e_{13}$. For example, assuming that the queue is initially empty, two possible values are possible for $e_{7}: a$ and nil.

1. In the first case, depicted on Figure 2.5, the linearization of the first dequeue of $p_{1}$ would be before the first enqueue of $p_{2}$. We depict the linearization, and the corresponding linearization points on Figure 2.6.
2. In the second case, depicted on Figure 2.7, the linearization of the first dequeue of $p_{1}$ would be after the first enqueue of $p_{2}$. We depict the linearization, and the corresponding linearization points on Figure 2.8.

It is important to notice that, in order to ensure linearizability, the only possible values for $e_{7}$ are $a$ and nil. If any other value was returned, the history of Figure 2.7. would not have been linearizable. For instance, if the value was $b$, i.e., if the first dequeue of $p_{1}$ returned $b$, then we could not have found any possible linearization of the history. Indeed, the dequeue should be linearizable after the enqueue of $b$, which is after the enqueue of $a$. To be legal, the linearization should have a dequeue of $a$ before the dequeue of $b-$ a contradiction.

Example with a register. Figure 2.9 highlights a history of two processes accessing a shared register. The history contains events $e_{1} \ldots e_{12}$. The history has no pending operations, and is consequently complete.

Assuming that the register initially stores value 0 , two possible returned values are possible for $e_{5}$ in order for the history to be linearizable: 0 and 1 . In the first case, the linearization of the first read of $p_{1}$ would be right after the first write of $p_{2}$. In the second case, the linearization of the first read of $p_{1}$ would be right after the second write of $p_{2}$.

For the second read of $p_{1}$, the history is linearizable, regardless of whether the second read of $p_{1}$ returns values 1,2 or 3 in event $e_{7}$. If this second read had returned a 0 , the history would not be linearizable.

### 2.3.2. The case of incomplete histories

So far we considered only complete histories. These are histories with at least one process whose last operation is pending: the invocation event of this operation appears in the history while the corresponding response event does not. Extending linearizability to incomplete histories is important as it allows to state what responses are correct when processes crash. We cannot decide when processes crash and then cannot expect from a process to first terminate a pending operation before crashing.

Definition 3 A history $H$ (whether it is complete or not) is linearizable if $H$ can be completed in such a way that every invocation of a pending operation is either removed or completed with a response event, so that the resulting (complete) history $H^{\prime}$ is linearizable.

Basically, this definition transforms the problem of determining whether an incomplete history $H$ is linearizable to the problem of determining whether a complete history $H^{\prime}$, obtained by completing $H$, is linearizable. $H^{\prime}$ is obtained by adding response events to certain pending operations of $H$, as if these operations have indeed been completed, or by removing invocation events from some of the pending operations of $H$. (All complete operations of $H$ are preserved in $H^{\prime}$.) It is important to notice that the term "complete" is here a language abuse as we might "complete" a history by removing some of its pending invocations. It is also important to notice that, given an incomplete history $H$, we can complete it in several ways and derive several histories $H^{\prime}$ that satisfy the required conditions.

Example with a queue. Figure 2.10 depicts an incomplete history $H$. We can complete $H$ by adding to it the response $b$ to the second dequeue of $p_{1}$ and a response to the last enqueue of $p_{2}$ : we would obtain history $H^{\prime}$ of Figure 2.5 which is linearizable. We could also have "completed" $H$ by removing any of the pending operations, or both of them. In all cases, we would have obtained a complete history that is linearizable.

Figure 2.11 also depicts an incomplete history. However, no matter how we try to complete it, either by adding responses or removing invocations, there is no way to determine a linearization of the completed history.

Example with a register. Figure 2.12 depicts an incomplete history of a register. The only way to complete the history in order to make it linearizable is to complete the second write of $p_{2}$. This would enable the read of $p_{1}$ to be linearized right after it.

### 2.3.3. Completing a linearizable history

An interesting characteristic of linearizability is its nonblocking flavour: every pending operation in a history $H$ can be completed without having to wait for any other operation to complete nor sacrificing the linearizability of the resulting history. The following theorem captures this characteristic.

Theorem 1 Let $H$ be any finite linearizable history and inv $[o p]$ any pending operation invocation in $H$. There is a response $r=\operatorname{resp}[o p]$ such that $H \cdot r$ is linearizable.

Proof As $H$ is incomplete and linearizable, there is a completion of $H, H^{\prime}$ that is linearizable, i.e., that has a linearization $L$. of $H$. If $L$ contains $i n v[o p]$ and its matching response $r$, then $L$ is also linearization of $H \cdot r$. If $L$ contains neither $i n v[o p]$ not $r$ (i.e., $H^{\prime}$ does not contain $i n v[o p]$ ), then $L^{\prime}=L \cdot i n v[o p] \cdot r$ is a linearization of $H^{\prime} \cdot i n v[o p] \cdot r$, which means that $H \cdot r$ is linearizable.

### 2.4. Composition

A property is a set of histories. A property $P$ is said to be compositional if it is enough to prove that it holds for each of the objects of a set in order to prove that it holds for the entire set: for each history $H$, we have $\forall X H \mid X \in P$ if and only if $H \in P$. Intuitively, compositionality enables to derive the correctness of a composed system from the correctness of the components. This property is crucial for modularity of programming: a correct (linearizable) compositions can be obtained from correct (linearizable) components.

Theorem 2 A history $H$ is linearizable if and only if, for each object $X$ involved in $H, H \mid X$ is linearizable.

Proof The "only if" direction is an immediate consequence of the definition of linearizability: if $H$ is linearizable then, for each object $X$ involved in $H, H \mid X$ is linearizable. Indeed, for every linearization $S$ of $H, S \mid X$ is a linearization of $H \mid X$.

To prove the other direction, consider a history $H$, where for each object $X, H \mid X$ has a linearization, denoted $S_{X}$, let $\rightarrow_{X}$ denote the total order in $S_{X}$ of the operation on $X$ in $H$. We show below that the relation $\rightarrow=\bigcup_{X}\left\{\rightarrow_{X}\right\} \cup\left\{\rightarrow_{H}\right\}$ does not induce any cycle. This means that its transitive closure is a partial order, and its linear extension $S$ is a linearization of $H$.

Assume by contradiction that $\rightarrow$ contains a cycle. Recall that $\rightarrow_{X}$ and $\rightarrow_{H}$ are transitive. We can thus replace any fragment of the form $o p_{1} \rightarrow_{X} o p_{2} \rightarrow_{X} o p_{3}$ (respectively, $o p_{1} \rightarrow_{H} o p_{2} \rightarrow_{H} o p_{3}$ ) with $o p_{1} \rightarrow_{X} o p_{3}$ (respectively, $o p_{1} \rightarrow_{H} o p_{3}$ ). Moreover, since every operation concerns exactly one object, the cycle cannot contain fragments of the form $o p_{1} \rightarrow_{X} o p_{2} \rightarrow_{Y} o p_{3}$ for $X \neq Y$. Hence, the cycle alternate edges of the form $\rightarrow_{X}$ with edges $\rightarrow_{H}$.

Now consider the fragment $o p_{1} \rightarrow_{H} o p_{2} \rightarrow_{X} o p_{3} \rightarrow_{H} o p_{4}$. Recall that $\rightarrow_{X}$ is the order of operations in $S_{X}$, a linearization of $H \mid X$. Since $S_{X}$ respect real time, we have $o p_{3} \nrightarrow_{X} o p_{2}$, i.e., the invocation of $o p_{2}$ precedes the response of $o p_{3}$ in $H \mid X$ (and, thus, in $H$ ). Since $o p_{1} \rightarrow_{H} o p_{2}$, the response of $o p_{1}$ precedes the invocation of $o p_{2}$ and, thus, the response of $o p_{3}$. Since $o p_{3} \rightarrow_{H} o p_{4}$, the response of $o p_{3}$ and, thus, the response of $o p_{1}$ precedes the invocation of $o p_{4}$ in $H$. Hence, $o p_{1} \rightarrow_{H} o p_{4}$, i.e., we can shorten the fragment to one edge $\rightarrow_{H}$. By eliminating all edges of the form $\rightarrow_{X}$ we obtain a cycle of edges $\rightarrow_{H}$-a contradiction with the definition of $\rightarrow_{H}$ based on the real-time precedence between operations in $H$ that cannot induce cycles.

Hence the transitive closure of $\rightarrow$ is irreflexive and anti-symmetric and, thus, has a linear extension: a total order on operations in $H$ that respects $\rightarrow_{H}$ and $\rightarrow_{X}$, for all $X$. Consider the sequential history $S$ induced by any such total order. Since, for all $X, S \mid X=S_{X}$ and $S_{X}$ is legal, $S$ is legal. Since $\rightarrow_{H} \subseteq \rightarrow_{S}, S$ respects the real-time order of $H$. Finally, since each $S_{X}$ is equivalent to a completion of $H \mid X, S$ is equivalent to a completion of $H$, where each incomplete operation on an object $X$ is completed in the way it is completed in $S_{X}$. Hence, $S$ is a linearization of $H$.

## The importance of real time

Linearizability stipulates correctness with respect to a sequential execution: an operation needs to appear to take effect instantaneously, respecting the sequential specification of the object. In this respect, linearizability is similar to sequential consistency, a classical correctness criteria for shared objects. There is however a fundamental difference between linearizability and sequential consistency, and this difference is crucial to making linearizability compositional, which is not the case for sequential consistenty, as we explain below.

Sequential consistency is a relaxation of linearizability. It only requires that the real-time order is preserved if the operations are invoked by the same process, i.e., $S$ is only supposed to respect the process-order relation.

More specifically, a history $H$ is sequentially consistent if there is a "witness" history $S$ such that:

1. $H$ and $S$ are equivalent,
2. $S$ is sequential and legal.

Both linearizability and sequential consistency require a witness sequential history. However, and as we pointed out, sequential consistency has no further requirement related to the occurrence order of operations issued by different processes (and captured by the real-time order). It is based only on a logical time (the one defined by the witness history). In some sense, with linearizablity, after $p_{1}$ has finished its operation en enqueued element $a, p_{1}$ could "call" $p_{2}$ and inform it about the availability of "a": $p_{2}$ will then be sure to find $a$. Everything happens as if indeed the enqueue of $a$ was executed at a single point in time.

Clearly, any linearizable history is also sequentially consistent. The contrary is not true. A major drawback of sequential consistency is that it is not compositional. To illustrate this, consider the counterexample described in Figure 2.13. The history $H$ depicted in the picture involves two processes $p_{1}$ and $p_{2}$ accessing two shared registers $R_{1}$ and $R_{2}$. It is easy to see that the restriction $H$ to each of the registers is sequentially consistent. Indeed, concerning register $R_{1}$, we can re-order the read of $p_{1}$ before the write of $p_{2}$ to obtain a sequential history that respects the semantics of a register (initialized to 0 ). This
is possible because the resuting sequential history does not need to respect the real-time ordering of the operations in the original history. Note that the history restricted to $R_{1}$ is not linearizable. As for register $R_{2}$, we simply need to order the read of $p_{1}$ after the write of $p_{2}$.

Nevertheless, the system composed of the two registers $R_{1}$ and $R_{2}$ is not sequentially consistent. In every legal equivalent to $H$, the write on $R_{2}$ performed by $p_{2}$ should precede the read of $R_{2}$ performed by $p_{1}: p_{1}$ reads the value written by $p_{2}$. If we also want to respect the process-order relation of $H$ on $p_{1}$ and $p_{2}$, we obtain the following sequential history: $p_{2}$.Write $R_{1}(1) ; p_{2}$.Write $R_{2}(1) ; p_{1} \cdot \operatorname{Read}_{R_{2}}() 1$; $p_{1} \cdot \operatorname{Read}_{R_{1}}() 0$. But the history is not legal: the value read by $p_{1}$ in $R_{1}$ is not the last written value.

### 2.5. Safety

It is convenient to reason about the correctness of a shared object implementation by splitting its properties into safety and liveness. Intuitively, safety properties ensure that nothing "bad" is ever going to happen whilst liveness properties guarantee that something "good" eventually happens.

More specifically, a property is a set of (finite or infinite) histories. Now a property $P$ is a safety property if:

- $P$ is prefix-closed: if $H \in P$, then for every prefix $H^{\prime}$ of $H, H^{\prime} \in P$.
- $P$ is limit-closed: for every infinite sequence $H_{0}, H_{1}, \ldots$ of histories, where each $H_{i}$ is a prefix of $H_{i+1}$ and each $H_{i} \in P$, the limit history $H=\lim _{i \rightarrow \infty} H_{i}$ is in $P$.

Knowing that a property is a safety one helps prove it in the following sense. To ensure that a safety property $P$ holds for a given implementation, it is enough to show that every finite history is in $P$ : a history is in $P$ if and only if each of its finite prefixes is in $P$. Indeed, every infinite history of an implementation is the limit of some sequence of ever-extending finite histories and thus should also be in $P$.

Theorem 3 Linearizability is a safety property.

The proof of Theorem 3 uses a slight generalization of König's infinity lemma formulated as follows:

Lemma 1 (König's Lemma) Let $G$ be an infinite directed graph such that (1) each node of $G$ has finite outdegree, (2) each vertex of $G$ is reachable from some root vertex of $G$ (a vertex with zero indegree), and (3) $G$ has only finitely many roots. Then $G$ has an infinite path with no repeated nodes starting from some root.

Now we prove Theorem 3, i.e., we show that the set of linearizable histories is prefix- and limit-closed. Recall that we only consider objects with finite non-determinism: an operation applied to a given object state may return only finitely many responses and cause only a finite number of state transitions.

Proof Consider a linearizable history $H$. Since linearizability is compositional, we can simply assume that $H$ is a history of operations on a single (composed) object $X$. We show first that any $H^{\prime}$, a prefix of $H$, is also linearizable (with respect to $X$ ).

Let $S$ be any linearization of $H$, i.e., a sequential legal history that is equivalent to (a completion of $H)$ and respects the real-time order of $H$. Now we construct a sequential history $S^{\prime}$ as follows: we take the shortest prefix of $S$ that contains all complete operations of $H^{\prime}$. Since $S$ contains all compete operations of $H^{\prime}$, such a prefix of $S$ exists.

We claim that $S^{\prime}$ is a linearization of $H^{\prime}$. Indeed, let us complete $H^{\prime}$ by removing operations that do not appear in $S^{\prime}$ and adding responses to incomplete operations in $H^{\prime}$ that are present in $S^{\prime}$. This way only incomplete operations are removed from $H^{\prime}$ since, by construction, all operations that are complete in $H^{\prime}$ appear in $S^{\prime}$. Let $\bar{H}^{\prime}$ denote the resulting complete history.

First we observe that complete histories $S^{\prime}$ and $\bar{H}^{\prime}$ consist the same set of operations. By construction, every operation in $\bar{H}^{\prime}$ appears in $S^{\prime}$. Now suppose, by contradiction, that $S^{\prime}$ contains an operation op that does not appear in $\bar{H}^{\prime}$. Since only operations that do not appear in $S^{\prime}$ were removed from $H^{\prime}$ to obtain $\bar{H}^{\prime}$, op does not appear in $H^{\prime}$ either. Since $S^{\prime}$ is the shortest prefix of $S$ that contains all complete operations of $H, o p$ cannot be the last operation appearing in $S^{\prime}$. Moreover, for the same reason, the last operation in $S^{\prime}$ must be complete in $H^{\prime}$, let us denote this operation by $o p^{\prime}$. Since $o p$ does not appear in $H^{\prime}$ and $o p^{\prime}$ is complete in $H^{\prime}$, we have $o p^{\prime}<_{H} o p$. But $o p$ precedes $o p^{\prime}$ in $S^{\prime}$ (and, thus, in $S$ ), i.e., $o p<_{S} o p^{\prime}$. Hence, $S$ violates the real-time order of $H-$ a contradiction.

Since $S^{\prime}$ is a prefix of a legal history it is also legal. Moreover, $S^{\prime}$ and $\bar{H}^{\prime}$ contain the same set of operations and $S^{\prime}$ respects the real-time order in $\bar{H}^{\prime}$ : if $<_{\bar{H}^{\prime}} \subseteq<_{S^{\prime}}$ (otherwise, $S$ would violate the real-time order in $H$ ).

Consider any local history $\bar{H}^{\prime} \mid p_{i}$. Recall that we only assume well-formed histories and, thus, $\bar{H}^{\prime} \mid p_{i}$ is sequential. Since $S^{\prime}$ and $\bar{H}^{\prime}$ contain the same set of operations and $S^{\prime}$ respects the real-time order of $\bar{H}^{\prime}$, we have $S^{\prime}\left|p_{i}=\bar{H}^{\prime}\right| p_{i}$. Hence, $S^{\prime}$ and $\bar{H}^{\prime}$ are equivalent.

Thus, $S^{\prime}$ is indeed a linearization of $H^{\prime}$ and, thus, linearizability is prefix-closed.

To show that linearizability is limit-closed, we consider an infinite sequence of ever-extending linearizable histories $H_{0}, H_{1}, H_{2}, \ldots$. Our goal is to show that $H=\lim _{i \rightarrow \infty} H_{i}$ is linearizable. We assume that $H_{0}$ is the empty history and each $H_{i+1}$ is a one-event extension of $H_{i}$ (by prefix-closedness, prefix of every $H_{i}$ is linearizable, so we do not lose generality this way).

Now we construct a directed graph $G=(V, E)$ as follows. Vertices of $G$ are all tuples $\left(H_{i}, S, Q\right)$, where $i=0,1, \ldots,|H|, S$ is any linearization of $H_{i}$ that ends with a complete operation present in $H_{i}$, and $Q$ is any sequence of object states that witnesses the legality of $H$. Now there is an directed edge $\left(\left(H_{i}, S, Q\right),\left(H_{j}, S^{\prime}, Q^{\prime}\right)\right.$ in $G$ if and only if $j=i+1, S$ is a prefix of $S^{\prime}$ and $Q$ is a prefix of $Q^{\prime}$.

Note that each $H_{i}$ has at least one vertex $\left(H_{i}, S, Q\right)$. Indeed, by taking any linearization of $H_{i}$ and removing operations at the end of it that are incomplete in $H_{i}$, we obtain a linearization of a completion of $H_{i}$ in which these operations are removed. Thus, there exists a linearization $S$ of $H_{i}$ that ends with a complete operation in $H_{i}$. Since $S$ is legal, it must have a witness sequence of states $Q$.

We use König's lemma to show that the resulting graph $G$ contains an infinite path $\left(H_{0}, S_{0}\right),\left(H_{1}, S_{1}\right), \ldots$ and the limit $\lim _{i \rightarrow \infty} S_{i}$ is a linearization of the infinite limit history $H$.

First we observe that each non-empty vertex $\left(H_{i+1}, S^{\prime}, Q^{\prime}\right)$ is connected to some $\left(H_{i}, S, Q\right)$. There are two cases to consider:

- The last operation $o p$ of $S^{\prime}$ is a complete operation in $H_{i}$. In this case, $S^{\prime}$ is also a linearization of $H_{i}$. Indeed, even if the last event of $H_{i+1}$ is the invocation of a new operation $o p^{\prime}$, this operation cannot appear in $S^{\prime}$ : it can only appear before $o p$ in $S^{\prime}$ violating the real-time order in $H_{i+1}$. Thus, $\left(H_{i}, S^{\prime}, Q^{\prime}\right)$ is a vertex in $G$.
- The last operation $o p$ of $S^{\prime}$ is not a complete operation in $H_{i}$. Recall that $S^{\prime}$ ends with an operation
op that is complete in $H_{i+1}$ and $H_{i+1}$ extends $H_{i}$ with one event only. Thus, the last event of $H_{i+1}$ is the response of op. Thus, $H_{i}$ and $H_{i+1}$ contain the same set of operations, except that $o p$ is incomplete in $H_{i}$. Let $S$ be the longest prefix of $S^{\prime}$ that ends with a complete operation in $H_{i}$. Since $S^{\prime}$ is legal, $S$ is also legal. By construction, every complete operation in $H_{i}$ appears in $S$ and no operation appears in $S$ if it does not appear in $H_{i}$. Thus, $S$ is a linearization of $H_{i}$ and $\left(H_{i}, S, Q\right)$, where $Q$ is the prefix of $Q^{\prime}$ that witnesses the legality of $S$, is a vertex in $G$.

Inductively, we derive that each vertex $\left(H_{i}, S, Q\right)$ is reachable from vertex $\left(H_{0}, S_{0}, Q_{0}\right)$, where $H_{0}$, $S_{0}$ and $W_{0}$ are empty sequences. The only root vertex of $G$ (a vertex that has no incoming edges) is thus $\left(H_{0}, S_{0}, W_{0}\right)$.

Now we show that the outdegree of every vertex of $G$ is finite. There are only finitely many operations in $H_{i+1}$ and each linearization of $H_{i+1}$ is a permutation of these operations. Thus, since we only consider objects with finite non-determinism, there can only be finitely many vertices of the form $\left(H_{i+1}, S^{\prime}, Q^{\prime}\right)$. Since all outgoing edges of any vertex $\left(H_{i}, S, Q\right)$ are directed to vertices of the form $\left(H_{i+1}, S^{\prime}, Q^{\prime}\right)$, the outdegree of every such vertex is also finite.

By König's lemma, $G$ contains an infinite path starting from the root vertex: $\left(H_{0}, S_{0}, Q_{0}\right),\left(H_{1}, S_{1}, Q_{1}\right), \ldots$. We argue now that the limit $S=\lim _{i \rightarrow \infty} S_{i}$ is a linearization of the infinite limit history $H$. By construction, $S$ respects the real-time order of $H$, otherwise there would be a vertex $\left(H_{i}, S_{i}, Q_{i}\right)$ such that $S_{i}$ is not equivalent to $H_{i}$ or violates the real-time order of $H_{i}$. Also, $S$ contains all complete operations of $H$ and, thus, $S$ is equivalent to a completion of $H . S$ is also legal since each of its prefixes is legal. Thus, $S$ is indeed a linearization of $H$, which concludes the proof that linearizability is a safety property.

Thus, the set of linearizable histories is indeed prefix-closed and limit-closed, so in the rest of this book, we only consider finite histories in the proofs of linearizability.

### 2.6. Summary

This chapter studies the meaning of the notion of a correct object implementation. Namely, to be correct, all histories generated by the object implementation need to be linearizable. The responses returned by the object in a concurrent history are those that could have been returned by the object if accessed sequentially. Proving this typically boils down to determining a linearization point for each operation in the given history.

Linearizability has some important characteristics. First, it reduces the difficult problem of reasoning about a concurrent system into the problem of reasoning about a sequential one. We simpy need a sequential specification of an object to reason about the correctness of a system made of processes concurrently accessing that object. Linearizabiliy is also compositional. It is enough to prove that each object in a set (of objects) is linearizable to conclude that the system composed of the set is linearizable. Linearizability is also non-blocking, which basically means that ensuring it never forces processes to wait for each other.

As pointed out however, linearizability is only a partial answer to the question of correctness. It does say what response should be forbidden to be returned by an object but does not say when the object should actually return some response. In fact, and as we will see in the next chapter, to be considered correct, the object implementation should not only be linearizable but should also be wait-free. Whilst linearizability covers safety, wait-freedom covers liveness.

