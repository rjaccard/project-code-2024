# 4. PROBABILISTIC INFORMATION RETRIEVAL 

The notion of similarity in the vector space model does not directly imply relevance

- The similarity values have no interpretation, they are just used to rank
- An information retrieval model deals with uncertainty on the users information needs
- Probability theory provides a principled approach to reason about this uncertainty

Probabilistic IR models attempt to directly model relevance as a probability

One of the key drawbacks of the vector space retrieval model is the lack of interpretability of the similarity values. This gave rise to the development of probabilistic retrieval models, that attempt to "compute" relevance as a probability.

## Query Likelihood Model

Given query $q$, determine the probability $P(d \mid q)$ that document $d$ is relevant to query $q$

Bayes Rule $P(d \mid q)=\frac{P(q \mid d) P(d)}{P(q)}$

Assumptions

- $\quad P(d)$, the probability of a document occurring is uniform across a collection
- $P(q)$ is the same for all documents

Thus: $P(d \mid q)$ can be derived from $P(q \mid d)$

The problem of retrieval can be understood in a probabilistic setting as the problem of determining the probability of a document $d$ being relevant, given a query q. We observe that the probability of a document to occur in a collection is constant (which makes sense assuming all documents are different), and the probability of a query to occur is the same for all documents. Thus, using Bayes rule, the problem of determining whether a document is relevant for a query is equivalent to the problem of determining whether a query is relevant to a document. The latter probability $\mathrm{P}(\mathrm{q} \mid \mathrm{d})$ is also called the query likelihood.

## Language Modeling

Query likelihood: determine $P(q \mid d)$

Assume each document $d$ is generated by a Language Model $M_{d}$

- a language model is a mechanism that generates the words of the language

Then $P(q \mid d)$ can be interpreted as the probability that the query $q$ was generated by the language model $M_{d}$

The notion of query likelihood gives now rise to the following approach to model relevance. We assume that documents are the result of language model. A language model is a (in general probabilistic) process that produces text, and a given document $\mathrm{d}$ is assumed to be produced by its specific language model $\mathrm{M}_{\mathrm{d}}$. Then the problem of retrieval can be viewed in the following way: if a query is relevant to a document, it should have been produced by the same language model as the document. Using this argument, the query likelihood corresponds to the probability that the query has been produced by the same language model as the document.

Let's have now a more detailed look in what a language model is and how we use it implement this intuitive model practically.

## What is a Language Model?

## Deterministic language model $=$ automaton $=$ grammar

In the simplest case a language model is a deterministic automaton. In theoretical computer science deterministic automatons are those that can recognize (or produce) regular languages.

## Probabilistic Language Model

Unigram model: assign a probability to each term to appear

- More complex models can be used, e.g., bigrams

| Model $\mathbf{M}_{\mathbf{1}}$ |  | Model $\mathbf{M}_{\mathbf{2}}$ |  |
| :--- | :--- | :--- | :--- |
| STOP | 0.2 | STOP | 0.25 |
| the | 0.2 | the | 0.15 |
| a | 0.1 | a | 0.12 |
| frog | 0.03 | frog | 0.0002 |
| toad | 0.03 | toad | 0.0001 |
| said | 0.02 | said | 0.01 |
| likes | 0.015 | likes | 0.01 |
| dog | 0.01 | dog | 0.04 |

Two different language models derived from 2 documents

Instead of using a deterministic automaton, we can also use a probabilistic state automaton, in other words, a Markov process. In the simplest case the automaton has a single state, and every state transition emits with a certain probability one term out of a vocabulary. In addition, the automaton can stop with a certain probability. The table captures the transition probabilities of two possible models M1 and M2. In the two models, the probability to stop is given as $\mathrm{P}(\mathrm{STOP} \mid \mathrm{Q})=0.2$.

## Probability to Create a Query

What is the probability that a query $q$ has been generated by model $M$

Example: $q=$ the frog said dog STOP

$P\left(q \mid M_{1}\right)=0.2 * 0.03 * 0.02 * 0.01 * 0.2=0.00000024$

So retrieval becomes the problem of computing for a query $q$ the probability $P\left(q \mid M_{d}\right)$ for all the documents $d$

Given a language model for the generation of documents, we can now compute within that model the probability that a given query $q$ has been generated by the model of a document $\mathrm{d}$. We give one example showing such a computation. With this approach we are now ready to compute query likelihood for all documents of a document collection.

## Learning and Using the Model

Learning the model: Maximum Likelihood Estimation (MLE) of probabilities under Unigram Model

$$
\hat{P}_{m l e}\left(t \mid M_{d}\right)=\frac{tf_{t, d}}{L_{d}}
$$

where

- $t f_{t, d}$ is the number of occurrences of $t$ in $d$ (term frequency)
- $L_{d}$ is the number of terms in the document (document length)

Using the model

$$
\hat{P}\left(q \mid M_{d}\right)=\prod_{t \in q} \hat{P}_{m l e}\left(t \mid M_{d}\right)
$$

For applying the probabilistic retrieval method described before, we need first to learn the language model of each document. The learning is performed using Maximum Likelihood Estimation (MLE). In the case of the unigram model, this is a straightforward task. We just estimate the term probabilities by counting the document frequencies and normalizing by document length. When using the model for a query $\mathrm{q}$, we then use those estimates, to estimate the relevance of a query for the document, as illustrated before.

## Issues with MLE Estimation

Problem 1: if query contains a term not occurring in the document $\hat{P}\left(q \mid M_{d}\right)=0$ !

Problem 2: this is an estimation! A term that occurs once, might have been "lucky", whereas another one with same probability to occur is not contained in the document

need to give non-zero probability to unseen terms!

Applying the afore mentioned approach to estimate relevance of a document to a query has a practical problem: if the query contains a term not occurring in the document the estimated probability will be unavoidably zero, since one of the factors of the product computing that probability will be zero. In other words, the query cannot be generated by the document model, thus the document is not relevant to the query. This is not only impractical, but also not meaningful from a more theoretical perspective. Since we used MLE to generate the model, we were using the statistics of one specific document, that has been generated by a potentially complex model, that may contain other terms that just were not generated for this document.

## Smoothing

Idea: add a small weight for non-occurring terms in a document, that is smaller than the normalized collection frequency

$$
\hat{P}\left(t \mid M_{c}\right) \leq c f_{t} / T
$$

where

- $c f_{t}=$ number of times term $t$ occurs in collection
- $T=$ total number of terms in collection


## Smoothed estimate

$$
\hat{P}(t \mid d)=\lambda \hat{P}_{m l e}\left(t \mid M_{d}\right)+(1-\lambda) \hat{P}_{m l e}\left(t \mid M_{c}\right)
$$

$M_{c}=$ language model of the whole collection

$\lambda=$ tuning parameter

To fix the aforementioned problem an approach called smoothing is applied. The basic idea is to assume that in fact every term potentially could occur in the document generated by its document model, including those that are not part of the actual document; only that the probability of terms not seen in the document is presumably less likely to occur as it would be expected to occur in the overall document collection. The smoothed estimate then combines the estimated likelihood to occur in the document according to the model generated from the document, with the estimated likelihood of a term occurring in the general document collection, modeled as a generic language model using the statistics from the document collection.

## Probabilistic Retrieval

With smoothing the relevance is computed as

$$
P(d \mid q) \propto P(d) \prod_{t \in q}\left((1-\lambda) P\left(t \mid M_{c}\right)+\lambda P\left(t \mid M_{d}\right)\right)
$$

From a technical perspective the probabilities are computed using term frequencies, thus same data used as in vector space retrieval

Probabilistically motivated models show generally better performance

- But parameter tuning $(\lambda)$ is critical
- $\lambda$ can be query-dependent, e.g., query size

Here we summarize the approach for probabilistic retrieval. From a more technical perspective computational cost of probabilistic retrieval is not very different from vector space retrieval. The computation of the likelihoods for the document models requires determination of term frequencies, so in that sense it is equivalent. For the collection models the global term frequencies need to be computed, which again is similar to computing inverse document frequencies in a document collection.

In practice, the fine tuning of the model parameters (in that case $\lambda$ ) is essential for that the model performs well. Different methods have been devised for that. It is also possible to make the parameters dependent on the query, in particular on the query size.

## Example

## Collection consisting of $d_{1}$ and $d_{2}$

 $d_{1}$ : Einstein was one of the greatest scientists $\mathrm{d}_{2}$ : Albert Einstein received the Nobel prize
## Query q: Albert Einstein

$$
\begin{aligned}
& \text { Using } \lambda=1 / 2 \text { : } \\
& \quad P\left(q \mid d_{1}\right)=1 / 2 *(0 / 7+1 / 13) * 1 / 2 *(1 / 7+2 / 13) \approx 0.0057 \\
& P\left(q \mid d_{2}\right)=1 / 2 *(1 / 6+1 / 13) * 1 / 2 *(1 / 6+2 / 13) \approx 0.0195
\end{aligned}
$$

This is a simple example illustrating the use of probabilistic retrieval. Notate that the document lengths of $\mathrm{d} 1$ and $\mathrm{d} 2$ are 7 and 6 , and that the collection length is 13 .

## Example: Comparing VS and PR

| Precision |  |  |  |  |  |
| :--- | :--- | :--- | ---: | ---: | ---: |
| Rec. | tf-idf | LM | \%chg |  |  |
| 0.0 | 0.7439 | 0.7590 | +2.0 |  |  |
| 0.1 | 0.4521 | 0.4910 | +8.6 |  |  |
| 0.2 | 0.3514 | 0.4045 | +15.1 | $*$ |  |
| 0.3 | 0.2761 | 0.3342 | +21.0 | $*$ |  |
| 0.4 | 0.2093 | 0.2572 | +22.9 | $*$ |  |
| 0.5 | 0.1558 | 0.2061 | +32.3 | $*$ |  |
| 0.6 | 0.1024 | 0.1405 | +37.1 | $*$ |  |
| 0.7 | 0.0451 | 0.0760 | +68.7 | $*$ |  |
| 0.8 | 0.0160 | 0.0432 | +169.6 | $*$ |  |
| 0.9 | 0.0033 | 0.0063 | +8.3 |  |  |
| 1.0 | 0.0028 | 0.0050 | +76.9 |  |  |
| Ave | 0.1868 | 0.2233 | +19.55 | $*$ |  |

This is a result reported from comparing vector space retrieval with probabilistic retrieval. It shows that in this experiment probabilistic retrieval improves precision significantly, in particular for higher values of recall. ( $\mathrm{LM}=$ language model).

## Overview of Retrieval Model Properties

|  | Vector Space Model | Language Model | BM25 (another prob. <br> Model) |
| :--- | :--- | :--- | :--- |
| Model | geometric | probabilistic | probabilistic |
| Length normalization | Requires extensions <br> (pivot normalization) | Inherent to model | Tuning parameters |
| Inverse document <br> frequency | Used directly | Smoothing and <br> collection frequency <br> has similar effect | Used directly |
| Multiple term <br> occurrences | Taken into account | Taken into account | Ignored |
| Simplicity | No tuning required | Tuning essential | Tuning essential |

Here we compare the characteristics of the vector space model with the probabilistic retrieval model based on language models, and BM25 another model based on a probabilistic approach, that is today considered as one of the most performant retrieval models.

One aspect that is taken implicitly care off in the probabilistic retrieval model based on language models is normalization for document length. For vector space retrieval specific extensions have been developed, that modify the weighting parameters with the document length. For collections with widely varying document lengths this proved to be a useful improvement. In general, the vector space model is preferred when a quick and simple solution is sought. For probabilistic models better performance can be achieved, but this depends on careful parameter tuning which requires specialized expertise.

## 5. INDEXING FOR INFORMATION RETRIEVAL

## Architecture of Text Retrieval Systems

This figure illustrates the basic architecture with the different functional components of a text retrieval system. We can distinguish three main groups of components:

1. the feature extraction component: it performs text processing to turn queries and text documents into a keyword-based representation
2. the ranking system: it implements the retrieval model. In a first step user queries are potentially modified (in particular if user relevance feedback is used), then the documents required for producing the result are retrieved from the database and finally the similarity values are computed according to the retrieval model in order to compute the ranked result.
3. the data access system: it supports the ranking system by efficiently retrieving documents containing specific keywords from large document collections. The standard technique to implement this component is called inverted files.

In addition we recognize two components to interface the system to the user on the one hand, and to the data collection on the other hand.

## Term Search

Problem: text retrieval algorithms need to find words in documents efficiently

- Boolean retrieval, probabilistic and vector space retrieval
- Given index term $\mathrm{k}_{\mathrm{i}}$, find document $\mathrm{d}_{\mathrm{j}}$

In order to implement text retrieval models efficiently, efficient search for term occurrences in documents must be supported. For that purpose different indexing techniques exist, among which inverted files are the by far most widely used.

## Inverted Files

An inverted file is a word-oriented mechanism for indexing a text collection in order to speed up the term search task

- Addressing of documents and word positions within documents
- Most frequently used indexing technique for large text databases
- Appropriate when text collection is large and semi-static

Inverted files support efficient addressing of words within documents. Inverted file are optimized for supporting search on relatively static text collections. For example, frequent updates are not supported with inverted files. This distinguishes inverted files from typical database indexing techniques, such as B+-Trees.

## Inverted Files

## Inverted list $I_{k}$ for a term $k$

$$
l_{k}=\left[f_{k}: d_{i_{1}}, \ldots, d_{i_{f_{k}}}\right]
$$

$-f_{k}$ number of documents in which $k$ occurs

$-d_{i 1}, \ldots, d_{i f k}$ list of document identifiers of documents containing $k$

Inverted File: lexicographically ordered sequence of inverted lists

$$
I F=\left[i, k_{i}, l_{k_{i}}\right], i=1, \ldots, m
$$

Inverted files are constructed by concatenating the inverted lists for all terms occurring in the document collection. Inverted lists enumerate all occurrences of the terms in documents, by keeping the document identifiers and the frequency of occurrence. Storing the frequency is useful for determining term frequency and inverse document frequency.

## Physical Organization of Inverted Files

Inverted files are a logical data structure, for which a physical storage organization needs to be designed. The physical organization has to take into account the quantitative characteristics of the inverted file structure. To that extent the key observation is that the number of references to documents, corresponding to the occurrences of index terms in the documents is much larger than the number of index terms, and thus the number of inverted lists. In fact, for a document collection of size $n$ the number of occurrences of index terms is $O(n)$, whereas the number of different index terms is typically $O\left(n^{\wedge} \beta\right)$, where $\beta$ is roughly 0.5 (Heap's law). For example, a document collection of size $\mathrm{n}=10^{\wedge} 6$ would have approximate $\mathrm{m}=10^{\wedge} 3$ index terms. Therefore the index terms and the corresponding frequencies of occurrences can be kept in main memory, whereas the references to documents are kept in secondary storage. Index terms and their frequencies are stored in an index file that is kept in main memory. The access to this index file is supported by any suitable data access structure. Typically binary search, hash tables or tree-based structures, such as B+-Trees, or tries are used for that purpose. The posting files consist of the sequence of all term occurrences of the inverted file. The index file is related to the posting file by keeping for each index term a reference to the position in the posting file, where the entries
related to the index terms start. The occurrences stored in the posting file in turn refer to entries in the document file, which is also kept in secondary storage.

## Searching the Inverted File

## Step 1: Vocabulary search

- the words present in the query are searched in the index file


## Step 2: Retrieval of occurrences

- the lists of the occurrences of all words found are retrieved from the posting file


## Step 3: Manipulation of occurrences

- the occurrences are processed in the document file to process the query

Search in an inverted file is a straightforward process. Using the data access structure, first the index terms occurring in the query are searched in the index file. Then the occurrences can be sequentially retrieved from the postings file. Afterwards the corresponding document portions are accessed and can be processed (e.g. for counting term frequencies).

## Construction of the Inverted File - Step 1

## Step 1: Search phase

- The vocabulary is kept in an ordered data structure, e.g. a trie or sorted array, storing for each word a list of its occurrences
- Each word of the text is read sequentially and searched in the vocabulary
- If it is not found, it is added to the vocabulary with an empty list of occurrences
- The word position is added to the end of its list of occurrences

The index construction is performed by first constructing dynamically a trie structure, in order to generate a sorted vocabulary and to collect the occurrences of index terms.

## Construction of the Inverted File - Step 2

Step 2: Storage phase (once the text is exhausted)

- The list of occurrences is written contiguously to the disk (posting file)
- The vocabulary is stored in lexicographical order (index file) in main memory together with a pointer for each word to its list in the posting file

Overall cost $O(n)$

After the complete document collection has been traversed, the trie structure is sequentially traversed and the posting file is written to secondary storage. The trie structure itself can be used as a data access structure for the index file that is kept in main memory.

## Index Construction in Practice

When using a single node not all index information can be kept in main memory $\rightarrow$ Index merging

- When no more memory is available, a partial index $I_{i}$ is written to disk
- The main memory is erased before continuing with the rest of the text
- Once the text is exhausted, a number of partial indices $I_{i}$ exist on disk
- The partial indices are merged to obtain the final index

On a single node machine the index construction will be inefficient or impossible if the size of the trie structure with the associated posting lists exceeds the main memory space. Then the index construction process has to be partitioned in the following way: while the document collection is sequentially traversed, partial indices are written to the disk whenever the main memory is full. This results in a number of partial indices, indexing consecutive partitions of the text. In a second phase the partial indices need to be merged into one index.

## Index Merging

This figure illustrates the merging process: 8 partial indices have been constructed. Step by step the indices are merged, by merging two indices into one, until one final index remains. The merging can be performed, such that the two partial indices which are to be merged are in parallel sequentially scanned on the disk, and while scanning the resulting index is written sequentially to the disk.

## Example

Merging the indices requires first merging the vocabularies. As we mentioned earlier, the vocabularies are comparably small and thus the merging of the vocabularies can take place in main memory. In case a vocabulary term occurs in both partial indices, their list of occurrences from the posting file need to be combined. Here we can take advantage of the fact that the partial indices have been constructed by sequentially traversing the document file. Therefore these lists can be directly concatenated without sorting.

The total computational complexity of the merging algorithm is $O(n$ $\left.\log _{2}(n / M)\right)$. This implies that the additional cost of merging as compared to the purely main memory based construction of inverted files is a factor of $\left.\mathrm{O}\left(\log _{2}(n / M)\right)\right)$. This is small in practice, e.g., if the database size $n$ is 64 times larger than the main memory size, then this factor would be 6 .

This example illustrates how the merging process can be performed for example when the database is partitioned into two parts.

## Addressing Granularity

Documents can be addressed at different granularities

- coarser: text blocks spanning multiple documents
- finer: paragraph, sentence, word level

General rule

- the finer the granularity the less post-processing but the larger the index

Example: index size in $\%$ of document collection size

| Index | Small collection <br> (1Mb) | Medium collection <br> (200Mb) | Large collection <br> (2Gb) |
| :--- | :--- | :--- | :--- |
| Addressing words | $73 \%$ | $64 \%$ | $63 \%$ |
| Addressing documents | $26 \%$ | $32 \%$ | $47 \%$ |
| Addressing 256K blocks | $25 \%$ | $2.4 \%$ | $0.7 \%$ |

The posting file has the by far largest space requirements. An important factor determining the size of an inverted file is the addressing granularity used. The addressing granularity determines of how exactly positions of index terms are recorded in the posting file. There exist three main options:

-Exact word position

-Occurrence within a document

-Occurrence within an arbitrary sized block = equally sized partitions of the document file spanning probably multiple documents

The larger the granularity, the fewer entries occur in the posting file. In turn, with coarser granularity additional post-processing is required in order to determine exact positions of index terms.

Experiments illustrate the substantial gains that can be obtained with coarser addressing granularities. Coarser granularities lead to a reduction of the index size for two reasons:

-a reduction in pointer size (e.g. from 4 Bytes for word addressing to 1 Byte with block addressing)

-and a lower number of occurrences.

Note that in the example for a 2GB document collection with $256 \mathrm{~K}$ block addressing the index size is reduced by a factor of almost 100 .

## Index Compression

Documents are ordered and each document identifier $d_{i j}$ is replaced by the difference to the preceding document identifier

- Document identifiers are encoded using fewer bits for smaller, common numbers

$$
\begin{aligned}
& l_{k}=\left\langle f_{k}: d_{i_{1}}, \ldots, d_{i_{k}}\right\rangle \rightarrow \\
& l_{k}^{\prime}=\left\langle f_{k}: d_{i_{1}}, d_{i_{2}}-d_{i_{1}}, \ldots, d_{i_{j_{k}}}-d_{i_{j_{k}}-1}\right\rangle
\end{aligned}
$$

- Use of varying length compression further reduces space requirement

| $\mathbf{X}$ | $\boldsymbol{\operatorname { c o d e }}(\mathbf{X})$ |
| :--- | :--- |
| 1 | 0 |
| 2 | 100 |
| 3 | 101 |
| 4 | 11000 |
| 5 | 11001 |
| 6 | 11010 |
| 7 | 11011 |
| 8 | 1110000 |
| 63 | 11111011111 |

- In practice index is reduced to $10-15 \%$ of database size

A further reduction of the index size can be achieved by applying compression techniques to the inverted lists. In practice, the inverted list of a single term can be rather large. A first improvement is achieved by storing only differences among subsequent document identifiers. Since they occur in sequential order, the differences are much smaller integers than the absolute position identifiers.

In addition number encoding techniques can be applied to the resulting integer values. Since small values will be more frequent than large ones this leads to a further reduction in the size of the posting file.

## Web-Scale Index Construction: Map-Reduce

For Web scale document collections traditional methods of index construction are no longer feasible. Therefore Google developed new approaches in terms of infrastructure and computing model to index very large document collections. A key element is the map-reduce programming model. It allows to parallelize index construction, within an infrastructure using potentially unreliable commodity hardware. The map-reduce programming model has been key in the ability of Google and later other web providers to scale up the applications. It actually led to a novel distributed programming paradigm and systems approach, that is tuned towards cost-efficiency and simplicity of programming.

## Map-Reduce Programming Model

Reduce function: key-value pairs $(k, v)$

$\left(k_{\text {in }}, v_{\text {in }}\right) \rightarrow\left[\left(k_{\text {inter }}, v_{\text {inter }}\right)\right]$

$\left(k_{\text {inter }},\left[v_{\text {inter }}\right]\right) \rightarrow\left[\left(k_{\text {out }}, v_{\text {out }}\right)\right]$

## Example: basic word counter program

```
def mapper(document, line):
    for word in line.split(): output(word, 1)
def reducer(key, values): output(key, sum(values))
```

The map-reduce programming model is based on key-value pairs and lists of key value pairs (denoted by angle brackets here). The map function receives some input data (typically a piece of text to analyze or index), and produces a list of key-value pairs, that represent some partial results of the analysis (e.g. the counts of words in the text). A combiner function can locally aggregate results on a node executing the mapper function (e.g. aggregating all counts of the same word), thus reducing the number of intermediate results.

The reducer process receives as input all local results for a given key value, that have been computed by different mapper functions. It computes then an output value (e.g. the total count of words in the document corpus).

## Map-Reduce Processing Model

This figure illustrates the basic steps of a map-reduce computation for the basic example of word counting. The document collection is partitioned and assigned to different mapper nodes. The mapper nodes extract word statistics for their partition of the document collection. For each word a reducer node is responsible. Based on the key, i.e., a word, the mapper nodes send their local results for the word to the responsible reducer node. This can be controlled e.g. by hashngi the key values. The reducer nodes aggregate the statistics that they receive from all the mapper nodes. Once the reducer nodes have finalized generating the partial indices for their key space, the results are written to the file system. The allocation of resources for the processes for mappers and reducers is performed automatically by the system and completely transparent to the developer of the code.

## 6. DISTRIBUTED RETRIEVAL

## Retrieval Processing

## Centralized retrieval

- Aggregate the weights for ALL documents by scanning the posting lists of the query terms
- Scanning is relatively efficient
- Computationally quite expensive (memory, processing)
- 

When using inverted files, a query involving multiple search terms requires the scanning of the postings lists of all terms. Typically in this process the term frequencies are computed for ALL documents in the document collection containing any of the query terms. In a centralized server this can be implemented relatively efficiently, though still resource-intensive, since scanning of disks is a comparably efficient operation.

## Distributed retrieval

- Posting lists for different terms stored on different nodes
- The transfer of complete posting lists can become prohibitively expensive in terms of bandwidth consumption

In a distributed setting the picture changes quite significantly. Assuming that posting lists for different terms are stored on different nodes, complete posting lists have to be transferred over the network. Assuming that these postings lists can contain up to millions of entries, data in the order of megabytes needs to be transferred in order to compute the query result, which results in a prohibitively high network bandwidth consumption. So the question is, whether there exist more efficient ways to determine the top ranked (top-k) for the results of a query, avoiding complete scans of posting lists.

Remark: in the following we will use $\mathrm{k}$ to indicate the number of results retrieved, despite the fact that we have used earlier $\mathrm{k}$ to denote the size of the vocabulary. The terminology top-k is so well established today, that it would be confusing to deviate here for notational consistency.

## Fagin's Algorithm 

One approach to deal with this problem is Fagin's algorithm. It has been originally developed for multimedia queries, where multiple features of an object (e.g., an image) need to be combined to determine the most similar ones. The algorithm tries to minimize the number of objects (in our case documents) that need to be considered in that process.

An important assumption that is made in Fagin's algorithm, is that the elements in a posting list are ordered according to the scores of the documents. In that case we would consider the tf-idf weights as the scores. Note that this assumption implies that an additional cost is occurred for sorting the posting lists (once). The algorithm proceeds as follows:

Phase 1: The algorithm scans in a round-robin fashion the elements of the posting lists starting from those with the highest score. Whenever an element is encountered in multiple lists, their scores are combined (e.g., added). This is continued till k elements are detected that appear in all lists.

Phase 2: By then many other documents also may have been detected, but not in all lists. Thus in a next step the missing scores are retrieved from the lists. This requires random (and not scanning) access, e.g., supported by an index. This constitutes the most expensive part of the algorithm.

Phase 3: Finally the $\mathrm{k}$ elements with the highest scores are returned. These are not necessarily corresponding to those that have been identified in the Phase 1 as those $\mathrm{k}$ elements that occur in all lists. They also might include elements for which additional scores have been retrieved in Phase 2.

The algorithm returns provably always the $\mathrm{k}$ elements with the highest combined score.

## Complexity

$-O\left((k n)^{1 / 2}\right)$ entries are read in each list for $n$ documents

- Assuming that entries are uncorrelated
- Improves if they are positively correlated

In distributed settings optimizations to reduce the number of roundtrips

- Send a longer prefix of one list to the other node


## Useful for many applications

- Multimedia, image retrieval
- Top-k processing in relational databases
- Document filtering
- Sensor data processing

Other Variants: threshold algorithm(s)

It can be shown that the complexity of the Fagin algorithm in the case of two lists is $\mathrm{O}\left((\mathrm{k} n)^{1 / 2}\right)$ for the number of entries that are read from each list, where $\mathrm{n}$ is the number of documents in the document collection. This is significantly smaller than reading the complete lists, and reduces further if the entries are positively correlated (i.e., if a document is highly ranked in one list, then it has also higher probability to be highly ranked in the other list), which is likely to be the case. The results generalizes to the case of multiple lists.

In a distributed setting applying Fagin's algorithm directly is still not very practical, since for every element retrieved from a list a message would have to be exchanged with another node. To avoid this, variants of this algorithm have been proposed, where larger chunks of the list from one node are sent to the other. In the ideal case one node "guesses" how many entries from its list would have to be read and transmits this set of entries to the other node(s).

Fagin's algorithm has found many applications apart from distributed retrieval. It is being used in multimedia retrieval (it's original application), but also in processing data from relational databases (e.g. finding tuples with a highest combined value for multiple attributes), sensor data processing, but also in text document filtering. Also alternative algorithms for solving the same problem have been proposed. They are known under the name of threshold algorithms. They work in a similar fashion, but have slightly different performance characteristics

