
Writing a parser with parser combinators
----------------------------------------

In Scala, you can (ab)use the operator overload to create an embedded DSL (EDSL) for grammars. While a grammar may look as follows in a grammar description language (Bison, Yak, ANTLR, â€¦):

```
    Expr ::= Term {'+' Term | 'âˆ’' Term}
    Term ::= Factor {'âˆ—' Factor | '/' Factor}
    Factor ::= Number | '(' Expr ')'
```

In Scala, we can model it as follows:

```
    def expr: Parser[Any] = term ~ rep("+" ~ term | "âˆ’" ~ term)
    def term: Parser[Any] = factor ~ rep("âˆ—" ~ factor | "/" ~ factor)
    def factor: Parser[Any] = "(" ~ expr ~ ")" | numericLit
```
    

This is perhaps a little less elegant, but allows us to encode it directly into our language, which is often useful for interop.

The `~`, `|`, `rep` and `opt` are **parser combinators**. These are primitives with which we can construct a full parser for the grammar of our choice.

### Boilerplate

First, letâ€™s define a class `ParseResult[T]` as an ad-hoc monad; parsing can either succeed or fail:

```
    sealed trait ParseResult[T]
    case class Success[T](result: T, in: Input) extends ParseResult[T]
    case class Failure(msg : String, in: Input) extends ParseResult[Nothing]
```
    
> ðŸ‘‰ `Nothing` is the bottom type in Scala; it contains no members, and nothing can extend it

Letâ€™s also define the tokens produced by the lexer (which we wonâ€™t define) as case classes extending `Token`:

```
    sealed trait Token
    case class Keyword(chars: String) extends Token
    case class NumericLit(chars: String) extends Token
    case class StringLit(chars: String) extends Token
    case class Identifier(chars: String) extends Token
```
    

Input into the parser is then a lazy stream of tokens (with positions for error diagnostics, which weâ€™ll omit here):

```
    type Input = Reader[Token]
```
    

We can then define a standard, sample parser which looks as follows on the type-level:

    
``` 
    class StandardTokenParsers {
        type Parser = Input => ParseResult
    }
```
    

### The basic idea

For each language (defined by a grammar symbol `S`), define a function `f` that, given an input stream `i` (with tail `i'`):

*   if a prefix of `i` is in `S`, return `Success(Pair(x, i'))`, where `x` is a result for `S`
*   otherwise, return `Failure(msg, i)`, where `msg` is an error message string

The first is called _success_, the second is _failure_. We can compose operations on this somewhat conveniently, like we would on a monad (like `Option`).

### Simple parser primitives

All of the above boilerplate allows us to define a parser, which succeeds if the first token in the input satisfies some given predicate `pred`. When it succeeds, it reads the token string, and splits the input there.

    
```    
    def token(kind: String)(pred: Token => boolean) = new Parser[String] {
        def apply(in : Input) =
            if (pred(in.head)) Success(in.head.chars, in.tail)
            else Failure(kind + " expected ", in)
    }
```

We can use this to define a keyword parser:

```
    implicit def keyword(chars: String) = token("'" + chars + "'") {
        case Keyword(chars1) => chars == chars1
        case _ => false
    }
```    

Marking it as `implicit` allows us to write keywords as normal strings, where we can omit the `keyword` call (this helps us simplify the notation in our DSL; we can write `"if"` instead of `keyword("if")`).

We can make other parsers for our other case classes quite simply:

    
```
    def numericLit = token("number")(_.isInstanceOf[NumericLit])
    def stringLit = token("string literal")(_.isInstanceOf[StringLit])
    def ident = token("identifier")(_.isInstanceOf[Identifier])
```
    

### Parser combinators

We are going to define the following parser combinators:

*   `~`: sequential composition
*   `<~`, `>~`: sequential composition, keeping left / right only
*   `|`: alternative
*   `opt(X)`: option (like a `?` quantifier in a regex)
*   `rep(X)`: repetition (like a `*` quantifier in a regex)
*   `repsep(P, Q)`: interleaved repetition
*   `^^`: result conversion (like a `map` on an `Option`)
*   `^^^`: constant result (like a `map` on an `Option`, but returning a constant value regardless of result)

But first, weâ€™ll write some very basic parser combinators: `success` and `failure`, that respectively always succeed and always fail:

    
```
    def success[T](result: T) = new Parser[T] {
        def apply(in: Input) = Success(result, in)
    }
    
    def failure(msg: String) = new Parser[Nothing] {
        def apply(in: Input) = Failure(msg, in)
    }
```
    

All of the above are methods on a `Parser[T]` class. Thanks to infix space notation in Scala, we can denote `x.y(z)` as `x y z`, which allows us to simplify our DSL notation; for instance `A ~ B` corresponds to `A.~(B)`.

    
```
    abstract class Parser[T] {
        // An abstract method that defines the parser function
        def apply(in : Input): ParseResult
    
        def ~[U](rhs: Parser[U]) = new Parser[T ~ U] {
            def apply(in: Input) = Parser.this(in) match {
                case Success(x, tail) => rhs(tail) match {
                    case Success(y, rest) => Success(new ~(x, y), rest)
                    case failure => failure
                }
                case failure => failure
            }
        }
    
        def |(rhs: => Parser[T]) = new Parser[T] {
            def apply(in : Input) = Parser.this(in) match {
                case s1 @ Success(_, _) => s1
                case failure => rhs(in)
            }
        }
    
        def ^^[U](f: T => U) = new Parser[U] {
            def apply(in : Input) = Parser.this(in) match {
                case Success(x, tail) => Success(f(x), tail)
                case x => x
            }
        }
    
        def ^^^[U](r: U): Parser[U] = ^^(x => r)
    }
    
```

> ðŸ‘‰ In Scala, `T ~ U` is syntactic sugar for `~[T, U]`, which is the type of the case class weâ€™ll define below

For the `~` combinator, when everything works, weâ€™re using `~`, a case class that is equivalent to `Pair`, but prints the way we want to and allows for the concise type-level notation above.

```
    case class ~[T, U](_1 : T, _2 : U) {
        override def toString = "(" + _1 + " ~ " + _2 +")"
    }
```
    

At this point, we thus have **two** different meanings for `~`: a _function_ `~` that produces a `Parser`, and the `~(a, b)` _case class_ pair that this parser returns (all of this is encoded in the function signature of the `~` function).

Note that the `|` combinator takes the right-hand side parser as a call-by-name argument. This is because we donâ€™t want to evaluate it unless it is strictly neededâ€”that is, if the left-hand side fails.

`^^` is like a `map` operation on `Option`; `P ^^ f` succeeds iff `P` succeeds, in which case it applies the transformation `f` on the result of P. Otherwise, it fails.

### Shorthands

We can now define shorthands for common combinations of parser combinators:

    
```
    def opt[T](p : Parser[T]): Parser[Option[T]] = p ^^ Some | success(None)
    
    def rep[T](p : Parser[T]): Parser[List[T]] = 
        p ~ rep(p) ^^ { case x ~ xs => x :: xs } | success(Nil)
    
    def repsep[T, U](p : Parser[T], q : Parser[U]): Parser[List[T]] = 
        p ~ rep(q ~> p) ^^ { case r ~ rs => r :: rs } | success(Nil)
```
    

Note that none of the above can fail. They may, however, return `None` or `Nil` wrapped in `success`.

As an exercise, we can implement the `rep1(P)` parser combinator, which corresponds to the `+` regex quantifier:

    
```    
    def rep1[T](p: Parser[T]) = p ~ rep(p)
```
    

### Example: JSON parser

Letâ€™s define a JSON parser. Scalaâ€™s parser combinator library has a `StandardTokenParsers` that give us a variety of utility methods for lexing, like `lexical.delimiters`, `lexical.reserved`, `stringLit` and `numericLit`.

    
```
    object JSON extends StandardTokenParsers {
        lexical.delimiters += ("{", "}", "[", "]", ":")
        lexical.reserved += ("null", "true", "false")
    
        // Return Map
        def obj: Parser[Any] = "{" ~ repsep(member, ",") ~ "}" ^^ (ms => Map() ++ ms)
    
        // Return List
        def arr: Parser[Any] = "[" ~> repsep(value, ",") <~ "]"
    
        // Return name/value pair:
        def member: Parser[Any] = stringLit ~ ":" ~ value ^^ {
            case name ~ ":" ~ value => (name, value) 
        }
    
        // Return correct Scala type
        def value: Parser[Any] =
              obj 
            | arr 
            | stringLit
            | numericLit ^^ (_.toInt)
            | "null" ^^^ null
            | "true" ^^^ true
            | "false" ^^^ false
    }
```

### The trouble with left-recursion

Parser combinators work top-down and therefore do not allow for left-recursion. For example, the following would go into an infinite loop, where the parser keeps recursively matching the same token unto `expr`:

    
```
    def expr = expr ~ "-" ~ term
```
    

Letâ€™s take a look at an arithmetic expression parser:

    
``` 
    object Arithmetic extends StandardTokenParsers {
        lexical.delimiters ++= List("(", ")", "+", "âˆ’", "âˆ—", "/")
        def expr: Parser[Any] = term ~ rep("+" ~ term | "âˆ’" ~ term)
        def term: Parser[Any] = factor ~ rep("âˆ—" ~ factor | "/" ~ factor)
        def factor: Parser[Any] = "(" ~ expr ~ ")" | numericLit
    }
```

This definition of `expr`, namely `term ~ rep("-" ~ term)` produces a right-leaning tree. For instance, `1 - 2 - 3` produces `1 ~ List("-" ~ 2, ~ "-" ~ 3)`.

The solution is to combine calls to `rep` with a final foldLeft on the list:

    
```   
    object Arithmetic extends StandardTokenParsers {
        lexical.delimiters ++= List("(", ")", "+", "âˆ’", "âˆ—", "/")
        def expr: Parser[Any] = term ~ rep("+" ~ term | "âˆ’" ~ term) ^^ reduceList
        def term: Parser[Any] = factor ~ rep("âˆ—" ~ factor | "/" ~ factor) ^^ reduceList
        def factor: Parser[Any] = "(" ~ expr ~ ")" | numericLit
    
        private def reduceList(list: Expr ~ List[String ~ Expr]): Expr = list match {
            case x ~ xs => (x foldLeft ps)(reduce)
        }
    
        private def reduce(x: Int, r: String ~ Int) = r match {
            case "+" ~ y => x + y
            case "âˆ’" ~ y => x âˆ’ y
            case "âˆ—" ~ y => x âˆ— y
            case "/" ~ y => x / y
            case => throw new MatchError("illegal case: " + r)
        }
    }
```
  

> ðŸ‘‰ It used to be that the standard library contained parser combinators, but those are now a [separate module](https://github.com/scala/scala-parser-combinators). This module contains a `chainl` (chain-left) method that reduces after a `rep` for you.

Arithmetic expressions â€” abstract syntax and proof principles
-------------------------------------------------------------

This section follows Chapter 3 in TAPL.

### Basics of induction

Ordinary induction is simply:

    Suppose P is a predicate on natural numbers.
    Then:
        If P(0)
        and, for all i, P(i) implies P(i + 1)
        then P(n) holds for all n
    

We can also do complete induction:

    Suppose P is a predicate on natural numbers.
    Then:
        If for each natural number n,
        given P(i) for all i < n we can show P(n)
        then P(n) holds for all n
    

It proves exactly the same thing as ordinary induction, it is simply a restated version. Theyâ€™re _interderivable_; assuming one, we can prove the other. Which one to use is simply a matter of style or convenience. Weâ€™ll see some more equivalent styles as we go along.

### Mathematical representation of syntax

Letâ€™s assume the following grammar:

    
```
    t ::= 
        true
        false
        if t then t else t
        0
        succ t
        pred t
        iszero t
```   

What does this really define? A few suggestions:

*   A set of character strings
*   A set of token lists
*   A set of abstract syntax trees

It depends on how you read it; a grammar like the one above contains information about all three.

However, we are mostly interested in the ASTs. The above grammar is therefore called an **abstract grammar**. Its main purpose is to suggest a mapping from character strings to trees.

For our use of these, we wonâ€™t be too strict with these. For instance, weâ€™ll freely use parentheses to disambiguate what tree we mean to describe, even though theyâ€™re not strictly supported by the grammar. What matters to us here arenâ€™t strict implementation semantics, but rather that we have a framework to talk about ASTs. For our purposes, weâ€™ll consider that two terms producing the same AST are basically the same; still, weâ€™ll distinguish terms that only have the same evaluation result, as they donâ€™t necessarily have the same AST.

How can we express our grammar as mathematical expressions? A grammar describes the legal _set_ of terms in a program by offering a recursive definition. While recursive definitions may seem obvious and simple to a programmer, we have to go through a few hoops to make sense of them mathematically.

#### Mathematical representation 1

We can use a set T of terms. The grammar is then the smallest set such that:

1.  {true,false,0}âŠ†T,
2.  If t1âˆˆT then {succÂ t1,predÂ t1,iszeroÂ t1}âŠ†T,
3.  If t1,t2,t3âˆˆT then we also have ifÂ t1Â thenÂ t2Â elseÂ t3âˆˆT.

#### Mathematical representation 2

We can also write this somewhat more graphically:

trueÂ âˆˆT,falseÂ âˆˆT,0âˆˆT

\frac{t1âˆˆT}{succÂ t1âˆˆT}, \frac{t1âˆˆT}{predÂ t1âˆˆT},\frac{t1âˆˆT}{iszeroÂ t1âˆˆT}
\frac{t1âˆˆT,t2âˆˆT,t3âˆˆT}{ifÂ t1Â thenÂ t2Â elseÂ t3âˆˆT}

This is exactly equivalent to representation 1, but we have just introduced a different notation. Note that â€œthe smallest set closed underâ€¦â€ is often not stated explicitly, but implied.

#### Mathematical representation 3

Alternatively, we can build up our set of terms as an infinite union:

S0\=âˆ…
S_{i+1}\={true,Â false,0}âˆª{succÂ t1,predÂ t1,iszeroÂ t1âˆ£t1âˆˆSi}âˆª{ifÂ t1Â thenÂ t2Â elseÂ t3âˆ£t1,t2,t3âˆˆSi}

We can thus build our final set as follows:

S\=â‹ƒ_i S_i

Note that we can â€œpull outâ€ the definition into a generating function F:

S_0\=âˆ…
S_{i+1}\=F(S_i)
S\=â‹ƒ_i S_i

The generating function is thus defined as:

F1(U)\={true}
F2(U)\={false}
F3(U)\={0}
F4(U)\={succÂ t1âˆ£t1âˆˆU}
F5(U)\={predÂ t1âˆ£t1âˆˆU}
F6(U)\={iszeroÂ t1âˆ£t1âˆˆU}
F7(U)\={ifÂ t1Â thenÂ t2Â elseÂ t3âˆ£t1,t2,t3âˆˆU}
F(U)\=â‹ƒ_{i\=1}^7 F_i(U)

Each function takes a set of terms U as input and produces â€œterms justified by Uâ€ as output; that is, all terms that have the items of U as subterms.

The set U is said to be **closed under F** or **F-closed** if F(U)âŠ†U.

The set of terms T as defined above is the smallest F-closed set. If O is another F-closed set, then TâŠ†O.

#### Comparison of the representations

Weâ€™ve seen essentially two ways of defining the set (as representation 1 and 2 are equivalent, but with different notation):

1.  The smallest set that is closed under certain rules. This is compact and easy to read.
2.  The limit of a series of sets. This gives us an _induction principle_ on which we can prove things on terms by induction.

The first one defines the set â€œfrom aboveâ€, by intersecting F-closed sets.

The second one defines it â€œfrom belowâ€, by starting with âˆ… and getting closer and closer to being F-closed.

These are equivalent (we wonâ€™t prove it, but Proposition 3.2.6 in TAPL does so), but can serve different uses in practice.

### Induction on terms

First, letâ€™s define depth: the **depth** of a term t is the smallest i such that tâˆˆSi.

The way we defined Si, it gets larger and larger for increasing i; the depth of a term t gives us the step at which t is introduced into the set.

We see that if a term t is in Si, then all of its immediate subterms must be in Siâˆ’1, meaning that they must have smaller depth.

This justifies the principle of **induction on terms**, or **structural induction**. Let P be a predicate on a term:

```
    If, for each term s,
        given P(r) for all immediate subterms r of s we can show P(s)
        then P(t) holds for all t
```

All this says is that if we can prove the induction step from subterms to terms (under the induction hypothesis), then we have proven the induction.

We can also express this structural induction using generating functions, which we [introduced previously](https://kjaer.io/fos/#mathematical-representation-3).

```
    Suppose T is the smallest F-closed set.
    If, for each set U,
        from the assumption "P(u) holds for every u âˆˆ U",
        we can show that "P(v) holds for every v âˆˆ F(U)"
    then
        P(t) holds for all t âˆˆ T
```

Why can we use this?

*   We assumed that T was the smallest F-closed set, which means that TâŠ†O for any other F-closed set O.
*   Showing the pre-condition (â€œfor each set U, from the assumptionâ€¦â€) amounts to showing that the set of all terms satisfying P (call it O) is itself an F-closed set.
*   Since TâŠ†O, every element of T satisfies P.

### Inductive function definitions

An [inductive definition](https://en.wikipedia.org/wiki/Recursive_definition) is used to define the elements in a set recursively, as we have done above. The [recursion theorem](https://en.wikipedia.org/wiki/Recursion#The_recursion_theorem) states that a well-formed inductive definition defines a function. To understand what being well-formed means, letâ€™s take a look at some examples.

Letâ€™s define our grammar function a little more formally. Constants are the basic values that canâ€™t be expanded further; in our example, they are `true`, `false`, `0`. As such, the set of constants appearing in a term t, written Consts(t), is defined recursively as follows:

Consts(true)\={true}
Consts(false)\={false}
Consts(0)\={0}
Consts(succÂ t1)\=Consts(t1)
Consts(predÂ t1)\=Consts(t1)
Consts(iszeroÂ t1)\=Consts(t1)
Consts(ifÂ t1Â thenÂ t2Â elseÂ t3\=Consts(t1)âˆªConsts(t2)âˆªConsts(t3)

This seems simple, but these semantics arenâ€™t perfect. First off, a mathematical definition simply assigns a convenient name to some previously known thing. But here, weâ€™re defining the thing in terms of itself, recursively. And the semantics above also allow us to define ill-formed inductive definitions:

BadConsts(true)\={true}
BadConsts(false)\={false}
BadConsts(0)\={0}
BadConsts(0)\={}\=âˆ…
BadConsts(succÂ t1)\=BadConsts(t1)
BadConsts(predÂ t1)\=BadConsts(t1)
BadConsts(iszeroÂ t1)\=BadConsts(iszero iszeroÂ t1)

The last rule produces infinitely large rules (if we implemented it, weâ€™d expect some kind of stack overflow). Weâ€™re missing the rules for if-statements, and we have a useless rule for `0`, producing empty sets.

How do we tell the difference between a well-formed inductive definition, and an ill-formed one as above? What is well-formedness anyway?

#### What is a function?

A relation over T,U is a subset of TÃ—U, where the Cartesian product is defined as:

$TÃ—U\={(t,u):tâˆˆT,uâˆˆU}$

A function f from A (domain) to B (co-domain) can be viewed as a two-place relation, albeit with two additional properties:

*   It is **total**: âˆ€aâˆˆA,âˆƒbâˆˆB:(a,b)âˆˆf
*   It is **deterministic**: (a,b1)âˆˆf,(a,b2)âˆˆfâŸ¹b1\=b2

Totality ensures that the A domain is covered, while being deterministic just means that the function always produces the same result for a given input.

#### Induction example 1

As previously stated, Consts is a _relation_. It maps terms (A) into the set of constants that they contain (B). The induction theorem states that it is also a _function_. The proof is as follows.

Consts is total and deterministic: for each term t there is exactly one set of terms C such that (t,C)âˆˆConsts[1](https://kjaer.io/fos/#fn:in-relation-notation) . The proof is done by induction on t.

To be able to apply the induction principle for terms, we must first show that for an arbitrary term t, under the following induction hypothesis:

> For each immediate subterm s of t, there is exactly one set of terms Cs such that (s,Cs)âˆˆConsts

Then the following needs to be proven as an induction step:

> There is **exactly one** set of terms C such that (t,C)âˆˆConsts

We proceed by cases on t:

*   If t is 0, true or false
    
    We can immediately see from the definition that of Consts that there is exactly one set of terms C\={t}) such that (t,C)âˆˆConsts.
    
    This constitutes our base case.
    
*   If t is succÂ t1, predÂ t1 or iszeroÂ t1
    
    The immediate subterm of t is t1, and the induction hypothesis tells us that there is exactly one set of terms C1 such that (t1,C1)âˆˆConsts. But then it is clear from the definition that there is exactly one set of terms C\=C1 such that (t,C)âˆˆConsts.
    
*   If t is ifÂ t1Â thenÂ t2Â elseÂ t3
    
    The induction hypothesis tells us:
    
    *   There is exactly one set of terms C1 such that (t1,C1)âˆˆConsts
    *   There is exactly one set of terms C2 such that (t2,C2)âˆˆConsts
    *   There is exactly one set of terms C3 such that (t3,C3)âˆˆConsts
    
    It is clear from the definition of Consts that there is exactly one set C\=C1âˆªC2âˆªC3 such that (t,C)âˆˆConsts.
    

This proves that Consts is indeed a function.

But what about BadConsts? It is also a relation, but it isnâ€™t a function. For instance, we have BadConsts(0)\={0} and BadConsts(0)\=âˆ…, which violates determinism. To reformulate this in terms of the above, there are two sets C such that (0,C)âˆˆBadConsts, namely C\={0} and C\=âˆ….

Note that there are many other problems with BadConsts, but this is sufficient to prove that it isnâ€™t a function.

#### Induction example 2

Letâ€™s introduce another inductive definition:

size(true)\=1
size(false)\=1
size(0)\=1
size(succÂ t1)\=size(t1)+1
size(predÂ t1)\=size(t1)+1
size(iszeroÂ t1)\=size(t1)+1
size(ifÂ t1Â thenÂ t2Â elseÂ t3)\=size(t1)+size(t2)+size(t3)

Weâ€™d like to prove that the number of distinct constants in a term is at most the size of the term. In other words, that |Consts(t)|â‰¤size(t)

The proof is by induction on t:

*   t is a constant; t\=true, t\=false or t\=0
    
    The proof is immediate. For constants, the number of constants and the size are both one: |Consts(t)|\=|{t}|\=1\=size(t)
    
*   t is a function; t\=succÂ t1, t\=predÂ t1 or t\=iszeroÂ t1
    
    By the induction hypothesis, |Consts(t1)|â‰¤size(t1).
    
    We can then prove the proposition as follows: |Consts(t)|\=|Consts(t1)|â‰¤size(t1)\=size(t)+1<size(t)
    
*   t is an if-statement: t\=ifÂ t1Â thenÂ t2Â elseÂ t3
    
    By the induction hypothesis, |Consts(t1)|â‰¤size(t1), |Consts(t2)|â‰¤size(t2) and |Consts(t3)|â‰¤size(t3).
    
    We can then prove the proposition as follows:

|Consts|\=|Consts(t1)âˆªConsts(t2)âˆªConsts(t3)|â‰¤|Consts(t1)|+|Consts(t2)|+|Consts(t3)|â‰¤size(t1)+size(t2)+size(t3)<size(t)

### Operational semantics and reasoning

#### Evaluation

Suppose we have the following syntax

```   
    t ::=                  // terms
        true                   // constant true
        false                  // constant false 
        if t then t else t     // conditional
```   

The evaluation relation tâŸ¶tâ€² is the smallest relation closed under the following rules.

The following are _computation rules_, defining the â€œrealâ€ computation steps:

if true thenÂ t2Â elseÂ t3âŸ¶t2	(E-IfTrue)
if false thenÂ t2Â elseÂ t3âŸ¶t3	(E-IfFalse)

The following is a _congruence rule_, defining where the computation rule is applied next:

\frac{t1âŸ¶tâ€²1}{ifÂ t1Â thenÂ t2Â elseÂ t3âŸ¶ifÂ tâ€²1Â thenÂ t2Â elseÂ t3}	(E-If)

We want to evaluate the condition before the conditional clauses in order to save on evaluation; weâ€™re not sure which one should be evaluated, so we need to know the condition first.

#### Derivations

We can describe the evaluation logically from the above rules using derivation trees. Suppose we want to evaluate the following (with parentheses added for clarity): `if (if true then true else false) then false else true`.

In an attempt to make all this fit onto the screen, `true` and `false` have been abbreviated `T` and `F` in the derivation below, and the `then` keyword has been replaced with a parenthesis notation for the condition.

\frac{\frac{ifÂ (T)Â TÂ elseÂ FâŸ¶T (E-IfTrue)}{ifÂ (ifÂ (T)Â TÂ elseÂ F)Â FÂ elseÂ TâŸ¶ifÂ (T)Â FÂ elseÂ T	(E-If)} ifÂ (T)Â FÂ elseÂ TâŸ¶F	(E-IfTrue)}{ifÂ (ifÂ (T)Â TÂ elseÂ F)Â FÂ elseÂ TâŸ¶T}

The final statement is a **conclusion**. We say that the derivation is a **witness** for its conclusion (or a **proof** for its conclusion). The derivation records all reasoning steps that lead us to the conclusion.

#### Inversion lemma

We can introduce the **inversion lemma**, which tells us how we got to a term.

Suppose we are given a derivation D witnessing the pair (t,tâ€²) in the evaluation relation. Then either:

1.  If the final rule applied in D was ([E-IfTrue](https://kjaer.io/fos/#mjx-eqn-eq%3Ae-iftrue)), then we have ifÂ trueÂ thenÂ t2Â elseÂ t3 and tâ€²\=t2 for some t2 and t3
2.  If the final rule applied in D was ([E-IfFalse](https://kjaer.io/fos/#mjx-eqn-eq%3Ae-iffalse)), then we have ifÂ falseÂ thenÂ t2Â elseÂ t3 and tâ€²\=t2 for some t2 and t3
3.  If the final rule applied in D was ([E-If](https://kjaer.io/fos/#mjx-eqn-eq%3Ae-if)), then we have t\=ifÂ t1Â thenÂ t2Â elseÂ t3 and tâ€²\=t\=ifÂ tâ€²1Â thenÂ t2Â elseÂ t3, for some t1,tâ€²1,t2,t3. Moreover, the immediate subderivation of D witnesses (t1,tâ€²1)âˆˆâŸ¶.

This is super boring, but we do need to acknowledge the inversion lemma before we can do induction proofs on derivations. Thanks to the inversion lemma, given an arbitrary derivation D with conclusion tâŸ¶tâ€², we can proceed with a case-by-case analysis on the final rule used in the derivation tree.

Letâ€™s recall our [definition of the size function](https://kjaer.io/fos/#induction-example-2). In particular, weâ€™ll need the rule for if-statements:

size(ifÂ t1Â thenÂ t2Â elseÂ t3)\=size(t1)+size(t2)+size(t3)

We want to prove that if tâŸ¶tâ€², then size(t)\>size(tâ€²).

1.  If the final rule applied in D was ([E-IfTrue](https://kjaer.io/fos/#mjx-eqn-eq%3Ae-iftrue)), then we have t\=ifÂ trueÂ thenÂ t2Â elseÂ t3 and tâ€²\=t2, and the result is immediate from the definition of size
2.  If the final rule applied in D was ([E-IfFalse](https://kjaer.io/fos/#mjx-eqn-eq%3Ae-iffalse)), then we have t\=ifÂ falseÂ thenÂ t2Â elseÂ t3 and tâ€²\=t2, and the result is immediate from the definition of size
3.  If the final rule applied in D was ([E-If](https://kjaer.io/fos/#mjx-eqn-eq%3Ae-if)), then we have t\=ifÂ t1Â thenÂ t2Â elseÂ t3 and tâ€²\=ifÂ tâ€²1Â thenÂ t2Â elseÂ t3. In this case, t1âŸ¶tâ€²1 is witnessed by a derivation D1. By the induction hypothesis, size(t1)\>size(tâ€²1), and the result is then immediate from the definition of size

### Abstract machines

An abstract machine consists of:

*   A set of **states**
*   A **transition** relation of states, written âŸ¶

tâŸ¶tâ€² means that t evaluates to tâ€² in one step. Note that âŸ¶ is a relation, and that tâŸ¶tâ€² is shorthand for (t,tâ€²)âˆˆâŸ¶. Often, this relation is a partial function (not necessarily covering the domain A; there is at most one possible next state). But without loss of generality, there may be many possible next states, determinism isnâ€™t a criterion here.

### Normal forms

A normal form is a term that cannot be evaluated any further. More formally, a term t is a normal form if there is no tâ€² such that tâŸ¶tâ€². A normal form is a state where the abstract machine is halted; we can regard it as the result of a computation.

#### Values that are normal form

Previously, we intended for our values (true and false) to be exactly that, the result of a computation. Did we get that right?

Letâ€™s prove that a term t is a value âŸº it is in normal form.

*   The âŸ¹ direction is immediate from the definition of the evaluation relation âŸ¶.
*   The âŸ¸ direction is more conveniently proven as its contrapositive: if t is not a value, then it is not a normal form, which we can prove by induction on the term t.
    
    Since t is not a value, it must be of the form ifÂ t1Â thenÂ t2Â elseÂ t3. If t1 is directly `true` or `false`, then [E-IfTrue](https://kjaer.io/fos/#mjx-eqn-eq%3Ae-iftrue) or [E-IfFalse](https://kjaer.io/fos/#mjx-eqn-eq%3Ae-iffalse) apply, and we are done.
    
    Otherwise, if t\=ifÂ t1Â thenÂ t2Â elseÂ t3 where t1 isnâ€™t a value, by the induction hypothesis, there is a tâ€²1 such that t1âŸ¶tâ€²1. Then rule [E-If](https://kjaer.io/fos/#mjx-eqn-eq%3Ae-if) yields ifÂ tâ€²1Â thenÂ t2Â elseÂ t3, which proves that t is not in normal form.
    

#### Values that are not normal form

Letâ€™s introduce new syntactic forms, with new evaluation rules.

    
```
    t ::=        // terms
        0            // constant 0
        succ t       // successor
        pred t       // predecessor 
        iszero t     // zero test
    
    v ::=  nv     // values
    
    nv ::=        // numeric values
        0             // zero value
        succ nv       // successor value
```    

The evaluation rules are given as follows:

\frac{t1âŸ¶tâ€²1}{succÂ t1âŸ¶succÂ tâ€²1}	(E-Succ)
predÂ 0âŸ¶0	(E-PredZero)
pred succÂ nv1âŸ¶nv1	(E-PredSucc)
\frac{t1âŸ¶tâ€²1}{predÂ t1âŸ¶predÂ tâ€²1}	(E-Pred)
iszeroÂ 0âŸ¶trueis	(E-IszeroZero)
zero succÂ nv1âŸ¶false	(E-IszeroSucc)
\frac{t1âŸ¶tâ€²1}{iszeroÂ t1âŸ¶iszeroÂ tâ€²1} 	(E-Iszero)

All values are still normal forms. But are all normal forms values? Not in this case. For instance, `succ true`, `iszero true`, etc, are normal forms. These are **stuck terms**: they are in normal form, but are not values. In general, these correspond to some kind of type error, and one of the main purposes of a type system is to rule these kinds of situations out.

### Multi-step evaluation

Letâ€™s introduce the _multi-step evaluation_ relation, âŸ¶âˆ—. It is the reflexive, transitive closure of single-step evaluation, i.e. the smallest relation closed under these rules:

\frac{tâŸ¶tâ€²}{tâŸ¶âˆ—tâ€²}
tâŸ¶âˆ—t	(Reflexivity)
\frac{tâŸ¶âˆ—tâ€²	tâ€²âŸ¶âˆ—tâ€²â€²}{tâŸ¶âˆ—tâ€²â€²}	(Transitivity)

In other words, it corresponds to any number of single consecutive evaluations.

### Termination of evaluation

Weâ€™ll prove that evaluation terminates, i.e. that for every term t there is some normal form tâ€² such that tâŸ¶âˆ—tâ€².

First, letâ€™s [recall our proof](https://kjaer.io/fos/#induction-example-2) that tâŸ¶tâ€²âŸ¹size(t)\>size(tâ€²). Now, for our proof by contradiction, assume that we have an infinite-length sequence t0,t1,t2,â€¦ such that:

t0âŸ¶t1âŸ¶t2âŸ¶â€¦âŸ¹size(t0)\>size(t1)\>size(t2)\>â€¦

But this sequence cannot exist: since size(t0) is a finite, natural number, we cannot construct this infinite descending chain from it. This is a contradiction.

Most termination proofs have the same basic form. We want to prove that the relation RâŠ†XÃ—X is terminating â€” that is, there are no infinite sequences x0,x1,x2,â€¦ such that (xi,xi+1)âˆˆR for each i. We proceed as follows:

1.  Choose a well-suited set W with partial order < such that there are no infinite descending chains w0\>w1\>w2\>â€¦ in W. Also choose a function f:Xâ†’W.
2.  Show f(x)\>f(y)âˆ€(x,y)âˆˆR
3.  Conclude that are no infinite sequences (x0,x1,x2,â€¦) such that (xi,xi+1)âˆˆR for each i. If there were, we could construct an infinite descending chain in W.

As a side-note, **partial order** is defined as the following properties:

1.  **Anti-symmetry**: Â¬(x<yâˆ§y<x)
2.  **Transitivity**: x<yâˆ§y<zâŸ¹x<z

We can add a third property to achieve **total order**, namely xâ‰ yâŸ¹x<yâˆ¨y<x.

Lambda calculus
---------------

Lambda calculus is Turing complete, and is higher-order (functions are data). In lambda calculus, all computation happens by means of function abstraction and application.

Lambda calculus is isomorphic to Turing machines.

Suppose we wanted to write a function `plus3` in our previous language:

```
    plus3 x = succ succ succ x
```

The way we write this in lambda calculus is:

plus3Â \=Î»x.Â succ(succ(succ(x)))

Î»x.t is written `x => t` in Scala, or `fun x -> t` in OCaml. Application of our function, say `plus3(succ 0)`, can be written as:

(Î»x.succ succ succÂ x)(succÂ 0)

Abstraction over functions is possible using higher-order functions, which we call Î»\-abstractions. An example of such an abstraction is the function g below, which takes an argument f and uses it in the function position.

g\=Î»f.f(f(succÂ 0))

If we apply g to an argument like plus3, we can just use the substitution rule to see how that defines a new function.

Another example: the `twice` function below takes two arguments, as a curried function would. First, it takes the function to apply twice, then the argument on which to apply it, and then returns f(f(y)).

twice\=Î»f.Î»y.f(f(y))

### Pure lambda calculus

Once we have Î»\-abstractions, we can actually throw out all other language primitives like booleans and other values; all of these can be expressed as functions, as weâ€™ll see below. In pure lambda-calculus, _everything_ is a function.

Variables will always denote a function, functions always take other functions as parameters, and the result of an evaluation is always a function.

The syntax of lambda-calculus is very simple:

    
```
    t ::=      // terms, also called Î»-terms
        x         // variable
        Î»x. t     // abstraction, also called Î»-abstractions
        t t       // application
```
    

A few rules and syntactic conventions:

*   Application associates to the left, so tÂ uÂ v means (tÂ u)Â v, not tÂ (uÂ v).
*   Bodies of lambda abstractions extend as far to the right as possible, so Î»x.Î»y.Â xÂ y means Î»x.Â (Î»y.xÂ y), not Î»x.Â (Î»y.Â x)Â y

#### Scope

The lambda expression Î»x.Â t **binds** the variable x, with a **scope** limited to t. Occurrences of x inside of t are said to be _bound_, while occurrences outside are said to be _free_.

Let fv(t) be the set of free variables in a term t. Itâ€™s defined as follows:

fv(x)\={x}
fv(Î»x.Â t1)\=fv(t1)âˆ–{x}
fv(t1Â t2)\=fv(t1)âˆªfv(t2)

#### Operational semantics

As we saw with our previous language, the rules could be distinguished into _computation_ and _congruence_ rules. For lambda calculus, the only computation rule is:

(Î»x.t12)Â v2âŸ¶\[xâ†¦v2\]t12	(E-AppAbs)

The notation \[xâ†¦v2\]t12 means â€œthe term that results from substituting free occurrences of x in t12 with v2â€.

The congruence rules are:

\frac{t1âŸ¶tâ€²1}{t1Â t2âŸ¶tâ€²1Â t2}	(E-App1)
\frac{t2âŸ¶tâ€²2}{t1Â t2âŸ¶t1Â tâ€²2}	(E-App2)

A lambda-expression applied to a value, (Î»x.Â t)Â v, is called a **reducible expression**, or **redex**.

#### Evaluation strategies

There are alternative evaluation strategies. In the above, we have chosen call by value (which is the standard in most mainstream languages), but we could also choose:

*   **Full beta-reduction**: any redex may be reduced at any time. This offers no restrictions, but in practice, we go with a set of restrictions like the ones below (because coding a fixed way is easier than coding probabilistic behavior).
*   **Normal order**: the leftmost, outermost redex is always reduced first. This strategy allows to reduce inside unapplied lambda terms
*   **Call-by-name**: allows no reductions inside lambda abstractions. Arguments are not reduced before being substituted in the body of lambda terms when applied. Haskell uses an optimized version of this, call-by-need (aka lazy evaluation).

### Classical lambda calculus

Classical lambda calculus allows for full beta reduction.

#### Confluence in full beta reduction

The congruence rules allow us to apply in different ways; we can choose between [E-App1](https://kjaer.io/fos/#mjx-eqn-eq%3Ae-app1) and [E-App2](https://kjaer.io/fos/#mjx-eqn-eq%3Ae-app2) every time we reduce an application, and this offers many possible reduction paths.

While the path is non-deterministic, is the result also non-deterministic? This question took a very long time to answer, but after 25 years or so, it was proven that the result is always the same. This is known the **Church-Rosser confluence theorem**:

Let t,t1,t2 be terms such that tâŸ¶âˆ—t1 and tâŸ¶âˆ—t2. Then there exists a term t3 such that t1âŸ¶âˆ—t3 and t2âŸ¶âˆ—t3

#### Alpha conversion

Substitution is actually trickier than it looks! For instance, in the expression Î»x.Â (Î»y.Â x)Â y, the first occurrence of y is bound (it refers to a parameter), while the second is free (it does not refer to a parameter). This is comparable to scope in most programming languages, where we should understand that these are two different variables in different scopes, y1 and y2.

The above example had a variable that is both bound and free, which is something that weâ€™ll try to avoid. This is called a hygiene condition.

We can transform a unhygienic expression to a hygienic one by renaming bound variables before performing the substitution. This is known as **alpha conversion**. Alpha conversion is given by the following conversion rule:

\frac{yâˆ‰fv(t)}{(Î»x.Â t)\=_Î± (Î»y.Â \[xâ†¦y\]Â t)}(Î±)

And these equivalence rules (in mathematics, equivalence is defined as symmetry and transitivity):

\frac{t1\=_Î± t2}{t2\=_Î± t1}	(Î±\-Symm)
\frac{t1\=_Î± t2	   t2\=_Î± t3}{t1\=_Î± t3}  (Î±\-Trans)

The congruence rules are as usual.

### Programming in lambda-calculus

#### Multiple arguments

The way to handle multiple arguments is by currying: Î»x.Â Î»y.Â t

#### Booleans

The fundamental, universal operator on booleans is if-then-else, which is what weâ€™ll replicate to model booleans. Weâ€™ll denote our booleans as tru and fls to be able to distinguish these pure lambda-calculus abstractions from the true and false values of our previous toy language.

We want `true` to be equivalent to `if (true)`, and `false` to `if (false)`. The terms tru and fls _represent_ boolean values, in that we can use them to test the truth of a boolean value:

truÂ \=Î»t.Â Î»f.Â t
flsÂ \=Î»t.Â Î»f.Â f

We can consider these as booleans. Equivalently `tru` can be considered as a function performing `(t1, t2) => if (true) t1 else t2`. To understand this, letâ€™s try to apply tru to two arguments:

truÂ vÂ w \=(Î»t.Â (Î»f.Â t))Â vÂ w âŸ¶ (Î»f.Â v)Â w âŸ¶ v

This works equivalently for `fls`.

We can also do inversion, conjunction and disjunction with lambda calculus, which can be read as particular if-else statements:

notÂ \=Î»b.Â bÂ flsÂ true
andÂ \=Î»b.Â Î»c.Â bÂ cÂ fls
orÂ \=Î»b.Â Î»c.Â bÂ truÂ c

*   `not` is a function that is equivalent to `not(b) = if (b) false else true`.
*   `and` is equivalent to `and(b, c) = if (b) c else false`
*   `or` is equivalent to `or(b, c) = if (b) true else c`

#### Pairs

The fundamental operations are construction `pair(a, b)`, and selection `pair._1` and `pair._2`.

pairÂ \=Î»f.Â Î»s.Â Î»b.Â bÂ fÂ s
fstÂ \=Î»p.Â pÂ tru
sndÂ \=Î»p.Â pÂ fls

*   `pair` is equivalent to `pair(f, s) = (b => b f s)`
*   When `tru` is applied to `pair`, it selects the first element, by definition of the boolean, and that is therefore the definition of `fst`
*   Equivalently for `fls` applied to `pair`, it selects the second element

#### Numbers

Weâ€™ve actually been representing numbers as lambda-calculus numbers all along! Our `succ` function represents whatâ€™s more formally called **Church numerals**.

c0\=Î»s.Â Î»z.Â z
c1\=Î»s.Â Î»z.Â sÂ z
c2\=Î»s.Â Î»z.Â sÂ sÂ z
c3\=Î»s.Â Î»z.Â sÂ sÂ sÂ z

Note that c0â€™s implementation is the same as that of fls (just with renamed variables).

Every number n is represented by a term cn taking two arguments, which are s and z (for â€œsuccessorâ€ and â€œzeroâ€), and applies s to z, n times. Fundamentally, a number is equivalent to the following:

cn\=Î»f.Â Î»x.Â fÂ â€¦Â fx

With this in mind, let us implement some functions on numbers.

sccÂ \=Î»n.Â Î»s.Â Î»z.Â sÂ (nÂ sÂ z)
addÂ \=Î»s.Â Î»z.Â mÂ sÂ (nÂ sÂ z)mulÂ \=Î»m.Â Î»n.Â mÂ (addÂ n)Â c0
subÂ \=Î»m.Â Î»n.Â nÂ predÂ m
iszeroÂ \=Î»m.Â mÂ (Î»x.Â fls)Â tru

*   **Successor** scc: we apply the successor function to n (which has been correctly instantiated with s and z)
*   **Addition** add: we pass the instantiated n as the zero of m
*   **Subtraction** sub: we apply pred n times to m
*   **Multiplication** mul: instead of the successor function, we pass the addition by n function.
*   **Zero test** iszero: zero has the same implementation as false, so we can lean on that to build an iszero function. An alternative understanding is that weâ€™re building a number, in which we use true for the zero value z. If we have to apply the successor function s once or more, we want to get false, so for the successor function we use a function ignoring its input and returning false if applied.

What about predecessor? This is a little harder, and itâ€™ll take a few steps to get there. The main idea is that we find the predecessor by rebuilding the whole succession up until our number. At every step, we must generate the number and its predecessor: zero is (c0,c0), and all other numbers are (cnâˆ’1,cn). Once weâ€™ve reconstructed this pair, we can get the predecessor by taking the first element of the pair.

zz\=pairÂ c0Â c0
ss\=Î»p.Â pairÂ (sndÂ p)Â (sccÂ (sndÂ p))
prd\=Î»m.Â fstÂ (mÂ ss zz)

Sidenote

The story goes that Church was stumped by predecessors for a long time. This solution finally came to him while he was at the barber, and he jumped out half shaven to write it down.

#### Lists

Now what about lists?

nil\=Î»f.Â Î»g.Â g
cons\=Î»x.Â Î»xs.Â (Î»f.Â Î»g.Â fÂ xÂ xs)
head\=Î»xs.Â (Î»y.Â Î»ys.Â y)
isEmpty\=Î»xs.Â xsÂ (Î»y.Â Î»ys.Â fls)

### Recursion in lambda-calculus

Letâ€™s start by taking a step back. We talked about normal forms and terms for which we terminate; does lambda calculus always terminate? Itâ€™s Turing complete, so it must be able to loop infinitely (otherwise, weâ€™d have solved the halting problem!).

The trick to recursion is self-application:

Î»x.Â xÂ x

From a type-level perspective, we would cringe at this. This should not be possible in the typed world, but in the untyped world we can do it. We can construct a simple infinite loop in lambda calculus as follows:

Î©\=(Î»x.Â xÂ x)Â (Î»x.Â xÂ x)âŸ¶Â (Î»x.Â xÂ x)Â (Î»x.Â xÂ x)

The expression evaluates to itself in one step; it never reaches a normal form, it loops infinitely, diverges. This is not a stuck term though; evaluation is always possible.

In fact, there are no stuck terms in pure lambda calculus. Every term is either a value or reduces further.

So it turns out that omega isnâ€™t so terribly useful. Letâ€™s try to construct something more practical:

Yf\=(Î»x.Â fÂ (xÂ x))Â (Î»x.Â fÂ (xÂ x))

Now, the divergence is a little more interesting:

Yf\=(Î»x.Â fÂ (xÂ x))Â (Î»x.Â fÂ (xÂ x))âŸ¶fÂ ((Î»x.Â fÂ (xÂ x))Â (Î»x.Â fÂ (xÂ x)))\=fÂ (Yf)âŸ¶â€¦\=fÂ (fÂ (Yf))

This Yf function is known as a **Y combinator**. It still loops infinitely (though note that while it works in classical lambda calculus, it blows up in call-by-name), so letâ€™s try to build something more useful.

To delay the infinite recursion, we could build something like a poison pill:

poisonpill\=Î»y.Â omega

It can be passed around (after all, itâ€™s just a value), but evaluating it will cause our program to loop infinitely. This is the core idea weâ€™ll use for defining the **fixed-point combinator** fix (also known as the call-by-value Y combinator), which allows us to do recursion. Itâ€™s defined as follows:

fix\=Î»f.Â (Î»x.Â fÂ (Î»y.Â xÂ xÂ y))Â (Î»x.Â fÂ (Î»y.Â xÂ xÂ y))

This looks a little intricate, and we wonâ€™t need to fully understand the definition. Whatâ€™s important is mostly how it is used to define a recursive function. For instance, if we wanted to define a modulo function in our toy language, weâ€™d do it as follows:

    
```
    def mod(x, y) = 
        if (y > x) x
        else mod(x - y, y)
```
    

In lambda calculus, weâ€™d define this as:

mod\=fixÂ (Î»f.Â Î»x.Â Î»y.Â (gtÂ yÂ x)Â xÂ (f(subÂ aÂ b)Â b))

Weâ€™ve assumed that a greater-than gt function was available here.

More generally, we can define a recursive function as:

fixÂ (Î»f.Â (recursion onÂ f))

### Equivalence of lambda terms

Weâ€™ve seen how to define Church numerals and successor. How can we prove that succÂ cn is equal to cn+1?

The naive approach unfortunately doesnâ€™t work; they do not evaluate to the same value.

sccÂ c2\=(Î»n.Â Î»s.Â Î»z.Â sÂ (nÂ sÂ z))Â (Î»s.Â Î»z.Â sÂ (sÂ z))âŸ¶Î»s.Â Î»z.Â sÂ ((Î»s.Â Î»z.Â sÂ (sÂ z))Â sÂ z)â‰ Î»s.Â Î»z.Â sÂ (sÂ (sÂ z))\=c3

This still seems very close. If we could simplify a little further, we do see how they would be the same.

The intuition behind the Church numeral representation was that a number n is represented as a term that â€œdoes something n times to something elseâ€. scc takes a term that â€œdoes something n times to something elseâ€, and returns a term that â€œdoes something n+1 times to something elseâ€.

What we really care about is that sccÂ c2 _behaves_ the same as c3 when applied to two arguments. We want _behavioral equivalence_. But what does that mean? Roughly, two terms s and t are behaviorally equivalent if there is no â€œtestâ€ that distinguishes s and t.

Letâ€™s define this notion of â€œtestâ€ this a little more precisely, and specify how weâ€™re going to observe the results of a test. We can use the notion of **normalizability** to define a simple notion of a test:

> Two terms s and t are said to be **observationally equivalent** if they are either both normalizable (i.e. they reach a normal form after a finite number of evaluation steps), or both diverge.

In other words, we observe a termâ€™s behavior by running it and seeing if it halts. Note that this is not decidable (by the halting problem).

For instance, omega and tru are not observationally equivalent (one diverges, one halts), while tru and fls are (they both halt).

Observational equivalence isnâ€™t strong enough of a test for what we need; we need behavioral equivalence.

> Two terms s and t are said to be **behaviorally equivalent** if, for every finite sequence of values v1,v2,â€¦,vn the applications sÂ v1Â v2Â â€¦Â vn and tÂ v1Â v2Â â€¦Â vn are observationally equivalent.

This allows us to assert that true and false are indeed different:

truÂ xÂ Î©âŸ¶x
flsÂ xÂ Î©âŸ¶Î©

The former returns a normal form, while the latter diverges.

Types
-----

As previously, to define a language, we start with a _set of terms_ and _values_, as well as an _evaluation relation_. But now, weâ€™ll also define a set of **types** (denoted with a first capital letter) classifying values according to their â€œshapeâ€. We can define a _typing relation_ t:Â T. We must check that the typing relation is _sound_ in the sense that:

\frac{t:T	tâŸ¶âˆ—v}{v:T}
and
\frac{t:T}{âˆƒtâ€²Â such thatÂ tâŸ¶tâ€²}

These rules represent some kind of safety and liveness, but are more commonly referred to as [progress and preservation](https://kjaer.io/fos/#properties-of-the-typing-relation), which weâ€™ll talk about later. The first one states that types are preserved throughout evaluation, while the second says that if we can type-check, then evaluation of t will not get stuck.

In our previous toy language, we can introduce two types, booleans and numbers:

```
    T ::=     // types
        Bool     // type of booleans
        Nat      // type of numbers
```

Our typing rules are then given by:

trueÂ :Â Bool	 (T-True)
falseÂ :Â Bool	(T-False)
0:Â Nat		(T-Zero)
\frac{t1:Bool  t2:T  t3:T}{ifÂ t1Â thenÂ t2Â elseÂ t3}	(T-If)
\frac{t1:Nat}{succÂ t1:Nat}	(T-Succ)
\frac{t1:Nat}{predÂ t1:Nat}	(T-Pred)
\frac{t1:Nat}{iszeroÂ t1:Nat} 	(T-IsZero)

With these typing rules in place, we can construct typing derivations to justify every pair t:T (which we can also denote as a (t,T) pair) in the typing relation, as we have done previously with evaluation. Proofs of properties about the typing relation often proceed by induction on these typing derivations.

Like other static program analyses, type systems are generally imprecise. They do not always predict exactly what kind of value will be returned, but simply a conservative approximation. For instance, `if true then 0 else false` cannot be typed with the above rules, even though it will certainly evaluate to a number. We could of course add a typing rule for `if true` statements, but there is still a question of how useful this is, and how much complexity it adds to the type system, and especially for proofs. Indeed, the inversion lemma below becomes much more tedious when we have more rules.

### Properties of the Typing Relation

The safety (or soundness) of this type system can be expressed by the following two properties:

*   **Progress**: A well-typed term is not stuck.
    
    If tÂ :Â T then either t is a value, or else tâŸ¶tâ€² for some tâ€².
    
*   **Preservation**: Types are preserved by one-step evaluation.
    
    If tÂ :Â T and tâŸ¶tâ€², then tâ€²Â :Â T.
    

We will prove these later, but first we must state a few lemmas.

#### Inversion lemma

Again, for types we need to state the same (boring) inversion lemma:

1.  If true:R, then R\=Bool.
2.  If false:R, then R\=Bool.
3.  If ifÂ t1Â thenÂ t2Â elseÂ t3:R, then t1:Â Bool, t2:R and t3:R
4.  If 0:R then R\=Nat
5.  If succÂ t1:R then R\=Nat and t1:Nat
6.  If predÂ t1:R then R\=Nat and t1:Nat
7.  If iszeroÂ t1:R then R\=Bool and t1:Nat

From the inversion lemma, we can directly derive a typechecking algorithm:

    
```
    def typeof(t: Expr): T = t match {
        case True | False => Bool
        case If(t1, t2, t3) =>
            val type1 = typeof(t1)
            val type2 = typeof(t2)
            val type3 = typeof(t3)
            if (type1 == Bool && type2 == type3) type2
            else throw Error("not typable")
        case Zero => Nat
        case Succ(t1) => 
            if (typeof(t1) == Nat) Nat
            else throw Error("not typable")
        case Pred(t1) => 
            if (typeof(t1) == Nat) Nat
            else throw Error("not typable")
        case IsZero(t1) => 
            if (typeof(t1) == Nat) Bool
            else throw Error("not typable")
    }
    
```

#### Canonical form

A simple lemma that will be useful for lemma is that of canonical forms. Given a type, it tells us what kind of values we can expect:

1.  If v is a value of type Bool, then v is either true or false
2.  If v is a value of type Nat, then v is a numeric value

The proof is somewhat immediate from the syntax of values.

#### Progress Theorem

**Theorem**: suppose that t is a well-typed term of type T. Then either t is a value, or else there exists some tâ€² such that tâŸ¶tâ€².

**Proof**: by induction on a derivation of t:T.

*   The [T-True](https://kjaer.io/fos/#mjx-eqn-eq%3At-true), [T-False](https://kjaer.io/fos/#mjx-eqn-eq%3At-false) and [T-Zero](https://kjaer.io/fos/#mjx-eqn-eq%3At-zero) are immediate, since t is a value in these cases.
*   For [T-If](https://kjaer.io/fos/#mjx-eqn-eq%3At-if), we have t\=ifÂ t1Â thenÂ t2Â elseÂ t3, with t1:Bool, t2:T and t3:T. By the induction hypothesis, there is some tâ€²1 such that t1âŸ¶tâ€²1.
    
    If t1 is a value, then rule 1 of the [canonical form lemma](https://kjaer.io/fos/#canonical-form) tells us that t1 must be either true or false, in which case [E-IfTrue](https://kjaer.io/fos/#mjx-eqn-eq%3Ae-iftrue) or [E-IfFalse](https://kjaer.io/fos/#mjx-eqn-eq%3Ae-iffalse) applies to t.
    
    Otherwise, if t1âŸ¶tâ€²1, then by [E-If](https://kjaer.io/fos/#mjx-eqn-eq%3Ae-if), tâŸ¶ifÂ tâ€²1Â thenÂ t2Â elseÂ t3
    
*   For [T-Succ](https://kjaer.io/fos/#mjx-eqn-eq%3At-succ), we have t\=succÂ t1.
    
    t1 is a value, by rule 5 of the [inversion lemma](https://kjaer.io/fos/#inversion-lemma) and by rule 2 of the [canonical form](https://kjaer.io/fos/#canonical-form), t1\=nv for some numeric value nv. Therefore, succÂ (t1) is a value. If t1âŸ¶tâ€²1, then tâŸ¶succÂ t1.
    
*   The cases for [T-Zero](https://kjaer.io/fos/#mjx-eqn-eq%3At-zero), [T-Pred](https://kjaer.io/fos/#mjx-eqn-eq%3At-pred) and [T-IsZero](https://kjaer.io/fos/#mjx-eqn-eq%3At-iszero) are similar.

#### Preservation Theorem

**Theorem**: Types are preserved by one-step evaluation. If t:T and tâŸ¶tâ€², then tâ€²:T.

**Proof**: by induction on the given typing derivation

*   For [T-True](https://kjaer.io/fos/#mjx-eqn-eq%3At-true) and [T-False](https://kjaer.io/fos/#mjx-eqn-eq%3At-false), the precondition doesnâ€™t hold (no reduction is possible), so itâ€™s trivially true. Indeed, t is already a value, either t\=Â true or t\=Â false.
*   For [T-If](https://kjaer.io/fos/#mjx-eqn-eq%3At-if), there are three evaluation rules by which tâŸ¶tâ€² can be derived, depending on t1
    *   If t1\=true, then by [E-IfTrue](https://kjaer.io/fos/#mjx-eqn-eq%3Ae-iftrue) we have tâ€²\=t2, and from rule 3 of the [inversion lemma](https://kjaer.io/fos/#inversion-lemma-1) and the assumption that t:T, we have t2:T, that is tâ€²:T
    *   If t1\=false, then by [E-IfFalse](https://kjaer.io/fos/#mjx-eqn-eq%3Ae-iffalse) we have tâ€²\=t3, and from rule 3 of the [inversion lemma](https://kjaer.io/fos/#inversion-lemma-1) and the assumption that t:T, we have t3:T, that is tâ€²:T
    *   If t1âŸ¶tâ€²1, then by the induction hypothesis, tâ€²1:Bool. Combining this with the assumption that t2:T and t3:T, we can apply [T-If](https://kjaer.io/fos/#mjx-eqn-eq%3At-if) to conclude ifÂ tâ€²1Â thenÂ t2Â elseÂ t3:T, that is tâ€²:T

### Messing with it

#### Removing a rule

What if we remove [E-PredZero](https://kjaer.io/fos/#mjx-eqn-eq%3Ae-predzero)? Then `pred 0` type checks, but it is stuck and is not a value; the [progress theorem](https://kjaer.io/fos/#progress-theorem) fails.

#### Changing type-checking rule

What if we change the [T-If](https://kjaer.io/fos/#mjx-eqn-eq%3At-if) to the following?

\frac{t1:Bool  t2:Nat  t3:Nat}{(ifÂ t1Â thenÂ t2Â elseÂ t3):Nat}	(T-If 2)

This doesnâ€™t break our type system. Itâ€™s still sound, but it rejects if-else expressions that return other things than numbers (e.g. booleans). But that is an expressiveness problem, not a soundness problem; our type system disallows things that would otherwise be fine by the evaluation rules.

#### Adding bit

We could add a boolean to natural function `bit(t)`. Weâ€™d have to add it to the grammar, add some evaluation and typing rules, and prove progress and preservation.

bit trueâŸ¶0
bit falseâŸ¶1
\frac{t1âŸ¶tâ€²1}{bitÂ t1âŸ¶bitÂ tâ€²1}
\frac{t:Bool}{bitÂ t:Nat}

Weâ€™ll do something similar this below, so the full proof is omitted.

Simply typed lambda calculus
----------------------------

Simply Typed Lambda Calculus (STLC) is also denoted Î»â†’. The â€œpureâ€ form of STLC is not very interesting on the type-level (unlike for the term-level of pure lambda calculus), so weâ€™ll allow base values that are not functions, like booleans and integers. To talk about STLC, we always begin with some set of â€œbase typesâ€:

```
    T ::=     // types
        Bool    // type of booleans
        T -> T  // type of functions
```
    

In the following examples, weâ€™ll work with a mix of our previously defined toy language, and lambda calculus. This will give us a little syntactic sugar.

    
```
    t ::=                // terms
        x                   // variable
        Î»x. t               // abstraction
        t t                 // application
        true                // constant true
        false               // constant false
        if t then t else t  // conditional
    
    v ::=   // values
        Î»x. t  // abstraction value
        true   // true value
        false  // false value
```

    

### Type annotations

We will annotate lambda-abstractions with the expected type of the argument, as follows:

Î»x:T1.Â t1

We could also omit it, and let type inference do the job (as in OCaml), but for now, weâ€™ll do the above. This will make it simpler, as we wonâ€™t have to discuss inference just yet.

### Typing rules

In STLC, weâ€™ve introduced abstraction. To add a typing rule for that, we need to encode the concept of an environment Î“, which is a set of variable assignments. We also introduce the â€œturnstileâ€ symbol âŠ¢, meaning that the environment can verify the right hand-side typing, or that Î“ must imply the right-hand side.

\frac{(Î“âˆª(x1:T1))âŠ¢t2:T2}{Î“âŠ¢(Î»x:T1.Â t2):T1â†’T2}	(T-Abs)
\frac{x:TâˆˆÎ“}{Î“âŠ¢x:T}		(T-Var)
\frac{Î“âŠ¢t1:T11â†’T12  Î“âŠ¢t2:T11}{Î“âŠ¢t1Â t2:T12} (T-App)

This additional concept must be taken into account in our definition of progress and preservation:

*   **Progress**: If Î“âŠ¢t:T, then either t is a value or else tâŸ¶tâ€² for some tâ€²
*   **Preservation**: If Î“âŠ¢t:T and tâŸ¶tâ€², then Î“âŠ¢tâ€²:T

To prove these, we must take the same steps as above. Weâ€™ll introduce the inversion lemma for typing relations, and restate the canonical forms lemma in order to prove the progress theorem.

### Inversion lemma

Letâ€™s start with the inversion lemma.

1.  If Î“âŠ¢true:R then R\=Bool
2.  If Î“âŠ¢false:R then R\=Bool
3.  If Î“âŠ¢ifÂ t1Â thenÂ t2Â elseÂ t3:R then Î“âŠ¢t1:Bool and Î“âŠ¢t2,t3:R.
4.  If Î“âŠ¢x:R then x:RâˆˆÎ“
5.  If Î“âŠ¢Î»x:T1.Â t2:R then R\=T1â†’T2 for some R2 with Î“âˆª(x:T1)âŠ¢t2:R2
6.  If Î“âŠ¢t1Â t2:R then there is some type T11 such that Î“âŠ¢t1:T11â†’R and Î“âŠ¢t2:T11.

### Canonical form

The canonical forms are given as follows:

1.  If v is a value of type Bool, then it is either true or false
2.  If v is a value of type T1â†’T2 then v has the form Î»x:T1.Â t2

### Progress

Finally, we get to prove the progress by induction on typing derivations.

**Theorem**: Suppose that t is a closed, well typed term (that is, Î“âŠ¢t:T for some type T). Then either t is a value, or there is some tâ€² such that tâŸ¶tâ€².

*   For boolean constants, the proof is immediate as t is a value
*   For variables, the proof is immediate as t is closed, and the precondition therefore doesnâ€™t hold
*   For abstraction, the proof is immediate as t is a value
*   Application is the only case we must treat.
    
    Consider t\=t1Â t2, with Î“âŠ¢t1:T11â†’T12 and Î“âŠ¢t2:T11.
    
    By the induction hypothesis, t1 is either a value, or it can make a step of evaluation. The same goes for t2.
    
    If t1 can reduce, then rule [E-App1](https://kjaer.io/fos/#mjx-eqn-eq%3Ae-app1) applies to t. Otherwise, if it is a value, and t2 can take a step, then [E-App2](https://kjaer.io/fos/#mjx-eqn-eq%3Ae-app2) applies. Otherwise, if they are both values (and we cannot apply Î²\-reduction), then the canonical forms lemma above tells us that t1 has the form Î»x:T11.Â t12, and so rule [E-AppAbs](https://kjaer.io/fos/#mjx-eqn-eq%3Ae-appabs) applies to t.
    

### Preservation

**Theorem**: If Î“âŠ¢t:T and tâŸ¶tâ€² then Î“âŠ¢tâ€²:T.

**Proof**: by induction on typing derivations. We proceed on a case-by-case basis, as we have done so many times before. But one case is hard: application.

For t\=t1Â t2, such that Î“âŠ¢t1:T11â†’T12 and Î“âŠ¢t2:T11, and where T\=T12, we want to show Î“âŠ¢tâ€²:T12.

To do this, we must use the [inversion lemma for evaluation](https://kjaer.io/fos/#inversion-lemma) (note that we havenâ€™t written it down for STLC, but the idea is the same). There are three subcases for it, starting with the following:

The left-hand side is t1\=Î»x:T11.Â t12, and the right-hand side of application t2 is a value v2. In this case, we know that the result of the evaluation is given by tâ€²\=\[xâ†¦v2\]t12.

And here, we already run into trouble, because we do not know about how types act under substitution. We will therefore need to introduce some lemmas.

#### Weakening lemma

Weakening tells us that we can _add_ assumptions to the context without losing any true typing statements:

If Î“âŠ¢t:T, and the environment Î“ has no information about xâ€”that is, xâˆ‰dom(Î“)â€”then the initial assumption still holds if we add information about x to the environment:

(Î“âˆª(x:S))âŠ¢t:T

Moreover, the latter âŠ¢ derivation has the same depth as the former.

#### Permutation lemma

Permutation tells us that the order of assumptions in Î“ does not matter.

If Î“âŠ¢t:T and Î” is a permutation of Î“, then Î”âŠ¢t:T.

Moreover, the latter âŠ¢ derivation has the same depth as the former.

#### Substitution lemma

Substitution tells us that types are preserved under substitution.

That is, if Î“âˆª(x:S)âŠ¢t:T and Î“âŠ¢s:S, then Î“âŠ¢\[xâ†¦s\]t:T.

The proof goes by induction on the derivation of Î“âˆª(x:S)âŠ¢t:T, that is, by cases on the final typing rule used in the derivation.

*   Case [T-App](https://kjaer.io/fos/#mjx-eqn-eq%3At-app): in this case, t\=t1Â t2.
    
    Thanks to typechecking, we know that the environment validates (Î“âˆª(x:S))âŠ¢t1:T2â†’T1 and (Î“âˆª(x:S))âŠ¢t2:T2. In this case, the resulting type of the application is T\=T1.
    
    By the induction hypothesis, Î“âŠ¢\[xâ†¦s\]t1:T2â†’T1, and Î“âŠ¢\[xâ†¦s\]t2:T2.
    
    By [T-App](https://kjaer.io/fos/#mjx-eqn-eq%3At-app), the environment then also verifies the application of these two substitutions as T: Î“âŠ¢\[xâ†¦s\]t1Â \[xâ†¦s\]t2:T. We can factorize the substitution to obtain the conclusion, i.e. Î“âŠ¢\[xâ†¦s\](t1Â t2):T
    
*   Case [T-Var](https://kjaer.io/fos/#mjx-eqn-eq%3At-var): if t\=z (t is a simple variable z) where z:Tâˆˆ(Î“âˆª(x:S)). There are two subcases to consider here, depending on whether z is x or another variable:
    *   If z\=x, then \[xâ†¦s\]z\=s. The result is then Î“âŠ¢s:S, which is among the assumptions of the lemma
    *   If zâ‰ x, then \[xâ†¦s\]z\=z, and the desired result is immediate
*   Case [T-Abs](https://kjaer.io/fos/#mjx-eqn-eq%3At-abs): if t\=Î»y:T2.Â t1, with T\=T2â†’T1, and (Î“âˆª(x:S)âˆª(y:T2))âŠ¢t1:T1.
    
    Based on our [hygiene convention](https://kjaer.io/fos/#alpha-conversion), we may assume xâ‰ y and yâˆ‰fv(s).
    
    Using [permutation](https://kjaer.io/fos/#permutation-lemma) on the first given subderivation in the lemma (Î“âˆª(x:S)âŠ¢t:T), we obtain (Î“âˆª(y:T2)âˆª(x:S))âŠ¢t1:T1 (we have simply changed the order of x and y).
    
    Using [weakening](https://kjaer.io/fos/#weakening-lemma) on the other given derivation in the lemma (Î“âŠ¢s:S), we obtain (Î“âˆª(y:T2))âŠ¢s:S.
    
    By the induction hypothesis, (Î“âˆª(y:T2))âŠ¢\[xâ†¦s\]t1:T1.
    
    By [T-Abs](https://kjaer.io/fos/#mjx-eqn-eq%3At-abs), we have Î“âŠ¢(Î»y:T2.Â \[xâ†¦s\]t1):T1
    
    By the definition of substitution, this is Î“âŠ¢(\[xâ†¦s\]Î»y:T2.Â t1):T2â†’T1.
    

#### Proof

Weâ€™ve now proven the following lemmas:

*   Weakening
*   Permutation
*   Type preservation under substitution
*   Type preservation under reduction (i.e. preservation)

We wonâ€™t actually do the proof, weâ€™ve just set up the pieces we need for it.

### Erasure

Type annotations do not play any role in evaluation. In STLC, we donâ€™t do any run-time checks, we only run compile-time type checks. Therefore, types can be removed before evaluation. This often happens in practice, where types do not appear in the compiled form of a program; theyâ€™re typically encoded in an untyped fashion. The semantics of this conversion can be formalized by an erasure function:

erase(x)\=x
erase(Î»x:T1.t2)\=Î»x.erase(t2)
erase(t1Â t2)\=erase(t1)Â erase(t2)

### Curry-Howard Correspondence

The Curry-Howard correspondence tells us that there is a correspondence between constructive logic and typed lambda-calculus with product and sum types.

An implication PâŠƒQ (which could also be written PâŸ¹Q) can be proven by transforming evidence for P into evidence for Q. A conjunction Pâˆ§Q is a [pair](https://kjaer.io/fos/#pairs-1) of evidence for P and evidence for Q. For more examples of these correspondences, see the [Brouwerâ€“Heytingâ€“Kolmogorov (BHK) interpretation](https://en.wikipedia.org/wiki/Brouwer%E2%80%93Heyting%E2%80%93Kolmogorov_interpretation) or [Curry-Howard correspondence](https://en.wikipedia.org/wiki/Curry%E2%80%93Howard_correspondence) on Wikipedia.


Propositions <-> Types

PâŠƒQ or PâŸ¹Q <-> Function type Pâ†’Q

Pâˆ§Q  <-> [Pair type](https://kjaer.io/fos/#pairs-1) PÃ—Q

Pâˆ¨Q  <-> [Sum type](https://kjaer.io/fos/#sum-type) P+Q

âˆƒxâˆˆS:Ï•(x)  <-> Dependent type âˆ‘x:S,Ï•(x)

âˆ€xâˆˆS:Ï•(x)  <-> âˆ€(x:S):Ï•(x)

Proof of P  <-> Term t of type P

P is provable  <-> Type P is inhabited

Proof simplification  <-> Evaluation

In Scala, all types are inhabited except for the bottom type `Nothing`. Singleton types are only inhabited by a single term.

As an example of the equivalence, weâ€™ll see that application is equivalent to [modus ponens](https://en.wikipedia.org/wiki/Modus_ponens):

\frac{Î“âŠ¢t1:PâŠƒQ  Î“âŠ¢t2:P}{Î“âŠ¢t1Â  t2:Q}

This also tells us that if we can prove something, we can evaluate it.

How can we prove the following? Remember that â†’ is right-associative.

(Aâˆ§B)â†’Câ†’((Câˆ§A)âˆ§B)

The proof is actually a somewhat straightforward conversion to lambda calculus:

Î»p:AÃ—B.Â Î»c:C.Â pair(pair(cÂ fst(p))Â snd(p))

### Extensions to STLC

#### Base types

Up until now, weâ€™ve defined our base types (such as Nat and Bool) manually: weâ€™ve added them to the syntax of types, with associated constants (zero,true,false) and operators (succ,pred), as well as associated typing and evaluation rules.

This is a lot of minutiae though, especially for theoretical discussions. For those, we can often ignore the term-level inhabitants of the base types, and just treat them as uninterpreted constants: we donâ€™t really need the distinction between constants and values. For theory, we can just assume that some generic base types (e.g. B and C) exist, without defining them further.

#### Unit type

In C-like languages, this type is usually called `void`. To introduce it, we do not add any computation rules. We must only add it to the grammar, values and types, and then add a single typing rule that trivially verifies units.

Î“âŠ¢unit:Unit	(T-Unit)

Units are not too interesting, but _are_ quite useful in practice, in part because they allow for other extensions.

#### Sequencing

We can define sequencing as two statements following each other:

```
    t ::=
        ...
        t1; t2
```
    

This implies adding some evaluation and typing rules, defined below:

\frac{t1âŸ¶tâ€²1}{t1;Â t2âŸ¶tâ€²1;Â t2}		(E-Seq)
(unit;Â t2)âŸ¶t2			(E-SeqNext)
\frac{Î“âŠ¢t1:Unit  Î“âŠ¢t2:T2}{Î“âŠ¢t1;Â t2:T2} 	(T-Seq)

But thereâ€™s another way that we could define sequencing: simply as syntactic sugar, a derived form for something else. In this way, we define an external language, that is transformed to an internal language by the compiler in the desugaring step.

t1;Â t2\=(Î»x:Unit.Â t2)Â t1	whereÂ xâˆ‰Â FV(t2)

This is useful to know, because it makes proving soundness much easier. We do not need to re-state the inversion lemma, re-prove preservation and progress. We can simple rely on the proof for the underlying internal language.

#### Ascription

    
```
    t ::=
        ...
        t as T
```
    

Ascription allows us to have a compiler type-check a term as really being of the correct type:

\frac{Î“âŠ¢t1:T}{Î“âŠ¢t1Â asÂ T:T}	(T-Ascribe)

This seems like it preserves soundness, but instead of doing the whole proof over again, weâ€™ll just propose a simple desugaring, in which an ascription is equivalent to the term t applied the identity function, typed to return T:

tÂ asÂ T\=(Î»x:T.Â x)Â t

Alternatively, we could do the whole proof over again, and institute a simple evaluation rule that ignores the ascription.

v1Â asÂ TâŸ¶v1	(E-Ascribe)

#### Pairs

We can introduce pairs into our grammar.

    
```
    t ::= 
        ...
        {t, t}    // pair
        t.1       // first projection
        t.2       // second projection
    
    v ::=
        ...
        {v, v}    // pair value
    
    T ::=
        ...
        T1 x T2   // product types
 ```   
    

Note that product types are right-associative: AÃ—BÃ—C\=AÃ—(BÃ—C). We can also introduce evaluation rules for pairs:

{v1,v2}.1âŸ¶v1	(E-PairBeta1)
{v1,v2}.2âŸ¶v2	(E-PairBeta2)
\frac{t1âŸ¶tâ€²1}{t1.1âŸ¶tâ€²1.1}	(E-Proj1)
\frac{t1âŸ¶tâ€²1}{t1.2âŸ¶tâ€²1.2}	(E-Proj2)
\frac{t1âŸ¶tâ€²1}{{t1,t2}âŸ¶{tâ€²1,t2}}	(E-Pair1)
\frac{t2âŸ¶tâ€²2}{{t1,t2}âŸ¶{t1,tâ€²2}}	(E-Pair2)

The typing rules are then:

\frac{Î“âŠ¢t1:T1  Î“âŠ¢t2:T2}{Î“âŠ¢{t1,t2}:T1Ã—T2}	(T-Pair)
\frac{Î“âŠ¢t1:T11Ã—T12}{Î“âŠ¢t1.1:T11}		(T-Proj1)
\frac{Î“âŠ¢t1:T11Ã—T12}{Î“âŠ¢t1.2:T12} 	(T-Proj2)

Pairs have to be added â€œthe hard wayâ€: we do not really have a way to define them in a derived form, as we have no existing language features to piggyback onto.

#### Tuples

Tuples are like pairs, except that we do not restrict it to 2 elements; we allow an arbitrary number from 1 to n. We can use pairs to encode tuples: `(a, b, c)` can be encoded as `(a, (b, c))`. Though for performance and convenience, most languages implement them natively.

#### Records

We can easily generalize tuples to records by annotating each field with a label. A record is a bundle of values with labels; itâ€™s a map of labels to values and types. Order of records doesnâ€™t matter, the only index is the label.

If we allow numeric labels, then we can encode a tuple as a record, where the index implicitly encodes the numeric label of the record representation.

No mainstream language has language-level support for records (two case classes in Scala may have the same arguments but a different constructor, so itâ€™s not quite the same; records are more like anonymous objects). This is because theyâ€™re often quite inefficient in practice, but weâ€™ll still use them as a theoretical abstraction.

### Sums and variants

#### Sum type

A sum type T\=T1+T2 is a _disjoint_ union of T1 and T2. Pragmatically, we can have sum types in Scala with case classes extending an abstract object:

```
    sealed trait Option[+T]
    case class Some[+T] extends Option[T]
    case object None extends Option[Nothing]
```
    

In this example, `Option = Some + None`. We say that T1 is on the left, and T2 on the right. Disjointness is ensured by the tags inl and inr. We can _think_ of these as functions that inject into the left or right of the sum type T:

inl:T1â†’T1+T2
inr:T2â†’T1+T2

Still, these arenâ€™t really functions, they donâ€™t actually have function type. Instead, we use them them to tag the left and right side of a sum type, respectively.

Another way to think of these stems from [Curry-Howard correspondence](https://kjaer.io/fos/#curry-howard-correspondence). Recall that in the [BHK interpretation](https://en.wikipedia.org/wiki/Brouwer%E2%80%93Heyting%E2%80%93Kolmogorov_interpretation), a proof of Pâˆ¨Q is a pair `<a, b>` where `a` is 0 (also denoted inl) and `b` a proof of P, _or_ `a` is 1 (also denoted inr) and `b` is a proof of Q.

To use elements of a sum type, we can introduce a `case` construct that allows us to pattern-match on a sum type, allowing us to distinguishing the left type from the right one.

We need to introduce these three special forms in our syntax:

    
``` 
    t ::= ...                           // terms
        inl t                              // tagging (left)
        inr t                              // tagging (right)
        case t of inl x => t | inr x => t  // case
    
    v ::= ... // values
        inl v   // tagged value (left)
        inr v   // tagged value (right)
    
    T ::= ...  // types
        T + T     // sum type
```

    

This also leads us to introduce some new evaluation rules:

  \text{case } (\text{inl } v_0) \text{ of } \\
  \quad \text{inl } x_1 \Rightarrow t_1 \\
  \quad \text{inr } x_2 \Rightarrow t_2 \\
  \rightarrow [x_1 \mapsto v_0]t_1 	(E-CaseInl)


\text{case } (\text{inr } v_0) \text{ of } \\
\quad \text{inl } x_1 \Rightarrow t_1 \\
\quad \text{inr } x_2 \Rightarrow t_2 \\
\rightarrow [x_2 \mapsto v_0]t_2	(E-CaseInr)

\frac{\text{case } t_0 \text{ of }}
{\quad \text{inl } x_1 \Rightarrow t_1 \\
\quad \text{inr } x_2 \Rightarrow t_2 \\
\rightarrow \text{case } t_0' \text{ of } \\
\quad \text{inl } x_1 \Rightarrow t_1 \\
\quad \text{inr } x_2 \Rightarrow t_2 \\
\text{where } t_0 \rightarrow t_0'}	(E-Case)

\frac{t_1 \rightarrow t_1'}
{\text{inl } t_1 \rightarrow \text{inl } t_1'}	(E-Inl)

\frac{t_1 \rightarrow t_1'}
{\text{inr } t_1 \rightarrow \text{inr } t_1'}	(E-Inr)

And weâ€™ll also introduce three typing rules:

\frac{Î“âŠ¢t1:T1}{Î“âŠ¢inlÂ t1:T1+T2}		(T-Inl)
\frac{Î“âŠ¢t1:T2}{Î“âŠ¢inrÂ t1:T1+T2}		(T-Inr)
\frac{Î“âŠ¢t0:T1+T2  Î“âˆª(x1:T1)âŠ¢t1:T   Î“âˆª(x2:T2)âŠ¢t2:T}{Î“âŠ¢caseÂ t0Â of inlÂ x1â‡’t1âˆ£inrÂ x2â‡’t2:T}	(T-Case)

#### Sums and uniqueness of type

The rules [T-Inr](https://kjaer.io/fos/#mjx-eqn-eq%3At-inr) and [T-Inl](https://kjaer.io/fos/#mjx-eqn-eq%3At-inl) may seem confusing at first. We only have one type to deduce from, so what do we assign to T2 and T1, respectively? These rules mean that we have lost uniqueness of types: if t has type T, then inlÂ t has type T+U **for every** U.

There are a couple of solutions to this:

1.  We can infer U as needed during typechecking
2.  Give constructors different names and only allow each name to appear in one sum type. This requires generalization to [variants](https://kjaer.io/fos/#variants), which weâ€™ll see next. OCaml adopts this solution.
3.  Annotate each inl and inr with the intended sum type.

For now, we donâ€™t want to look at type inference and variance, so weâ€™ll choose the third approach for simplicity. Weâ€™ll introduce these annotation as ascriptions on the injection operators in our grammar:

```
    t ::=
        ...
        inl t as T
        inr t as T
    
    v ::=
        ...
        inl v as T
        inr v as T
```
    

The evaluation rules would be exactly the same as previously, but with ascriptions in the syntax. The injection operators just now also specify _which_ sum type weâ€™re injecting into, for the sake of uniqueness of type.

#### Variants

Just as we generalized binary products to labeled records, we can generalize binary sums to labeled variants. We can label the members of the sum type, so that we write âŸ¨l1:T1,l2:T2âŸ© instead of T1+T2 (l1 and l2 are the labels).

As a motivating example, weâ€™ll show a useful idiom that is possible with variants, the optional value. Weâ€™ll use this to create a table. The example below is just like in OCaml.

    
```
    OptionalNat = <none: Unit,  some: Nat>;
    Table = Nat -> OptionalNat;
    emptyTable = Î»t: Nat. <none=unit> as OptionalNat;
    
    extendTable = 
        Î»t: Table. Î»key: Nat. Î»val: Nat.
            Î»search: Nat.
                if (equal search key) then <some=val> as OptionalNat
                else (t search)
```
    

The implementation works a bit like a linked list, with linear look-up. We can use the result from the table by distinguishing the outcome with a `case`:

``` 
    x = case t(5) of
        <none=u> => 999
      | <some=v> => v
```
    

### Recursion

In STLC, all programs terminate. Weâ€™ll [go into a little more detail later](https://kjaer.io/fos/#strong-normalization), but the main idea is that evaluation of a well-typed program is guaranteed to halt; we say that the well-typed terms are _normalizable_.

Indeed, the infinite recursions from untyped lambda calculus (terms like omega and fix) are not typable, and thus cannot appear in STLC. Since we canâ€™t express fix in STLC, instead of defining it as a term in the language, we can add it as a primitive instead to get recursion.

```   
    t ::=
        ...
        fix t
```
    

Weâ€™ll need to add evaluation rules recreating its behavior, and a typing rule that restricts its use to the intended use-case.

fixÂ (Î»x:T1.Â t2)âŸ¶\[xâ†¦(fixÂ (Î»x:T1.Â t2))\]t2	(E-FixBeta)
\frac{t1âŸ¶tâ€²1}{fixÂ t1âŸ¶fixÂ tâ€²1}		(E-Fix)
\frac{Î“âŠ¢t1:T1â†’T1}{Î“âŠ¢fixÂ t1:T1} 	(T-Fix)

In order for a function to be recursive, the function needs to map a type to the same type, hence the restriction of T1â†’T1. The type T1 will itself be a function type if weâ€™re doing a recursion. Still, note that the type system doesnâ€™t enforce this. There will actually be situations in which it will be handy to use something else than a function type inside a fix operator.

Seeing that this fixed-point notation can be a little involved, we can introduce some nice syntactic sugar to work with it:

letrecÂ x:T1\=t1Â inÂ t2\=letÂ x\=fixÂ (Î»x:T1.Â t1)Â inÂ t2

This t1 can now refer to the x; thatâ€™s the convenience offered by the construct. Although we donâ€™t strictly need to introduce typing rules (itâ€™s syntactic sugar, weâ€™re relying on existing constructs), a typing rule for this could be:

\frac{Î“âˆª(x:T1)âŠ¢t1:T1	Î“âˆª(x:T1)âŠ¢t2:T2}{Î“âŠ¢letrecÂ x:T1\=t1Â inÂ t2:T2}

In Scala, a common error message is that a recursive function needs an explicit return type, for the same reasons as the typing rule above.

### References

#### Mutability

In most programming languages, variables are (or can be) mutable. That is, variables can provide a name referring to a previously calculated value, as well as a way of overwriting this value with another (under the same name). How can we model this in STLC?

Some languages (e.g. OCaml) actually formally separate variables from mutation. In OCaml, variables are only for naming, the binding between a variable and a value is immutable. However, there is the concept of _mutable values_, also called _reference cells_ or _references_. This is the style weâ€™ll study, as it is easier to work with formally. A mutable value is represented in the type-level as a `Ref T` (or perhaps even a `Ref(Option T)`, since the null pointer cannot produce a value).

The basic operations are allocation with the `ref` operator, dereferencing with `!` (in C, we use the `*` prefix), and assignment with `:=`, which updates the content of the reference cell. Assignment returns a `unit` value.

#### Aliasing

Two variables can reference the same cell: we say that they are _aliases_ for the same cell. Aliasing is when we have different references (under different names) to the same cell. Modifying the value of the reference cell through one alias modifies the value for all other aliases.

The possibility of aliasing is all around us, in object references, explicit pointers (in C), arrays, communication channels, I/O devices; thereâ€™s practically no way around it. Yet, alias analysis is quite complex, costly, and often makes is hard for compilers to do optimizations they would like to do.

With mutability, the order of operations now matters; `r := 1; r := 2` isnâ€™t the same as `r := 2; r := 1`. If we recall the [Church-Rosser theorem](https://kjaer.io/fos/#confluence-in-full-beta-reduction), weâ€™ve lost the principle that all reduction paths lead to the same result. Therefore, some language designers disallow it (Haskell). But there are benefits to allowing it, too: efficiency, dependency-driven data flow (e.g. in GUI), shared resources for concurrency (locks), etc. Therefore, most languages provide it.

Still, languages without mutability have come up with a bunch of abstractions that allow us to have some of the benefits of mutability, like monads and lenses.

#### Typing rules

Weâ€™ll introduce references as a type `Ref T` to represent a variable of type `T`. We can construct a reference as `r = ref 5`, and access the contents of the reference using `!r` (this would return `5` instead of `ref 5`).

Letâ€™s define references in our language:

    
```
    t ::=          // terms
        unit          // unit constant
        x             // variable
        Î»x: T. t      // abstraction
        t t           // application
        ref t         // reference creation
        !t            // dereference
        t := t        // assignment
```

   
\frac{Î“âŠ¢t1:T1}{Î“âŠ¢refÂ t1:RefÂ T1}		(T-Ref)
\frac{Î“âŠ¢t1:RefÂ T1}{Î“âŠ¢!t1:T1}		(T-Deref)
\frac{Î“âŠ¢t1:RefÂ T1  Î“âŠ¢t2:T1}{Î“âŠ¢t1:\=t2:Unit}	(T-Assign)

#### Evaluation

What is the _value_ of `ref 0`? The crucial observation is that evaluation `ref 0` must _do_ something. Otherwise, the two following would behave the same:

    
``` 
    r = ref 0
    s = ref 0
    
    r = ref 0 
    s = r
```
    

Evaluating `ref 0` should allocate some storage, and return a reference (or pointer) to that storage. A reference names a location in the **store** (also known as the _heap_, or just _memory_). Concretely, the store could be an array of 8-bit bytes, indexed by 32-bit integers. More abstractly, itâ€™s an array of values, or even more abstractly, a partial function from locations to values.

We can introduce this idea of locations in our syntax. This syntax is exactly the same as the previous one, but adds the notion of locations:

    
```
    v ::=         // values
        unit         // unit constant
        Î»x: T. t     // abstraction value
        l            // store location
    
    t ::=         // terms
        unit         // unit constant
        x            // variable
        Î»x: T. t     // abstraction
        t t          // application
        ref t        // reference creation
        !t           // dereference
        t := t       // assignment
        l            // store location 
```
    

This doesnâ€™t mean that weâ€™ll allow programmers to write explicit locations in their programs. We just use this as a modeling trick; weâ€™re enriching the internal language to include some run-time structures.

With this added notion of stores and locations, the result of an evaluation now depends on the store in which it is evaluated, which we need to reflect in our evaluation rules. Evaluation must now include terms t **and** store Î¼:

tâˆ£Î¼âŸ¶tâ€²âˆ£Î¼â€²

Letâ€™s take a look for the evaluation rules for STLC with references, operator by operator.

\frac{t1âˆ£Î¼âŸ¶tâ€²1âˆ£Î¼â€²}{t1:\=t2âˆ£Î¼âŸ¶tâ€²1:\=t2âˆ£Î¼â€²}		(E-Assign1)
\frac{t2âˆ£Î¼âŸ¶tâ€²2âˆ£Î¼â€²}{t1:\=t2âˆ£Î¼âŸ¶t1:\=tâ€²2âˆ£Î¼â€²}		(E-Assign2)
l:\=v2âˆ£Î¼âŸ¶unitâˆ£\[lâ†¦v2\]Î¼			(E-Assign)

The assignments [E-Assign1](https://kjaer.io/fos/#mjx-eqn-eq%3Ae-assign1) and [E-Assign2](https://kjaer.io/fos/#mjx-eqn-eq%3Ae-assign2) evaluate terms until they become values. When they have been reduced, we can do that actual assignment: as per [E-Assign](https://kjaer.io/fos/#mjx-eqn-eq%3Ae-assign), we update the store and return return `unit`.

\frac{t1âˆ£Î¼âŸ¶tâ€²1âˆ£Î¼â€²}{refÂ t1âˆ£Î¼âŸ¶refÂ tâ€²1âˆ£Î¼â€²}		(E-Ref)
\frac{lâˆ‰dom(Î¼)}{refÂ v1âˆ£Î¼âŸ¶lâˆ£(Î¼âˆª(lâ†¦v1))}		(E-RefV)

A reference refÂ t1 first evaluates t1 until it is a value ([E-Ref](https://kjaer.io/fos/#mjx-eqn-eq%3Ae-ref)). To evaluate the reference operator, we find a fresh location l in the store, to which it binds v1, and it returns the location l.

\frac{t1âˆ£Î¼âŸ¶tâ€²1âˆ£Î¼â€²}{!t1âˆ£Î¼âŸ¶!tâ€²1âˆ£Î¼â€²}		(E-Deref)
\frac{Î¼(l)\=v}{!lâˆ£Î¼âŸ¶vâˆ£Î¼}	(E-DerefLoc)

We find the same congruence rule as usual in [E-Deref](https://kjaer.io/fos/#mjx-eqn-eq%3Ae-deref), where a term !t1 first evaluates t1 until it is a value. Once it is a value, we can return the value in the current store using [E-DerefLoc](https://kjaer.io/fos/#mjx-eqn-eq%3Ae-derefloc).

The evaluation rules for abstraction and application are augmented with stores, but otherwise unchanged.

#### Store typing

What is the type of a location? The answer to this depends on what is in the store. Unless we specify it, a store could contain anything at a given location, which is problematic for typechecking. The solution is to type the locations themselves. This leads us to a typed store:

Î¼\=(l1â†¦Nat,l2â†¦Î»x:Unit.x)

As a first attempt at a typing rule, we can just say that the type of a location is given by the type of the value in the store at that location:

\frac{Î“âŠ¢Î¼(l):T1}{Î“âŠ¢l:RefÂ T1}

This is problematic though; in the following, the typing derivation for !l2 would be infinite because we have a cyclic reference:

Î¼\=Â (l1â†¦Î»x:Nat.Â !l2Â x,l2â†¦Î»x:Nat.Â !l1Â x)

The core of the problem here is that we would need to recompute the type of a location every time. But shouldnâ€™t be necessary. Seeing that references are strongly typed as `Ref T`, we know exactly what type of value we can place in a given store location. Indeed, the typing rules we chose for references guarantee that a given location in the store always is used to hold values of the same type.

So to fix this problem, we need to introduce a **store typing**. This is a partial function from location to types, which weâ€™ll denote by Î£.

Suppose weâ€™re given a store typing Î£ describing the store Î¼. We can use Î£ to look up the types of locations, without doing a lookup in Î¼:

Î£(l)\=T1Î“âˆ£Î£âŠ¢l:RefÂ T1(T-Loc)

This tells us how to check the store typing, but how do we create it? We can start with an empty typing Î£\=âˆ…, and add a typing relation with the type of v1 when a new location is created during evaluation of [E-RefV](https://kjaer.io/fos/#mjx-eqn-eq%3Ae-refv).

The rest of the typing rules remain the same, but are augmented with the store typing. So in conclusion, we have updated our evaluation rules with a _store_ Î¼, and our typing rules with a _store typing_ Î£.

#### Safety

Letâ€™s take a look at progress and preservation in this new type system. Preservation turns out to be more interesting, so letâ€™s look at that first.

Weâ€™ve added a store and a store typing, so we need to add those to the statement of preservation to include these. Naively, weâ€™d write:

Î“âˆ£Î£âŠ¢t:TÂ andÂ tâˆ£Î¼âŸ¶tâ€²âˆ£Î¼â€²âŸ¹Î“âˆ£Î£âŠ¢tâ€²:T

But this would be wrong! In this statement, Î£ and Î¼ would not be constrained to be correlated at all, which they need to be. This constraint can be defined as follows:

A store Î¼ is well typed with respect to a typing context Î“ and a store typing Î£ (which we denote by Î“âˆ£Î£âŠ¢Î¼) if the following is satisfied:

dom(Î¼)\=dom(Î£)andÎ“âˆ£Î£âŠ¢Î¼(l):Î£(l),Â âˆ€lâˆˆdom(Î¼)

This gets us closer, and we can write the following preservation statement:

Î“âˆ£Î£âŠ¢t:TÂ andÂ tâˆ£Î¼âŸ¶tâ€²âˆ£Î¼Â andÂ Î“âˆ£Î£âŠ¢Î¼âŸ¹Î“âˆ£Î£âŠ¢tâ€²:T

But this is still wrong! When we create a new cell with [E-RefV](https://kjaer.io/fos/#mjx-eqn-eq%3Ae-refv), we would break the correspondence between store typing and store.

The correct version of the progress theorem is the following:

Î“âˆ£Î£âŠ¢t:TÂ andÂ tâˆ£Î¼âŸ¶tâ€²âˆ£Î¼Â andÂ Î“âˆ£Î£âŠ¢Î¼âŸ¹for someÂ Î£â€²âŠ‡Î£,Î“âˆ£Î£â€²âŠ¢tâ€²:T

This progress theorem just asserts that there is _some_ store typing Î£â€²âŠ‡Î£ (agreeing with Î£ on the values of all old locations, but that may have also add new locations), such that tâ€² is well typed in Î£â€².

The progress theorem must also be extended with stores and store typings:

Suppose that t is a closed, well-typed term; that is, âˆ…âˆ£Î£âŠ¢t:T for some type T and some store typing Î£. Then either t is a value or else, for any store Î¼ such that âˆ…âˆ£Î£âŠ¢Î¼[2](https://kjaer.io/fos/#fn:well-typed-store-notation), there is some term tâ€² and store Î¼â€² with tâˆ£Î¼âŸ¶tâ€²âˆ£Î¼â€².

Type reconstruction and polymorphism
------------------------------------

In type checking, we wanted to, given Î“, t and T, check whether Î“âŠ¢t:T. So far, for type checking to take place, we required explicit type annotations.

In this section, weâ€™ll look into **type reconstruction**, which allows us to infer types when type annotations arenâ€™t present: given Î“ and t, we want to find a type T such that Î“âŠ¢t:T.

Immediately, we can see potential problems with this idea:

*   Abstractions without the parameter type annotation seem complicated to reconstruct (a parameter could almost have any type)
*   A term can have many types

To solve these problems, weâ€™ll introduce polymorphism into our type system.

### Constraint-based Typing Algorithm

The idea is to split the work in two: first, we want to generate and record constraints, and then, unify them (that is, attempt to satisfy the constraints).

In the following, weâ€™ll denote constraints as a set of equations {Ti^\=Ui}i\=1,â€¦,m, constraining type variables Ti to actual types Ui.

#### Constraint generation

The constraint generation algorithm can be described as the following function TP:Judgmentâ†’Equations

```
    TP: Judgment -> Equations
    TP(Î“ âŠ¦ t : T) = case t of
        x     :    {Î“(x) ^= T}
    
        Î»x. t1:    let a, b fresh in
                   {(a -> b) ^= T} âˆª
                   TP(Î“, (x: a) âŠ¦ t1 : b)
    
        t1 t2 :    let a fresh in
                   TP(Î“ âŠ¦ t1 : a -> T) âˆª
                   TP(Î“ âŠ¦ t2 : a)
```
    

This creates a set of constraints between type variables and the expected types.

The above essentially gives us constraint generation rules in algorithmic form. An alternative notation is as to give the set of constraint typing relations, which are denoted as:

Î“âŠ¢t:Tâˆ£Ï‡C

This can be read as â€œa term t has type T in the environment Î“ whenever constraints C with type variables Ï‡ are satisfiedâ€. The Ï‡ subscript keeps track of the fresh variables created in the various subderivations, and ensures that they are distinct.

The implementation we gave above could also be described by the following constraint generation rules:

\frac{x:TâˆˆÎ“}{Î“âŠ¢x:Tâˆ£âˆ…{}}		(CT-Var)
\frac{Î“âˆª(x:T1)âŠ¢t2:T2âˆ£Ï‡C}{Î“âŠ¢Î»x:T1.Â t2:T1â†’T2âˆ£Ï‡C}		(CT-Abs)
\frac{Î“âŠ¢t1:T1âˆ£_{Ï‡1} C1Î“âŠ¢t2:T2âˆ£_{Ï‡2}C2}{Î“âŠ¢t1Â t2:Xâˆ£_{Ï‡1âˆªÏ‡2âˆª{X}}C1âˆªC2âˆª{T1Â ^\=Â T2â†’X}	(CT-App)

We havenâ€™t explicitly written it in [CT-App](https://kjaer.io/fos/#mjx-eqn-eq%3Act-app), but we expect Ï‡1 and Ï‡2 to be distinct (i.e. Ï‡1âˆ©Ï‡2\=âˆ…), and we expect X to be fresh (i.e. not clash with anything else).

#### Soundness and completeness

In general a type reconstruction algorithm A assigns to an environment Î“ and a term t a set of types A(Î“,t).

The algorithm is **sound** if for every type TâˆˆA(Î“,t) we can prove the judgment Î“âŠ¢t:T.

The algorithm is **complete** if for every provable judgment Î“âŠ¢t:T we have TâˆˆA(Î“,t).

Soundness and completeness are the two directions of the following implication:

the algorithm can prove itâŸºit holds

Soundness and completeness are about the â‡ and â‡’ directions of the above, respectively. The TP function we defined previously for STLC is sound and complete, and the relationship is thus âŸº. We can write this mathematically as follows:

Î“âŠ¢t:TâŸºâˆƒÂ¯bÂ s.t.Â \[aâ†¦T\]EQNS

Where:

*   a is a new type variable
*   EQNS\=TP(Î“âŠ¢t:a) is the set of type constraints
*   Â¯b\=tv(EQNS)âˆ–tv(Î“), where tv denotes the set of free type variables.
*   \[aâ†¦T\]EQNS is notation for replacing a with T in EQNS

#### Substitutions

Now that weâ€™ve generated a constraint set in the form C\={TiÂ ^\=Â Ui}i\=1,â€¦,m, weâ€™d like a way to substitute these constraints into real types. We must generate a set of substitutions:

s\={ajâ†¦Tâ€²j}_{j\=1,â€¦,n}

These substitutions cannot be cyclical. The type variables may not appear recursively on their right-hand side (directly or indirectly). We can write this requirement as:

ajâˆ‰tv(Tâ€²k)forÂ j\=1,â€¦,n,Â k\=j,â€¦n

This substitution is an idempotent mapping from type variables to types, mapping all but a finite number of type variables to themselves. We can think of a substitution as a set of equations:

{aÂ ^\=Â T},aâˆ‰tv(T)

Alternatively, we can think of it as a function transforming types (based on the set of equations). Substitution is applied in a straightforward way:

s(X)\= T ifÂ (Xâ†¦T)âˆˆs 	X otherwise
s(Nat)\=Nat
s(Bool)\=Bool
s(Tâ†’U)\=sTâ†’sU

Substitution has two properties:

*   **Idempotence**: s(s(T))\=s(T)
*   **Composition**: (fâˆ˜g)Â x\=f(gÂ x), the composition of substitutions, is also a substitution

The composition of two substitutions f and g is:

fâˆ˜g\=\[Xâ†¦f(T)for eachÂ (Xâ†¦T)âˆˆg \\ Xâ†¦T for eachÂ (Xâ†¦T)âˆˆfÂ withÂ Xâˆ‰dom(g)\]

Essentially, if g modifies X, we apply the f substitution on top of g(X). If it leaves it unchanged, the result is just f(X).

#### Unification

We present a unification algorithm based on Robinsonâ€™s 1965 unification algorithm:

```
    mgu                      : (Type ^= Type) -> Subst -> Subst
    mgu(T ^= U) s            = mgu'(sT ^= sU) s
    
    mgu'(a ^= a) s           = s
    mgu'(a ^= T) s           = s âˆª {a â†’ T} if a âˆ‰ tv(T)
    mgu'(T ^= T) s           = s âˆª {a â†’ T} if a âˆ‰ tv(T)
    mgu'(T -> T' ^= U -> U') = (mgu(T' ^= U') â—¦ mgu(T ^= U)) s
    mgu'(K[T1, ..., Tn] ^= K[U1, ..., Un]) s
                             = (mgu(Tn ^= Un) â—¦ ... â—¦ mgu(T1 ^= U1)) s
    mgu'(T ^= U) s           = error
```
    

This function is called mgu, which stands for most general unifier.

A substitution u is a **unifier** of a set of equations {TiÂ ^\=Â Ui} if uTi\=uUi,âˆ€i. This means that it can find an assignment to the type variables in the constraints so that all equations are trivially true.

The substitution is a **most general unifier** if for every other unifier uâ€² of the same equations, there exists a substitution s such that uâ€²\=sâˆ˜u. In other words, it must be less specific (or more general) than all other unifiers.

If we give the following piece of code to a most general unifier, f will be typed as âˆ€a.aâ†’a, and not Intâ†’Int (more on universal types in [the next chapter](https://kjaer.io/fos/#polymorphism)). Both would be correct, but the former would be most general.

```
    let f = (Î»x. x) in f(3)
```
    

We wonâ€™t prove this, but just state it as a theorem: if we get a set of constraints EQNS which has a unifier, then mgu EQNS{} computes the most general unifier of the constraints. If the constraints do not have a unifier, it fails.

In other words, the TP function is sound and complete.

#### Single-pass unification

Previously, we defined constraint generation. Once we had _all_ the constraints, we passed them on to the unifier, which attempted to find a most general substitution satisfying the constraints.

In practice, however, itâ€™s more common to merge the two, and to unify earlier. This allows us to eliminate some constraints early (which is good for performance), but also to get better error reporting.

    
```
    TP: Judgment -> Subst -> Subst
    TP(Î“ âŠ¦ t : T) = case t of
        x     :    mgu({Î“(x) ^= T})
    
        Î»x. t1:    let a, b fresh in
                   mgu({(a -> b) ^= T}) â—¦
                   TP(Î“, (x: a) âŠ¦ t1 : b)
    
        t1 t2 :    let a fresh in
                   TP(Î“ âŠ¦ t1 : a -> T) â—¦
                   TP(Î“ âŠ¦ t2 : a)
    
```

This works because `mgu` is the _most general_ unifier, meaning that it only generates principal types ([more on these later](https://kjaer.io/fos/#principal-types)) at each step. The means that the algorithm never needs to re-analyze a subterm, as it only makes the minimum commitments to achieve typability at each step.

#### Strong normalization

With this typing inference in place, we can be tempted to try to run this on the diverging Î© that [we defined much earlier](https://kjaer.io/fos/#recursion-in-lambda-calculus), or perhaps on the [Y combinator](https://kjaer.io/fos/#recursion-in-lambda-calculus). But as we said before, self-application is not typable. In fact, we can state a stronger assertion:

**Strong Normalization Theorem**: if âŠ¢t:T, then there is a value V such that tâŸ¶âˆ—V.

In other words, if we can type it, it reduces to a value. In the case of the infinite recursion, we cannot type it, and it does not evaluate to a value (instead, it diverges). So looping infinitely isnâ€™t possible in STLC, which leads us to the corollary of this theorem: **STLC is not Turing complete**.

### Polymorphism

There are multiple forms of polymorphism:

*   **Universal polymorphism** (aka _generic types_): the ability to instantiate type variables
*   **Inclusion polymorphism** (aka _subtying_): the ability to treat a value of a subtype as a value of one of its supertypes
*   **Ad-hoc** (aka _overloading_): the ability to define several versions of the same function name with different types.

Weâ€™ll concentrate on universal polymorphism, of which there are to variants: explicit and implicit.

#### Explicit polymorphism

In STLC, a term can have many types, but a variable or parameter only has one type. With polymorphism, we open this up: we allow functions to be applied to arguments of many types. The resulting system is known as **System F**.

To do this, we can introduce a type abstraction with Î›: this does the same thing as a regular Î», except that it takes a type. For instance, we could build a polymorphic identity function:

id\=Î›X.Â Î»x:X.Â x

Application is like before, except that we write the type in square brackets \[T\] (like in Scala, where we use `[T]`, or `<T>` in Java). For instance, to get the identity function for natural numbers, we write:

idÂ \[Nat\]

This returns Î»x:Nat.Â x, which is an instance of the polymorphic function.

The type of the Î› abstraction is written as âˆ€X.Â Xâ†’X. This polymorphic type notation âˆ€a.T can be used as any other type. The typing rules are:

\frac{Î“âŠ¢t:âˆ€a.T}{Î“âŠ¢t\[U\]:\[aâ†¦U\]T}	(âˆ€E)
\frac{Î“âŠ¢t:T}{Î“âŠ¢Î›a.t:âˆ€a.T}	(âˆ€I)

For instance, the signature of `map` could be written as follows in Scala:

```
    def map[A][B](f: A => B)(xs: List[A]) = ...
```
    

In System F weâ€™d write:

Î›X.Â Î›Y.Â Î»f:Xâ†’Y.Â Î»xs:ListÂ \[X\].Â â€¦

#### Implicit polymorphism

An alternative type system is **Hindley-Milner**, which does not require annotations for parameter types, and instead opts for implicit polymorphism. The idea is that inference treats unannotated named values (i.e. `let ... in ...` statements) as polymorphic types. This explains why this feature is also known as _let-polymorphism_.

To have this feature, we must introduce the notion of **type schemes**. These are not fully general types, but are an internal construct used to type let expressions. A type scheme has the following syntax:

S::\=Tâˆ£âˆ€a.S

Not that a plain type is a type scheme, but that we can also add an arbitrary number of universal type arguments âˆ€a. before it.

The typing rules for the Hindley-Milner are given below. Here, we always use S as a metavariable for type schemes, and T and U for plain (non-polymorphic) types.

Î“âˆª(x:S)âˆªÎ“â€²âŠ¢x:S,xâˆ‰dom(Î“â€²)	(Var)
\frac{Î“âŠ¢t:âˆ€a.T}{Î“âŠ¢t:\[aâ†¦U\]T}	(âˆ€E)
\frac{Î“âŠ¢t:T	aâˆ‰tv(Î“)}{Î“âŠ¢âˆ€a.T}	(âˆ€I)
\frac{Î“âŠ¢t:S	Î“âˆª(x:S)âŠ¢tâ€²:T}{Î“âŠ¢letÂ x\=tÂ inÂ tâ€²:T}	(Let)
\frac{Î“âˆª(x:T)âŠ¢t:T}{Î“âŠ¢Î»x.t:Tâ†’U}		(â†’I)
\frac{Î“âŠ¢t1:Tâ†’U	Î“âŠ¢t2:T}{Î“âŠ¢t1Â t2:U} 	(â†’E)

[Var](https://kjaer.io/fos/#mjx-eqn-eq%3Ahm-var) means that we can verify x:S if (x:S) is in the environment and it isnâ€™t overwritten later (in Î“â€²). This allows us to have some concept of scoping of variables.

[âˆ€E](https://kjaer.io/fos/#mjx-eqn-eq%3Ahm-forall-e) allows to verify specific instances of a polymorphic type, and [âˆ€I](https://kjaer.io/fos/#mjx-eqn-eq%3Ahm-forall-i) allows to generalize to a polymorphic type (with a hygiene condition telling us that the type variable we choose isnâ€™t already in the environment).

[Let](https://kjaer.io/fos/#mjx-eqn-eq%3Ahm-let) is fairly straightforward. [â†’I](https://kjaer.io/fos/#mjx-eqn-eq%3Ahm-arrow-i) and [â†’E](https://kjaer.io/fos/#mjx-eqn-eq%3Ahm-arrow-e) are simply as in STLC.

#### Alternative Hindley Milner

A let-in statement can be regarded as shorthand for a substitution:

letÂ x\=tÂ inÂ tâ€²â‰¡\[xâ†¦t\]tâ€²

We can use this to get a revised Hindley-Milner system which we call HMâ€™, where [Let](https://kjaer.io/fos/#mjx-eqn-eq%3Ahm-let) is replaced by the following:

\frac{Î“âŠ¢t:T	Î“âŠ¢\[xâ†¦t\]tâ€²:U}{Î“âŠ¢letÂ x\=tÂ inÂ tâ€²:U}	(Let')

In essence, it only changes the typing rule for `let` so that they perform a step of evaluation before calculating the types. This is equivalent to the previous HM system; weâ€™ll state that as a theorem, without proof.

**Theorem**: Î“âŠ¢HMt:SâŸºÎ“âŠ¢HMâ€²t:S

The corollary to this theorem is that, if we let tâˆ— be the result of expanding all `let`s in t using the substitution above, then:

Î“âŠ¢HMt:TâŸ¹Î“âŠ¢F1tâˆ—:T

The converse is true if every let-bound name is used at least once:

Î“âŠ¢HMt:TâŸ¸Î“âŠ¢F1tâˆ—:T

### Principal types

We [previously remarked](https://kjaer.io/fos/#unification) that there is a most general unifier, which instantiates the type variables in the most general way, the _principal_ way. Principal types are a small formalization of this idea.

A type T is a **generic instance** of a type scheme S\=âˆ€Î±1.Â â€¦âˆ€Î±n.Â Tâ€² if there is a substitution s on Î±1,â€¦,Î±n such that T\=sTâ€². In this case, we write Sâ‰¤T.

A type scheme Sâ€² is a **generic instance** of a type scheme S iff for all types T:

Sâ€²â‰¤TâŸ¹Sâ‰¤T

In this case, we write Sâ‰¤Sâ€².

A type scheme S is **principal** (or _most general_) for Î“ and t iff:

*   Î“âŠ¢t:S
*   Î“âŠ¢t:Sâ€²âŸ¹Sâ‰¤Sâ€²

A type system TS has the **principal typing property** iff, whenever Î“âŠ¢TSt:S, there exists a principal type scheme for Î“ and t.

In other words, a type system with principal types is one where the type engine doesnâ€™t make any choices; it always finds the most general solution. The type checker may fail if it cannot advance without making a choice (e.g. for Î»x.x+x, where the typechecker would have to choose between Intâ†’Int, Floatâ†’Float, etc).

The following can be stated as a theorem:

1.  HMâ€™ without `let` has the principal typing property
2.  HMâ€™ with `let` has the principal typing property
3.  HM has the principal typing property

Subtyping
---------

### Motivation

Under [T-App](https://kjaer.io/fos/#mjx-eqn-eq%3At-app), the following is not well typed:

(Î»r.Â {x:Nat}.Â r.x)Â {x\=0,y\=1}

Weâ€™re passing a record to a function that selects its `x` member. This is not well typed, but would still evaluate just fine; after all, weâ€™re passing the function a _better_ argument than it needs.

In general, weâ€™d like to be able to define hierarchies of classes, with descendants having richer interfaces. These should still be usable instead of their ancestors. We solve this using subtyping.

We achieve this by introducing a subtyping relation S<:T, and a **subsumption rule**:

\frac{Î“âŠ¢t:S	S<:T}{Î“âŠ¢t:T}	(T-Sub)

This rule tells us that if S<:T, then any value of type S can also be regarded as having type T. With this rule in place, we just need to define the rules for when we can assert S<:T.

### Rules

#### General rules

Subtyping is reflective and transitive:
	
S<:S	(S-Refl)
\frac{S<:U	U<:T}{S<:T}	(S-Trans)

#### Records

To solve our previous example, we can introduce subtyping between record types:

{x:Nat,y:Nat}<:{x:Nat}

Using [T-Sub](https://kjaer.io/fos/#mjx-eqn-eq%3At-sub), we can see that our example is now well-typed. Of course, the subtyping rule we introduced here is too specific; we need something more general. We can do this by introducing three rules for subtyping of record types:

{li:T_i^{iâˆˆ1â€¦n+k}}<:{l_i:T_i^{iâˆˆ1â€¦n}}	(S-RcdWidth)
\frac{{kj:S_j^{jâˆˆ1â€¦n}}Â is a permutation ofÂ {l_i:T_i^{iâˆˆ1â€¦n}}}{{k_j:S_j^{jâˆˆ1â€¦n}}<:{l_i:T_i^{iâˆˆ1â€¦n}}}	(S-RcdPerm)
\frac{âˆ€iÂ S_i<:T_i}{{l_i:S_i^{iâˆˆ1â€¦n}}<:{l_i:T_i^{iâˆˆ1â€¦n}}}	(S-RcdDepth)

[S-RcdWidth](https://kjaer.io/fos/#mjx-eqn-eq%3As-rcdwidth) tells us that a record is a supertype of a record with additional fields to the right. Intuitively, the reason that the record _more_ fields is a _subtype_ of the record with fewer fields is because it places a stronger constraint on values, and thus describes fewer values (think of the Venn diagram of possible values).

Of course, adding fields to the right only is not strong enough of a rule, as order in a record shouldnâ€™t matter. We fix this with [S-RcdPerm](https://kjaer.io/fos/#mjx-eqn-eq%3As-rcdperm), which allows us to reorder the record so that all additional fields are on the right: [S-RcdPerm](https://kjaer.io/fos/#mjx-eqn-eq%3As-rcdperm), [S-RcdWidth](https://kjaer.io/fos/#mjx-eqn-eq%3As-rcdwidth) and [S-Trans](https://kjaer.io/fos/#mjx-eqn-eq%3As-trans) allows us to drop arbitrary fields within records.

Finally, [S-RcdDepth](https://kjaer.io/fos/#mjx-eqn-eq%3As-rcddepth) allows for the types of individual fields to be subtypes of the supertype recordâ€™s fields.

Note that real languages often choose not to adopt these [structural record subtyping](https://kjaer.io/fos/#aside-structural-vs-declared-subtyping) rules. For instance, Java has no depth subtyping (a subclass may not change the argument or result types of a method of its superclass), no permutation for classes (single inheritance means that each member can be assigned a single index; new members can be added as new indices â€œon the rightâ€), but has permutation for interfaces (multiple inheritance of interfaces is allowed).

#### Arrow types

Function types are contravariant in the argument and covariant in the return type. The rule is therefore:

\frac{T1<:S1   S2<:T2}{S1â†’S2<:T1â†’T2}	(S-Arrow)

#### Top type

For convenience, we have a top type that everything can be a subtype of. In Java, this corresponds to `Object`.

S<:Top(S-Top)

#### Aside: structural vs. declared subtyping

The [subtype relation we defined for records](https://kjaer.io/fos/#records-1) is _structural_: we decide whether S is a subtype of T by examining the structure of S and T. By contrast, most OO languages (e.g. Java) use _declared_ subtyping: S is only a subtype of T if the programmer has stated that it should be (with `extends` or `implements`).

Weâ€™ll come back to this when we talk about [Featherweight Java](https://kjaer.io/fos/#featherweight-java).

### Properties of subtyping

#### Safety

The problem with subtyping is that it changes how we do proofs. They become a bit more involved, as the typing relation is no longer syntax directed; when weâ€™re proving things, we need to start making choices, as the rule [T-Sub](https://kjaer.io/fos/#mjx-eqn-eq%3At-sub) could appear anywhere. Still, the proofs are possible.

#### Inversion lemma for subtyping

Before we can prove safety and preservation, weâ€™ll introduce the inversion lemma for subtyping.

**Inversion Lemma**: If U<:T1â†’T2, then U has the form U1â†’U2 with T1<:U1 and U2<:T2.

The proof is by induction on subtyping derivations:

*   Case [S-Arrow](https://kjaer.io/fos/#mjx-eqn-eq%3As-arrow), U\=U1â†’U2: immediate, as U already has the correct form, and as we can deduce T1<:U1 and U2<:T2 from [S-Arrow](https://kjaer.io/fos/#mjx-eqn-eq%3As-arrow).
*   Case [S-Refl](https://kjaer.io/fos/#mjx-eqn-eq%3As-refl), U\=T1â†’T2: by applying [S-Refl](https://kjaer.io/fos/#mjx-eqn-eq%3As-refl) twice, we get T1<:T1 and T2<:T2, as required.
*   Case [S-Trans](https://kjaer.io/fos/#mjx-eqn-eq%3As-trans), U<:W and W<:T1â†’T2
    
    By the IH on the second subderivation, we find that W has the form W1â†’W2 with T1<:W1 and W2<:T2.
    
    Applying the IH again to the first subderivation, we find that U has the form U1â†’U2 with W1<:U1 and U2<:W2
    
    By [S-Trans](https://kjaer.io/fos/#mjx-eqn-eq%3As-trans), we get T1<:U1, and by [S-Trans](https://kjaer.io/fos/#mjx-eqn-eq%3As-trans) again, U2<:T2 as required
    

#### Inversion lemma for typing

Weâ€™ll introduce another lemma, but this time for typing (not subtyping):

**Iversion lemma**: if Î“âŠ¢Î»x:S1.s2:T1â†’T2, then T1<:S1 and Î“âˆª(x:S1)âŠ¢s2:T2.

Again, the proof is by induction on typing derivations:

*   Case [T-Abs](https://kjaer.io/fos/#mjx-eqn-eq%3At-abs), where T1\=S1, T2\=S2 and Î“âˆª(x:S1)âŠ¢s2:S2: the result is immediate (using [S-Refl](https://kjaer.io/fos/#mjx-eqn-eq%3As-refl) to get T1<:S1 from T1\=S1).
*   Case [T-Sub](https://kjaer.io/fos/#mjx-eqn-eq%3At-sub), Î“âŠ¢Î»x:X1.Â s2:U and U<:T1â†’T2
    
    By the [inversion lemma for subtyping](https://kjaer.io/fos/#inversion-lemma-for-subtyping), we have U\=U1â†’U2, with T1<:U1 and U2<:T2.
    
    By the IH, we then have U1<:S1 and Î“âˆª(x:S1)âŠ¢s2:U2.
    
    We can apply [S-Trans](https://kjaer.io/fos/#mjx-eqn-eq%3As-trans) to U1<:S1 and T1<:U1 to get T1<:S1.
    
    We can apply [T-Sub](https://kjaer.io/fos/#mjx-eqn-eq%3At-sub) to the assumptions that Î“âˆª(x:S1)âŠ¢s2:U2 and U2<:T2 to conclude Î“âˆª(x:S1)âŠ¢s2:T2
    

#### Preservation

Remember that preservation states that if Î“âŠ¢t:T and tâŸ¶tâ€² then Î“âŠ¢tâ€²:T.

The proof is by induction on typing derivations:

*   Case [T-Sub](https://kjaer.io/fos/#mjx-eqn-eq%3At-sub): t:S and S<:T.
    
    By the IH, Î“âŠ¢tâ€²:S.
    
    By [T-Sub](https://kjaer.io/fos/#mjx-eqn-eq%3At-sub), Î“âŠ¢t:T.
    
*   Case [T-App](https://kjaer.io/fos/#mjx-eqn-eq%3At-app): t\=t1Â t2, Î“âŠ¢t1:T11â†’T12, Î“âŠ¢t2:T11 and T\=T12. By the inversion lemma for evaluation[3](https://kjaer.io/fos/#fn:inversion-lemma-evaluation-lambda), there are three rules by which tâŸ¶tâ€² can be derived:
    
    *   Subcase [E-App1](https://kjaer.io/fos/#mjx-eqn-eq%3Ae-app1): t1âŸ¶tâ€²1 and tâ€²\=tâ€²1Â t2. The result follows from the IH and [T-App](https://kjaer.io/fos/#mjx-eqn-eq%3At-app)
    *   Subcase [E-App2](https://kjaer.io/fos/#mjx-eqn-eq%3Ae-app2): t1\=v1, t2âŸ¶tâ€²2 and tâ€²\=v1Â tâ€²2. The result follows from the IH and [T-App](https://kjaer.io/fos/#mjx-eqn-eq%3At-app)
    *   Subcase [E-AppAbs](https://kjaer.io/fos/#mjx-eqn-eq%3Ae-appabs): t1\=Î»x:S11.Â t12, t2\=v2 and tâ€²\=\[xâ†¦v2\]t12.
        
        By the [inversion lemma for typing](https://kjaer.io/fos/#inversion-lemma-for-typing), T11<:S11 and Î“âˆª(x:S11)âŠ¢t12:T12.
        
        By [T-Sub](https://kjaer.io/fos/#mjx-eqn-eq%3At-sub), Î“âŠ¢t2:S11
        
        By the [substitution lemma](https://kjaer.io/fos/#substitution-lemma), Î“âŠ¢tâ€²:T12.
        

### Subtyping features

#### Casting

In languages like Java and C++, ascription is a little more interesting than [what we previously defined it as](https://kjaer.io/fos/#ascription). In these languages, ascription serves as a casting operator.

\frac{Î“âŠ¢t1:S}{Î“âŠ¢t1Â asÂ T:T}	(T-Cast)
\frac{âŠ¢rv1:T}{v1Â asÂ TâŸ¶v1}	(E-Cast)

Contrary to [T-Ascribe](https://kjaer.io/fos/#mjx-eqn-eq%3At-ascribe), the [T-Cast](https://kjaer.io/fos/#mjx-eqn-eq%3At-cast) rule allows the ascription to be of a different type than the term. This allows the programmer to have an escape hatch, and get around the type checker. However, this _laissez-faire_ solution means that a run-time check is necessary, as [E-Cast](https://kjaer.io/fos/#mjx-eqn-eq%3Ae-cast) shows.

#### Variants

The subtyping rules for [variants](https://kjaer.io/fos/#variants) are almost identical to those of records, with the main difference being the width rule allows variants to be _added_, not dropped:

âŸ¨li:T_i^{iâˆˆ1â€¦n}âŸ©<:âŸ¨l_i:T_i^{iâˆˆ1â€¦n+k}âŸ©	(S-VariantWidth)
\frac{âˆ€iÂ Si<:Ti}{âŸ¨l1:S_i^{iâˆˆ1â€¦n}âŸ©<:âŸ¨l1:T_i^{iâˆˆ1â€¦n}âŸ©}	(S-VariantDepth)
\frac{âŸ¨kj:S_j^{jâˆˆ1â€¦n}âŸ©Â is a permutation ofÂ âŸ¨li:T_i^{iâˆˆ1â€¦n}âŸ©}{âŸ¨k_j:S_j^{jâˆˆ1â€¦n}âŸ©<:âŸ¨li:T_i^{iâˆˆ1â€¦n}âŸ©}	(S-VariantPerm)
\frac{Î“âŠ¢t1:T1}{Î“âŠ¢âŸ¨l1\=t1âŸ©:âŸ¨l1:T1âŸ©}	(T-Variant)

The intuition for [S-VariantWidth](https://kjaer.io/fos/#mjx-eqn-eq%3As-variantwidth) is that a tagged expression âŸ¨l\=tâŸ© belongs to a variant type âŸ¨li:Tiiâˆˆ1â€¦nâŸ© if the label l is _one of the possible labels_ {li}. This is easy to understand if we consider the [`Option` example that we used previously](https://kjaer.io/fos/#variants): `some` and `none` are subtypes of `Option`.

#### Covariance

`List` is an example of a covariant type constructor: we want `List[None]` to be a subtype of `List[Option]`.

\frac{S1<:T1}{ListÂ S1<:ListÂ T1}		(S-List)

#### Invariance

References are not covariant nor invariant. An example of an invariant constructor is a [reference](https://kjaer.io/fos/#references).

*   When a reference is _read_, the context expects T1 so giving a S1<:T1 is fine
*   When a reference is _written_, the context provides a T1. If the the actual type of the reference is RefÂ S1, someone may later use the T1 as an S1, so we need T1<:S1

Similarly, arrays are invariant, for the same reason:

\frac{S1<:T1	T1<:S1}{ArrayÂ S1<:ArrayÂ T1}	(S-Array)

Instead, Java has covariant arrays:

\frac{S1<:T1}{ArrayÂ S1<:ArrayÂ T1}	(S-ArrayJava)

This is because the Java language designers felt that they needed to be able to write a sort routine for mutable arrays, and implemented this as a quick fix. Instead, it turned out to be a mistake that even the Java designers regret.

The solution to this invariance problem is based on the following observation: a `Ref T` can be used either for reading or writing. To be able to have contravariant reading and covariant writing, we can split a `Ref T` in three:

*   `Source T`: a reference with read capability
*   `Sink T`: a reference cell with write capability
*   `Ref T`: a reference cell with both capabilities

The typing rules then limit dereference to sources, and assignment to sinks:

\frac{Î“âˆ£Î£âŠ¢t1:SourceÂ T11}{Î“âˆ£Î£âŠ¢!t1:T11}		(T-DerefSource)
\frac{Î“âˆ£Î£âŠ¢t1:SinkÂ T11	Î“âˆ£Î£âŠ¢t2:T11}{Î“âˆ£Î£âŠ¢t1:\=t2:Unit}	(T-AssignSink)

The subtyping rules establish sources as covariant constructors, sinks as contravariant, and a reference as a subtype of both:

\frac{S1<:T1}{SourceÂ S1<:SourceÂ T1}	(S-Source)
\frac{T1<:S1}{SinkÂ S1<:SinkÂ T1}		(S-Sink)
RefÂ T1<:SourceÂ T1		(S-RefSource)
RefÂ T1<:SinkÂ T1		(S-RefSink)

### Algorithmic subtyping

So far, in STLC, our typing rules were _syntax directed_. This means that for every for every form of a term, a specific rule applied; which rule to choose was always straightforward.

The reason the choice is so straightforward is because we can divide the positions of a typing relation like [T-App](https://kjaer.io/fos/#mjx-eqn-eq%3At-app) into input positions (Î“ and t), and output positions (T11, T12).

However, by introducing subtyping, we introduced rules that break this: [T-Sub](https://kjaer.io/fos/#mjx-eqn-eq%3At-sub) and [S-Trans](https://kjaer.io/fos/#mjx-eqn-eq%3As-trans) apply to _any_ kind of term, and can appear at any point of a derivation. Every time our type checking algorithm encounters a term, it must decide which rule to apply. [S-Trans](https://kjaer.io/fos/#mjx-eqn-eq%3As-trans) also introduces the problem of having to pick an intermediary type U (which is neither an input nor an output position), for which there can be multiple choices. [S-Refl](https://kjaer.io/fos/#mjx-eqn-eq%3As-refl) also overlaps with the conclusions of other rules, although this is a less severe problem.

But this excess flexibility isnâ€™t strictly needed; we donâ€™t need 1000 ways to prove a given typing or subtyping statement, one is enough. The solution to these problems is to replace the ordinary, _declarative_ typing and subtyping relations with _algorithmic_ relations, whose sets of rules are syntax directed. This implies proving that the algorithmic relations are equivalent to the original ones, that subsumption, transitivity and reflexivity are consequences of our algorithmic rules.

Objects
-------

For simple objects and classes, we can easily use a translational analysis, converting ideas like dynamic dispatch, state, inheritance, into derived forms from lambda calculus such as (higher-order) functions, records, references, recursion, subtyping. However, for more complex features (like `this`), weâ€™ll need a more direct treatment.

In this section, weâ€™ll just identify the core features of object-oriented programming, and propose translations for the simpler features. The more complex features will lead us to defining Featherweight Java.

### Classes

Letâ€™s take a look at an example of a class:

```
    class Counter {
        protected int x = 1;
        int get() { return x; }
        void inc() { x++; }
    }
```
    

To represent this in lambda calculus, we could use a record in a let body:

    
```
 
    let x = ref 1 in {
        get = Î»_: Unit. !x
        inc = Î»_: Unit. x := succ(!x)
    }
```
    

More generally, the state may consist of more than a single reference cell, so we can let the state be represented by a variable `r` corresponding to a record with (potentially) multiple fields.

    
```

    let r = {x = ref 1} in {
        get = Î»_: Unit. !(r.x)
        inc = Î»_: Unit. r.x := succ(!(r.x))
    }
```

    

### Object generators

To create a new object, we can just define a function that creates and returns a `Counter`:

    
```
 
    newCounter = Î»_: Unit. 
        let r = {x = ref 1} in {
            get = Î»_: Unit. !(r.x)
            inc = Î»_: Unit. r.x := succ(!(r.x))
        }
 ```
   
    

This returns a `newCounter` object of type Unitâ†’Counter, where the Counter type is defined as:

Counter\={get:Unitâ†’Nat,Â inc:Unitâ†’Unit}

### Dynamic dispatch

When an operation is invoked on an object, the ensuing behavior depends on the object itself; indeed, two object of the same type may be implemented internally in completely different ways.

For instance, we can define two subclasses doing very different things:

    
```
 class A {
        int x = 0;
        int m() {
            x = x + 1;
            return x;
        }
    }
    
    class B extends A {
        int m() {
            x = x + 5;
            return x;
        }
    }
    
    class C extends A {
        int m() {
            x = x - 10;
            return x;
        }
    }
```

    

Here, `(new B()).m()` and `(new C()).m()` have different results.

Dynamic dispatch is a kind of _late binding_ for function calls. Rather than construct the binding from call to function at compile-time, the idea in dynamic dispatch is to bind at runtime.

### Encapsulation

In most OO languages, each object consists of some internal state. The state is directly accessible to the methods, but inaccessible from the outside.In Java, the encapsulation can be enabled with `protected`, which allows for a sort of information hiding.

The type of an object is just the set of operations that can be performed on it. It doesnâ€™t include the internal state.

### Inheritance and subtyping

Subtyping is a way to talk about types. Inheritance is more focused on the idea of sharing behavior, on avoiding duplication of code.

The basic mechanism of inheritance is classes, which can be:

*   **instantiated** to create new objects (â€œinstancesâ€), _or_
*   **refined** to create new classes (â€œsubclassesâ€). Subclasses are subtypes of their parent classes.

When we refine a class, itâ€™s usually to add methods to it. We saw previously that a record A with more fields than B is a subtype of B; letâ€™s try to extend that behavior to objects.

As an example, letâ€™s try to look at a `ResetCounter` inheriting from `Counter`, adding a `reset` method that sets `x` to 1. In Java, weâ€™d do this as follows:

```
 
    class Counter {
        protected int x = 1;
        int get() {
            return x;
        }
        void increment() {
            x++;
        }
    }
    
    class ResetCounter extends Counter {
        void reset() {
            x = 1;
        }
    }
    
```


How can we implement `ResetCounter` in lambda calculus? Initially, we can just try to do this by coping the `Counter` body into a new object `ResetCounter`, and add a `reset` method:

```
   
    newResetCounter =
        Î»_: Unit. let r = {x = ref 1} in {
            get   = Î»_: Unit. !(r.x),
            inc   = Î»_: Unit. r.x := succ(!(r.x)),
            reset = Î»_: Unit. r.x := 1
        }
    
```


But this goes against the [DRY principle](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself) from software engineering.

Another thing that we could try is to take a `Counter` as an argument in the `ResetCounter` object generator:

    
```

    resetCounterFromCounter = Î»c. Counter. 
        let r = {x = ref 1} in {
            get   = c.get,
            inc   = c.inc,
            reset = Î»_: Unit. r.x := 1
        }
    
```


However, this is problematic because weâ€™re not sharing the state; weâ€™ve got two separate counts in `Counter` and `ResetCounter`, and they cannot access each otherâ€™s state.

To solve this, we must separate the method definition from the object generator. To do this, we can use the age-old computer science adage of â€œevery problem can be solved with an additional level of indirectionâ€.

    
```

    counterClass = Î»r: CounterState. {
        get = Î»_: Unit. !(r.x),
        inc = Î»_: Unit. r.x := succ(!(r.x))
    };
    
    newCounter = Î»_: Unit. 
        let r = {x = ref 1} in counterClass r;
```
    

To define the subclass, weâ€™ll first have to introduce the notion of `super`. We know this construct from Java, among others. Javaâ€™s `super` gives us a mechanism to avoid dynamic dispatch, since we _specifically_ call the method in the superclass we inherit from.

For the subclass, the idea is to instantiate the `super`, and bind the methods of the object to the `super`â€™s methods. The classes both have access to the same value through the use of references.

    
```

    resetCounterClass = Î»r: CounterState.
        let super = counterClass r in {
            get   = super.get,
            inc   = super.inc,
            reset = Î»_: Unit. r.x := 1
        };
    
    newResetCounter = Î»_: Unit.
        let r = {x = ref 1} in resetCounterClass r;
    
```


This also allows us to call `super` in added or redefined methods (so `reset` could call `super.inc` if it needed to, or we could redefine `inc` to add functionality around `super.inc`).

Our state record `r` can even contain more variable than the superclass needs, as records with more fields are subtypes of those with a subset of fields. This allows us to have more instance variables in the subclass.

Note that if we wanted to be more rigorous, weâ€™d have to define this more precisely. In STLC, weâ€™ve defined records through _structural subtyping_. But most object-oriented languages use _nominal subtyping_, where things arenâ€™t subtypes of each other just because they have the same methods, but because we declare them to be so. Weâ€™ll see [more on this later](https://kjaer.io/fos/#structural-vs-nominal-type-systems).

### This

Above, we saw how to call methods from the parent class through `super`. To call methods between each other, we need to add `this`.

Letâ€™s consider the following class as an example:

    
```
 
    class SetCounter {
        protected int x = 0;
        int get() {
            return x;
        }
        void set(int i) {
            x = i
        }
        void inc() {
            this.set(this.get() + 1);
        }
    }
    
```

This example may be a little simplistic, but in practice, itâ€™s very useful to be able to use other methods within the same class.

In an initial attempt at implementing this in lambda calculus, we can add a fix operator to the class definition, so that we can call ourselves:

    
```
 
    setCounterClass = Î»r: CounterState. 
        fix (Î»this: SetCounter. {
            get = Î»_: Unit. !(r.x),
            set = Î»i: Nat.  r.x := i,
            int = Î»_: Unit. this.set (succ (this.get unit))
        });
```

    

As a small sanity check, we pass a SetCounterâŸ¶SetCounter type to `fix` operation, so the resulting type is indeed SetCounter.

We have â€œtied the knotâ€ by using the `fix` operator, which arranges for the very record we built to also be passed as `this`.

But this does not model the behavior of `this` in most object-oriented languages, which support a more general form of recursive call between methods, known as _open recursion_. This allows the methods of a superclass to call the methods of a subclass through `this`.

The problem here is that the fixed point operation is â€œclosedâ€: it only gives us the exact set we built in `this`, and isnâ€™t open to extension. To solve this, we can move the application of `fix` from the class definition to the object creation function (essentially switching the order of `fix` and `Î»r: CounterState`):

    
```

    setCounterClass = Î»r: CounterState. 
        Î»this: SetCounter. {
            get = Î»_: Unit. !(r.x),
            set = Î»i: Nat.  r.x := i,
            int = Î»_: Unit. this.set (succ (this.get unit))
        };
    
    newSetCounter = Î»_: Unit.
        let r = {x = ref 1} in fix (setCounterClass r);
    
```

Note that this changes the type signature of the class, which goes from:

SetCounterClass:CounterStateâŸ¶SetCounter

To the following:

SetCounterClass:CounterStateâŸ¶SetCounterâŸ¶SetCounter

But passing it the state, and passing that to `fix` does indeed give us a SetCounter type, so our constructor returns the expected type.

### Using `this`

Letâ€™s continue the example from above by defining a new class of counter object, keeping count of the number of times `set` has been called. Weâ€™ll call this an â€œinstrumented counterâ€ `InstrCounter`, extending the `SetCounter` we defined above:

    
```

    InstrCounter = {
        get: Unit -> Nat,
        set: Nat  -> Unit,
        inc: Unit -> Unit
        accesses: Unit -> Nat
    };
    
    IntrCounterState = {
        x: Ref Nat,
        a: Ref Nat
    };
    
    instrCounterClass = Î»r: InstrCounterState. Î»this: InstrCounter.
        let super = setCounterClass r this in {
            get = super.get,
            set = Î»i: Nat. (
                r.a := succ(!(r.a));
                super.set i
            ),
            inc = super.inc,
            accesses = Î»_: Unit. !(r.a)
        };
    
    newInstrCounter = Î»_: Unit. 
        let r = {x = ref 1, a = ref 0} in fix (instrCounterClass r);
    
```


A few notes about this implementation:

*   The methods use `this` (passed as a parameter) and `super` (constructed using `this` and the state variable `r`)
*   Because we allow for open recursion, the `inc` in `super` calls the `set` defined here, which calls the `super.set`

But this implementation is not very useful, as the object creator diverges! Intuitively, the problem is that the â€œunprotectedâ€ use of `this`. The argument that we pass to `fix` uses its own argument (`this`) too early; in general, to create fixed points abstractions that donâ€™t diverge, the assumption is that one should only use the argument in â€œprotectedâ€ locations, such as in the bodies of inner lambda abstractions.

A solution is to â€œdelayâ€ this by putting a dummy abstraction in front of it:

    
```

    setCounterClass = Î»r: CounterState. 
        Î»this: Unit -> SetCounter.
            Î»_: Unit. {
                get = Î»_: Unit. !(r.x),
                set = Î»i: Nat.  r.x := i,
                int = Î»_: Unit. this.set (succ (this.get unit))
            };
```

    

This essentially replaces call-by-value with call-by-name. Now, `this` is of type Unitâ†’SetCounter.

This works, but very slowly. All the delaying we added has a side effect. Instead of computing the method table just once, we now re-compute it every time we invoke a method. Indeed, every time we need it, since weâ€™re in call-by-name, we re-compute it every time.

The solution here is to use lazy values, which we can represent in lambda calculus as a reference, along with a flag about whether weâ€™ve computed it or not. Section 18.12 describes this in more detail.

Featherweight Java
------------------

Weâ€™ve now covered the essence of objects, but there are still certain things missing compared to Java. With objects, weâ€™ve captured the runtime aspect of classes, but we havenâ€™t really talked about the classes as types.

Weâ€™re also missing a discussion on:

*   Named types with declared subtyping (weâ€™ve only done structural subtyping)
*   Recursive types, like the ones we need for list tails, for instance
*   Run-time type analysis: most type systems have escape hatches known as casts, which we havenâ€™t talked about
*   Many other things

Seeing that we have plenty to talk about, letâ€™s try to define a model for Java. A model always abstracts details away, so thereâ€™s no such thing as a perfect model. Itâ€™s always a question of which trade-offs we choose for our specific use-case.

Java is used for a lot of different purposes, so we are going to have lots of different models. For instance, some of the choices we need to make are:

*   Source-level vs. bytecode level
*   Large (inclusive) vs small (simple) models
*   Type system vs. run-time
*   Models of specific features

Featherweight Java was proposed as a tool for analyzing GJ (Java with generics), and has since been used to study proposed Java extensions. It aims to be very simple, modeling just the core OO features and their types, _and nothing else_. It models:

*   Classes and objects,
*   Method and method invocation,
*   Fields and field access,
*   Inheritance (including open recursion through `this`)
*   Casting

It leaves out more complex topics such as reflection, concurrency, exceptions, loops, assignment (!) and overloading.

The model aims to be very explicit, and simple. To maintain this simplicity, it imposes some conventions:

*   Every class must declare a superclass
*   All classes must have a constructor
*   All fields must be represented 1-to-1 in the constructor
    *   It takes the same number of parameters of fields of the class
    *   It assigns constructor parameters to local fields
    *   Calls `super` constructor to assign remaining fields
    *   Nothing else!
*   The constructor must call `super()`
*   Always explicitly name receiver object in method invocation or field access (using `this.x` or `that.x`)
*   Methods are just a single `return` expression

### Structural vs. Nominal type systems

Thereâ€™s a big dichotomy in the world of programming languages.

On one hand, we have _structural_ type systems, where the names are convenient but inessential abbreviations of types. What really matters about a type in a structural type system is its structure. Itâ€™s somewhat cleaner and more elegant, easier to extend, but once we need to talk about recursive types, some of the elegance falls away. Examples include Haskell, Go and TypeScript.

On the other hand, whatâ€™s used in almost all mainstream programming languages is _nominal_ type systems. Here, recursive types are much simpler, and using names everywhere makes type checking much simpler. Having named types is also useful at run-time for casting, type testing, reflection, etc. Examples include Java, C++, C# and Kotlin.

### Representing objects

How can we represent an object? What defines it? Two objects are different if their constructors are different, or if their constructors have been passed different arguments. This observation leads us to the idea that we can identify an object fully by looking at the `new` expression. Here, having omitted assignments makes our life much easier.

### Syntax

The syntax of Featherweight Java is:

t::\=	terms
x	variable
t.f	field access
t.m(Â¯t)	method invocation
newÂ C(Â¯t)	object creation
(C)t	cast

v::\=	values
newÂ C(Â¯v)	object creation

K::\=	constructor declarations
C(Â¯CÂ Â¯f)Â {super(Â¯f);Â this.Â¯f\=Â¯f;}

M::\=	method declarations
CÂ m(Â¯CÂ Â¯x){returnÂ t;}

CL::\=	class declarations
classÂ CÂ extendsÂ CÂ {Â¯CÂ Â¯f;Â KÂ Â¯M}

Above and in the following, we use the following metavariables:

*   A, B, C, D, E for class names
*   f, g for field names
*   x for parameter names
*   s, t for terms
*   u, v for values
*   K for constructor declarations
*   M for method declarations

Weâ€™ll use the notation Â¯C to mean arbitrary repetition C1,â€¦,Cn of C, and similarly for Â¯f,Â¯x,Â¯t, etc. For method declaration, Â¯M means M1â€¦Mn (no commas).

The notation Â¯CÂ Â¯f means weâ€™ve â€œzippedâ€ the two together: C1Â f1,â€¦,CnÂ fn.

Similarly, this.Â¯f\=Â¯f means this.f1\=f1;â€¦;this.fn\=fn

### Subtyping

Java is a nominal type system, so subtyping in FJ is _declared_. This means that in addition to two properties for reflexion and transitivity, the subtyping relationship is given by the declared superclass.

C<:C
\frac{C<:D	D<:E}{C<:E}
\frac{CT(C)\=classÂ CÂ extendsÂ D{â€¦}}{C<:D}

We assume to have a _class table_ CT, mapping class names to their definition.

### Auxiliary definitions

We can glean a lot of useful properties from the definition in the class table, so itâ€™ll come in handy for evaluation and typing.

We said earlier that every class needed to define a superclass, but what about `Object`? The simplest way to deal with this is to let `Object` be an exception, a distinguished class name whose definition does not appear in the class table.

The fields of a class C, written fields(C) is the sequence Â¯CÂ Â¯f mapping the class to each field. The fields of a class are those defined within it, and those it inherits from the superclasses:

fields(Object)\=âˆ…
\frac{CT(C)\=classÂ CÂ extendsÂ D{Â¯CÂ Â¯f;KÂ Â¯M},fields(D)\=Â¯DÂ¯g}{fields(C)\=Â¯DÂ Â¯g,Â¯CÂ Â¯f}

The type of a method m, written mtype(m,C) is a pair Â¯Bâ†’B mapping argument types Â¯B to a result type B by searching up the chain of superclasses until we find the definition of the method m:

\frac{CT(C)\=classÂ CÂ extendsÂ DÂ {Â¯CÂ Â¯f;KÂ Â¯M},BÂ m(Â¯BÂ Â¯x)Â {returnÂ t;}âˆˆÂ¯M}{mtype(m,C)\=Â¯Bâ†’B}
\frac{CT(C)\=classÂ CÂ extendsÂ DÂ {Â¯CÂ Â¯f;KÂ Â¯M},mÂ is not defined inÂ Â¯M}{mtype(m,C)\=mtype(m,D)}

Method body lookups work in basically the same way. It returns a pair (Â¯x,t) of parameters Â¯x and a term t by searching up the chain of superclasses:

\frac{CT(C)\=classÂ CÂ extendsÂ DÂ {Â¯CÂ Â¯f;KÂ Â¯M},BÂ m(Â¯BÂ Â¯x)Â {returnÂ t;}âˆˆÂ¯M}{mbody(m,C)\=(Â¯x,t)}
\frac{CT(C)\=classÂ CÂ extendsÂ DÂ {Â¯CÂ Â¯f;KÂ Â¯M},mÂ is not defined inÂ Â¯M}{mbody(m,C)\=mbody(m,D)}

Featherweight Java also models overriding, so we can define a predicate function override(m,D,Â¯Câ†’C0) that checks whether the method m with argument types Â¯C and result type C0 is overridden in a subclass of D:

\frac{mtype(m,D)\=Â¯Dâ†’D0âŸ¹Â¯C\=Â¯Dâˆ§C0\=D0}{override(m,D,Â¯Câ†’C0)}

### Evaluation

FJ has three computation rules for field access, method invocation and casting.

\frac{fields(C)\=Â¯CÂ Â¯f}{(newÂ C(Â¯v)).fiâŸ¶vi}	(E-ProjNew)
\frac{mbody(m,C)\=(Â¯x,t0)}{(newÂ C(Â¯v)).m(Â¯u)âŸ¶\[Â¯xâ†¦Â¯u,thisâ†¦newÂ C(Â¯v)\]t0}	(E-InvkNew)
\frac{C<:D}{(D)Â (newÂ C(Â¯v))âŸ¶newÂ C(Â¯v)}		(E-CastNew)

It also has a bunch of congruence rules:

\frac{t0âŸ¶tâ€²0}{t0.fâŸ¶tâ€²0.f}		(E-Field)
\frac{t0âŸ¶tâ€²0}{t0.m(Â¯t)âŸ¶tâ€²0.m(Â¯t)}	(E-Invk-Recv)
\frac{tiâŸ¶tâ€²i}{v0.m(Â¯v,ti,Â¯t)âŸ¶v0.m(Â¯v,tâ€²i,Â¯t)}	(E-Invk-Arg)
\frac{tiâŸ¶tâ€²i}{newÂ C(Â¯v,ti,Â¯t)âŸ¶newÂ C(Â¯v,tâ€²i,Â¯t)}	(E-New-Arg)
\frac{t0âŸ¶tâ€²0}{(C)Â t0âŸ¶(C)Â tâ€²0}		(E-Cast)

As [E-InvkNew](https://kjaer.io/fos/#mjx-eqn-eq%3Afj-e-invknew) and [E-Invk-Arg](https://kjaer.io/fos/#mjx-eqn-eq%3Afj-e-invk-arg) show, it uses call-by-value evaluation order.

### Typing

\frac{x:CâˆˆÎ“}{Î“âŠ¢x:C}	(T-Var)
\frac{Î“âŠ¢t0:C0fields(C0)\=Â¯CÂ Â¯f}{Î“âŠ¢t0.fi:Ci}	(T-Field)
\frac{Î“âŠ¢t0:C0mtype(m,C0)\=Â¯Dâ†’CÎ“âŠ¢Â¯t:Â¯CÂ¯C<:Â¯D}{Î“âŠ¢t0.m(Â¯t):C}	(T-Invk)
\frac{fields(C)\=Â¯DÂ Â¯fÎ“âŠ¢Â¯t:Â¯CÂ¯C<:Â¯D}{Î“âŠ¢newÂ C(Â¯t):C}	(T-New)
\frac{Î“âŠ¢t0:DD<:C}{Î“âŠ¢(C)Â t0:C}	(T-UCast)
\frac{Î“âŠ¢t0:DC<:DCâ‰ D}{Î“âŠ¢(C)Â t0:C}	(T-DCast)
(Â¯x:Â¯C)âˆª(this:C)âŠ¢t0:E0	E0<:C0	
\frac{CT(C)\=classÂ CÂ extendsÂ DÂ {â€¦}override(m,D,Â¯Câ†’C0)}{C0Â mÂ (Â¯CÂ Â¯x)Â {returnÂ t0;}Â OK inÂ C}
K\=C(Â¯DÂ Â¯g,Â¯CÂ Â¯f)Â {super(Â¯g);this.Â¯f\=Â¯f;}	(M OK in C)
\frac{fields(D)\=Â¯DÂ Â¯gÂ¯MÂ OK inÂ C}{classÂ CÂ extendsÂ DÂ {Â¯CÂ Â¯f;KÂ Â¯M}Â OK}	(C OK)

[T-Var](https://kjaer.io/fos/#mjx-eqn-eq%3Afj-t-var) is as usual. [T-Field](https://kjaer.io/fos/#mjx-eqn-eq%3Afj-t-field) says that we can type-check the ith field by looking up the type of the ith field in the class.

We have two rules for casting: one for subtypes ([T-DCast](https://kjaer.io/fos/#mjx-eqn-eq%3Afj-t-dcast)), and one for supertypes ([T-UCast](https://kjaer.io/fos/#mjx-eqn-eq%3Afj-t-ucast)). We do not allow casting to an unrelated type, because FJ complies with Java, and Java doesnâ€™t allow it.

For methods and classes, we want to make sure that overrides are valid, that we pass the correct arguments to the superclass constructor.

Also note that the our typing rules often have subsumption built into them (e.g. see [T-Invk](https://kjaer.io/fos/#mjx-eqn-eq%3Afj-t-invk)), instead of having a separate subsumption rule. This allows us to have algorithmic subtyping, which we need for two reasons:

1.  To perform static overloading resolution (picking between different overloaded methods at compile-time), we need to be able to speak about the type of an expression (and we need one single type, not several of them)
2.  Weâ€™d run into trouble typing conditional expressions. This is not something that we have included in FJ, but regular Java has it, and we may wish to include it as an extension to FJ

Letâ€™s talk about this problem with conditionals (aka ternary expressions) in a little more detail. If we have a conditional t1?Â t2:t3, with t1:Bool, t2:T2 and t3:T3, what is the return type of the expression? The simple solution is the least common supertype (this corresponds to the lowest common ancestor), but that becomes problematic with interfaces, which allow for multiple inheritance (for instance, if T2 and T3 both implement I2 and I3, we wouldnâ€™t know which one to pick).

The actual Java rule thatâ€™s used is that the return type is min(T2,T3). Scala solves this (in Dotty) with union types, where the result type is T2âˆ£T3.

### Evaluation context

We canâ€™t actually prove progress, as well-typed programs can get stuck because of casting. Casting can fail, and weâ€™d get stuck. The solution is to weaken the statement of progress:

**FJ progress** (_informally_): a well-typed FJ term is either value, reduces to one, or gets stuck at a cast failure.

To formalize this, we need a little more work. Weâ€™ll first need to introduce **evaluation contexts**. For FJ, the evaluation context is defined as:

E::\=		evaluation contexts
\[\]		hole
E.f		field access
E.m(Â¯t)		method invocation (rcv)
v.m(Â¯v,E,Â¯t)	method invocation (arg)
newÂ C(Â¯v,E,Â¯t)	object creation (arg)
(C)Â E		cast

All expressions in E are recursive, except for \[\]; this means an expression is a nested composition of the above forms, with a hole somewhere inside it. We write E\[t\] for the term obtained by replacing the hole in E with t.

Evaluation contexts are essentially just shorthand notation to avoid the verbosity of congruence rules. Usually, congruence rules just â€œforwardâ€ the computation to some part of the expression, and thatâ€™s exactly what we capture with evaluation contexts: the position of \[\] tells us which part of the expression to evaluate.

Having defined the execution context, we can then express all congruence rules as a single rule:

\frac{tâŸ¶tâ€²}{E\[t\]âŸ¶E\[tâ€²\]}

### Properties

#### Progress

We can now restate progress more formally.

**FJ progress**: Suppose t is a closed, well-typed normal form. Then either:

1.  t is a value
2.  tâŸ¶tâ€² for some tâ€²
3.  For some evaluation context E, we can express t as t\=E\[(C)Â (newÂ D(Â¯v))\], with Â¬(D<:C)

#### Preservation

The preservation theorem can be stated as:

**Preservation**: If Î“âŠ¢t:C and tâŸ¶tâ€² then Î“âŠ¢tâ€²:Câ€² for some Câ€²<:C

But this doesnâ€™t actually for FJ. Because we allow casts to go up and down, we can upcast to Object before downcasting to another, unrelated type. Because FJ must model Java, we need to actually introduce a rule for this. In this new rule, we give a â€œstupid warningâ€ to indicate that the _implementation_ should generate a warning if this rule is used:

\Î“âŠ¢t0:D	Â¬(C<:D)Â¬(D<:C)
frac{stupid warning}{Î“âŠ¢(C)t0:C}		(T-SCast)

#### Correspondence with Java

FJ corresponds to Java; by this, we mean:

1.  Every syntactically well-formed FJ program is also a syntactically well-formed Java program.
2.  A syntactically well-formed FJ program is typable in FJ (without using [T-SCast](https://kjaer.io/fos/#mjx-eqn-eq%3Afj-t-scast)) âŸº it is typable in Java
3.  A well-typed FJ program behaves the same in FJ as in Java (e.g. diverges in FJ âŸº it diverges in Java)

Without a formalization of full Java, we cannot _prove_ this, but itâ€™s still useful to say what weâ€™re trying to accomplish, as it provides us with a rigorous way of judging potential counterexamples.

Foundations of Scala
--------------------

### Modeling lists

If weâ€™d like to apply everything weâ€™ve learned so far to model Scala, weâ€™ll run into problems fairly quickly. Say weâ€™d like to model a `List`.

    
```
    trait List[+T] {
        def isEmpty: Boolean
        def head: T
        def tail: List[T]
    }
    
    def Const[T](hd: T, tl: List[T]) = new List[T] {
        def isEmpty = false
        def head = hd
        def tail = tl
    }
    
    def Nil[T] = new List[T] {
        def isEmpty = true
        def head = ???
        def tail = ???
    } 
```

Immediately, we run into these problems:

*   Itâ€™s parameterized
*   Itâ€™s recursive
*   It can be invariant or covariant

To solve the parametrization, we need a way to express type constructors. Traditionally, the solution is to express this as _higher-kinded types_.

    
```
    *            // Kind of normal types (Boolean, Int, ...)
    * -> *       // Kind of unary type constructor: 
                 // something that takes a type, returns one
    * -> * -> *  // and so on...
    ...
```

    

Weâ€™ve previously had abstraction and application for terms, but weâ€™d now like to extends this to types. Weâ€™ll introduce Î¼, which works like Î» but for types.

This also leads us to solving the problem of modeling recursive types, as we can now create type-level functions, called _type operators_. For instance, we can define a constructor for recursive types Î¼t.T(t). For instance:

    
```

    mu ListInt. { head: Int, tail: ListInt }
```

    

However, we get into some tricky questions when it comes to equality and subtyping. For instance, in the following, how do `T` and `Int -> T` relate?

    
```
    type T = mu t. Int -> Int -> t
```
    

Finally, we need to model the covariance of lists. We can deal with variance by expressing definition site variance as use-site variance, using Java wildcards:

    
```

    // We can go from definition site variance...
    trait List[+T] { ... }
    trait Function1[-T, +U] { ... }
    
    List[C]
    Function1[D, E]
    
    // ... to use-site variance by rewriting with Java wildcards:
    trait List[T] { ... }
    trait Function1[T, U]
    
    List[_ <: C]
    Function1[_ >: D, _ <: E]
```
    

Here, we should understand `Function1[_ >: D, _ <: E]` as the type of functions from some (unknown) supertpye of `D` to some (unknown) subtype of `E`. How can we model this? One possibility is existential types:

```
    
    // Scala:
    Function1[X, Y] forSome {
        type X >: D
        type Y <: E
    }
    
    // more traditional notation, with existential types:
    âˆƒ X >: D, Y <: E. Function1[X, Y]
```
    

But this gets messy rather quickly. Can we find a nicer way of expressing this? As we saw above, Scala has type members, so we can re-formulate `List` as follows:

    
```

    trait List { self => 
        type T
        def isEmpty: Boolean
        def head: T
        def tail: List { type T <: self.T } // refinement handling co-variance
    }
    
    def Cons[X](hd: X, tl: List { type T <: X }) = new List {
        type T = X
        def isEmpty = false
        def head = hd
        def tail = tl
    }
    
    // analogous for Nil
```

    

This offers an alternative way to express the above, without using existential types, but instead using:

*   Variables, functions
*   Abstract types `type T <: B`
*   Refinements `List { ... }`
*   Path-dependent types `self.T`

### Abstract types

Abstract types are types without a concrete implementation. They may have an upper and/or lower bound, like `type L >: T <: U`, or no bounds like below:

    
```

    // Trait containing an abstract type:
    trait KeyGen {
        type Key
        def key(s: String): this.Key
    }
    
    // Implementation refining the abstract type:
    object HashKeyGen extends KeyGen {
        type Key = Int
        def key(s: String) = s.hashCode
    }
    
```

We can reference the `Key` type of a term `k` as `k.Key`. This is a _path-dependent_ type. For instance:

```

    def mapKeys(k: KeyGen, ss: List[String]): List[k.Key] = 
        ss.map(s => k.key(s))
```
    

The function `mapKeys` has a _dependent function type_. This is an interesting type, because the result type has an internal dependency: `(k: KeyGen, ss: List[String]) -> List[k.Key]`.

In Scala 2, we canâ€™t express this directly; weâ€™d have to go through a trait with an apply method, meaning that we have to define a type for every dependent function:

    
```

    trait KeyFun {
        def apply(k: KeyGen, ss: List[String]): List[k.Key]
    }
    
    mapKeys = new KeyFun {
        def apply(k: KeyGen, ss: List[String]): List[k.Key] = 
            ss.map(s => k.key(s))
    }
    
```


However, Scala 3 (dotty) [introduces these dependent function types](http://dotty.epfl.ch/docs/reference/new-types/dependent-function-types.html) at the language level; itâ€™s done with a similar trick to what we just saw.

In dotty, the intention was to have everything map to a simple object type; this has been formalized in a calculus called DOT, (path-)Dependent Object Types.

### DOT

The DOT syntax is described in the [DOT paper](http://lampwww.epfl.ch/~amin/dot/fool.pdf).

| Symbol | Representation | Description |
|--------|----------------|-------------|
| **Types** | | |
| `S, T, U` | `::=` | `T \| âŠ¥ \| {a : T} \| {A : S..T} \| x.A \| S âˆ§ T \| Î¼(x : T) \| âˆ€(x : S)T` |
| | `T` | top type |
| | `âŠ¥` | bottom type |
| | `{a : T}` | field declaration |
| | `{A : S..T}` | type declaration |
| | `x.A` | type projection |
| | `S âˆ§ T` | intersection type |
| | `Î¼(x : T)` | recursive type |
| | `âˆ€(x : S)T` | dependent function |
| **Values** | | |
| `v` | `::=` | `Î½(x : T)d \| Î»(x : T)t` |
| | `Î½(x : T)d` | object |
| | `Î»(x : T)t` | lambda |
| **Terms** | | |
| `s, t, u` | `::=` | `x \| v \| x.a \| x y \| let x = t in u` |
| | `x` | variable |
| | `v` | value |
| | `x.a` | selection |
| | `x y` | application |
| | `let x = t in u` | let |
| **Definitions** | | |
| `d` | `::=` | `{a = t} \| {A = T} \| d1 âˆ§ d2` |
| | `{a = t}` | field definition |
| | `{A = T}` | type definition |
| | `d1 âˆ§ d2` | aggregate definition |
We use the following metavariables:

*   x, y, z for variables
*   a, b, c for term members
*   A, B, C for type members

Types are in uppercase, terms in lowercase. Note that recursive types Î¼(x:T) are a little different from what weâ€™ve talked about, but weâ€™ll get to that later.

As a small technicality, DOT imposes the restriction of only allowing member selection and application on variables, and not on values or full terms. This is equivalent, because we could just assign the value to a variable before selection or application. This way of writing programs is also called _administrative normal form_ (ANF).

To simplify things, we can introduce a programmer-friendly notation with ASCII versions of DOT constructs:

    
```
    (x: T) => U          for   Î»(x : T)U
    (x: T) -> U          for   âˆ€(x : T)U
    new(x: T)d           or
    new { x: T => d }    for   Î½(x : T)d
    rec(x: T)            or
    { x => T }           for   Î¼(x: T)
    T & U                for   T âˆ§ U
    Any                  for   âŠ¤
    Nothing              for   âŠ¥
    { type A >: S <: T } for   {A: S..T}
    { def a = t }        for   {a = t}
    { type A = T }       for   {A = T}
```

    

This calculus does not have generic types, because we can encode them as dependent function types.

#### Example 1: Twice

Letâ€™s take a look at an example. The polymorphic type of the `twice` method (which we [defined previously](https://kjaer.io/fos/#lambda-calculus)) is:

âˆ€X.Â (Xâ†’X)â†’Xâ†’X

In other words, it takes a function from X to X, an argument of type X, and returns a value of type X, where X is some generic type. This is represented as:

    
```

    (cX: {A: Nothing..Any}) -> (cX.A -> cX.A) -> cX.A -> cX.A
```
    

The `cX` parameter is a kind of cell containing a type variance X (hence the name `cX`).

#### Example 2: Church booleans

Letâ€™s see how Church Booleans could be implemented:

    
```
 
    // Define an abstract "if type" IFT
    type IFT = { if: (x: {A: Nothing..Any}) -> x.A -> x.A -> x.A }
    
    let boolimpl =
        let boolImpl =
            new(b: { Boolean: IFT..IFT } &
                { true: IFT } &
                { false: IFT })
            { Boolean = IFT } &
            { true = { if = (x: {A: Nothing..Any}) => (t: x.A) => (f: x.A) => t } &
            { false = { if = (x: {A: Nothing..Any}) => (t: x.A) => (f: x.A) => f }
    in ...
```

    

We can hide the implementation details of this with a small wrapper to which we apply `boolImpl`.

    
```

    
    let bool =
        let boolWrapper =
            (x: rec(b: {Boolean: Nothing..IFT} &
                       {true: b.Boolean} &
                       {false: b.Boolean})) => x
        in boolWrapper boolImpl
```
    

This is all a little long-winded, so we can introduce some abbreviations:

    
```

    // Abstract types:
    type A                   for {A: Nothing..Any}
    type A = T               for {A: T..T}
    type A >: S              for {A: S..Any}
    type A <: U              for {A: Nothing..U}
    type A >: S <: U         for {A: S..U}
    
    // Intersections:
    { type A = T; a = t }    for {A = T} & {a = t}
    { type A <: T; a = T }   for {A: Nothing..T} & {a: T}
    
    // Ascription:
    t: T
    // Which expands to:
    ((x: T) => x) t
    // Which expands to:
    let y = (x: T) => x in
        let z = t in 
            y z
    
    // Object definition:
    new { x => d }           for new (x: T)d
    
```

With these in place, we can give an abbreviated definition:

    
```

    let bool =
        new { b =>
            type Boolean = {if: (x: { type A }) -> (t: x.A) -> (f: x.A) -> x.A}
            true = {if: (x: { type A }) => (t: x.A) => (f: x.A) => t}
            false = {if: (x: { type A }) => (t: x.A) => (f: x.A) => f}
        }: { b => type Boolean; true: b.Boolean; false: b.Boolean }
    
```


#### Example 3: Lists

Weâ€™ve now introduced all the concepts we need to actually define the covariant list in DOT. Weâ€™d like to model the following Scala code in DOT:

    
```

    
    package scala.collection.immutable
    
    trait List[+A] {
        def isEmpty: Boolean
        def head: A
        def tail: List[A]
    }
    
    object List{
        def nil: List[Nothing] = new List[Nothing] {
            def isEmpty = true
            def head = head // infinite loop
            def tail = tail // infinite loop
        }
    
        def cons[A](hd: A, tl: List[A]) = new List[A] {
            def isEmpty = false
            def head = hd
            def tail = tl
        }
    }
    
```

We can write this in DOT as:

    
```

    let scala_collection_immutable_impl = new { sci =>
        type List = { thisList =>
            type A
            isEmpty: bool.Boolean
            head: thisList.A
            tail: sci.List & {type A <: thisList.A }
        }
    
        cons = (x: {type A}) => (hd: x.A) =>
            (tl: sci.List & { type A <: x.A }) =>
                let l = new {
                    type A = x.A
                    isEmpty = bool.false
                    head = hd
                    tail = tl 
                } in l
    
        nil = (x: {type A}) =>
            let l = new { l =>
                type A = x.A
                isEmpty = bool.true
                head = l.head
                tail = l.tail
            } in l
    }
    
```

To hide the implementation, we can wrap `scala_collection_immutable_impl`:

    
```

    
    let scala_collection_immutable = scala_collection.immutable_impl: { sci =>
        type List <: { thisList =>
            type A
            isEmpty: bool.Boolean
            head: thisList.A
            tail: sci.List & {type A <: thisList.A }
        }
    
        nil: sci.List & { type A = Nothing }
    
        cons: (x: {type A}) ->
              (hd: x.A) ->
              (tl: sci.List & { type A <: x.A }) ->
              sci.List & { type A = x.A }
    }
    
```

This concept of hiding the implementation gives us _nominality_. A nominal type such as `List` is simply an abstract type with a hidden implementation. This shows that nominal and structural types arenâ€™t completely separated; we can do nominal types within a structural setting if we have these constructs.

### Evaluation

Evaluation is interesting, because weâ€™d like for it to keep terms in ANF.

First, to define the congruence rules, letâ€™s define an evaluation context E:

E::\=
\[\]
letÂ x\=\[\]Â inÂ t
letÂ x\=vÂ inÂ E

The rules are then:

\frac{tâŸ¶tâ€²}{E\[t\]âŸ¶E\[tâ€²\]}
frac{v\=Î»(z:T)t}{letÂ x\=vÂ inÂ E\[xÂ y\]âŸ¶letÂ x\=vÂ inÂ E\[\[zâ†¦y\]t\]}
frac{v\=Î½(z:T)...{a\=t}}{letÂ x\=vÂ inÂ E\[x.a\]âŸ¶letÂ x\=vÂ inÂ E\[t\]}
letÂ x\=yÂ inÂ tâŸ¶\[xâ†¦y\]t
letÂ x\=(letÂ y\=sÂ inÂ t)Â inÂ uâŸ¶letÂ y\=sÂ inÂ letÂ x\=tÂ inÂ u

### Type assignment and subtyping

These are in the slides and in the DOT paper.

### Abstract types

Abstract types turn out to be both the most interesting and most difficult part of this, so letâ€™s take a quick look at it before we go on.

Abstract types can be used to encode type parameters (as in `List`), hide information (as in `KeyGen`), and also to resolve some puzzlers like this one:

```

    
    trait Food
    trait Grass extends Food
    
    trait Animal {
        def eat(food: Food): Unit
    }
    
    trait Cow extends Animal with Food {
        // error: does not override Animal.eat because of contravariance
        def eat(food: Grass): Unit
    }
    
    trait Lion extends Animal {
        // error: does not override Animal.eat because of contravariance
        def eat(food: Cow): Unit
    }
    
```


Scala disallows this, but Eiffel, Dart and TypeScript and allow it. The trade-off that the latter languages choose is modeling power over soundness, though some languages have eventually come back around and tried to fix this (Dart has a strict mode, Eiffel proposed some data flow analysis, â€¦).

In Scala, this contravariance problem can be solved with abstract types:

```

    trait Animal {
        type Diet <: Food
        def eat(food: Diet): Unit
    }
    
    trait Cow extends Animal {
        type Diet <: Grass
        def eat(food: this.Diet): Unit
    }
    
    object Milka extends Cow {
        type Diet = AlpineGrass
        def eat(food: AlpineGrass): Unit
    }
    
```

### Progress and preservation

Progress is actually wrong. Hereâ€™s a counter example:

```
    t = let x = (y: Bool) => y in x
    
```


But we can extend our definition of progress. Instead of values, weâ€™ll just want to get answers, which we define as variables, values or let-bindings.

But this is difficult (and itâ€™s what took 8 years to prove), because we always need an inversion, and the subtyping relation is user-definable. This is not a problem for simple type bounds:

```

    type T >: S <: U
```
    

But it becomes complex for non-sensical bounds:

```

    type T >: Any <: Nothing
```
    

By transitivity, it would mean that `Any <: Nothing`, so by transitivity all types are subtypes of each other. This is bad because it means that inversion fails, as we cannot tell anything from the types anymore.

We might say that this should be easy to disallow in the compiler, but it isnâ€™t. The compiler cannot always tell.

    
```

    // S and T are both good:
    type S = { type A; type B >: A <: Bot }
    type T = { type A >: Top <: B; type B }
    
    // But their intersection is bad
    type S & T == { type A >: Top <: Bot; type B >: Top <: Bot }
    
```


Bad bounds can arise from intersecting types with good bounds. This isnâ€™t too bad in and of itself, as we could just check all intersection types, written or inferred, for these bad bounds. But thereâ€™s a final problem: bad bounds can arise at run-time. By preservation, if Î“âŠ¢t:T and tâŸ¶u then Î“âŠ¢u:T. Because of subsumption, u may also have a type S which is a true subtype of T, and that type S could have bad bounds (from an intersection for instance).

To solve this, the idea is to reason about environments Î“ arising from an actual computation in the preservation rule. This environment corresponds to an evaluated `let` binding, binding variables to values. Values are guaranteed to have good bounds because all type members are aliases.

In other words, the `let` prefix acts like a store, a set of bindings x\=v of variables to values. Evaluation will then relate terms _and_ stores:

sâˆ£tâŸ¶sâ€²âˆ£tâ€²

For the theorems of proofs and preservation, we need to relate environment and store. Weâ€™ll introduce a definition:

> An environment Î“ _corresponds_ to a store s, written Î“âˆ¼s if for every binding x\=v there is an entry Î“âŠ¢x:T where Î“âŠ¢!v:T.

Here âŠ¢! denotes an exact typing relation, whose typing derivation ends with `All-I` or `{}-I` (so no subsumption or structural rules).

By restating our theorems as follows, we can then prove them.

*   **Preservation**: If Î“âŠ¢t:T and Gâˆ¼s and sâˆ£tâŸ¶sâ€²âˆ£tâ€² then there exists an environment Î“â€²âŠ‚Î“ such that Î“â€²âŠ¢tâ€²:T and Î“â€²âˆ¼sâ€².
*   **Progress**: if Î“âŠ¢t:T and Î“âˆ¼s then either t is a normal form, or sâˆ£tâŸ¶sâ€²âˆ£tâ€² for some store sâ€² and term tâ€².