Sorting. We discussed sorting methods: a special purpose method that may be used as a convenient preprocessing step (bucket sort); a variety of classical methods ranging from pretty bad (bubble sort) to decent (insertion sort); and two more advanced methods (merge sort and quick sort). Next time we will discuss a final sorting method that does not have any of the disadvantages of the other ones but that is a bit less "intuitive": heap sort.

In the examples in these notes we use a list $L$ of integers to be sorted that is different from the one used in class (but generated in a comparable manner):

$688,106,15,880,661,410,464,808,225,798,518,397,613,214,942,239$

Bucket sort. The first sorting method below (bucket sort) exploits properties of the representation of the items in $L$ and may be combined as a (possibly repeated) preprocessing step with any of the other methods; all other methods that we discussed are generic in the sense that they rely only on the comparison of items, but not on anything that would involve their actual representation.

Assume that there is an integer number $k>1$ of distinct "easy to distinguish" properties among the items in $L$ depending on their representation: for instance, there may be $k=26$ first characters for lexicographic sorting; or $k=10$ different leading digits - but be careful that this may require leading zeros if integers of different sizes are sorted in this way. Given $k$ such easily recognizable "properties", it may make sense to use bucket sort, either exclusively or as preprocessing step to split up the problem of sorting $L$ into $k$ easier subproblems (the combined solutions of which should then lead to the solution of the overall sorting problem).

For ease of exposition suppose that $L$ contains $n$ items, but that all items can also be identified as belonging to some (other) set of size $k$, say the ordered set of "buckets" $\left\{s_{1}, s_{2}, \ldots, s_{k}\right\}$ with $s_{i}<s_{i+1}$ for $1 \leq i<k$. If $n>k$ it follows that there must be duplicates (i.e., distinct items in $L$ identified as belonging to the same $s_{j}$ ), which we must be able to account for; but also if $n \leq k$ there may be duplicates. To sort $L$, initialize $k$ distinct bucket counters $c_{i}$ as zero for $i=1,2, \ldots, k$, and then successively inspect all items in $L$ : if the item under inspection "belongs to" $s_{i}$, then increase $c_{i}$ by one. (During the lecture we put the actual items in the buckets: it all depends on how the result of bucket sort is going to be used.)

At the end of the process, i.e., once all items on $L$ have been inspected and accounted for in the applicable bucket, the sorted list consists of $c_{1}$ items belonging to $s_{1}$, followed by $c_{2}$ items belonging to $s_{2}$, followed by $c_{3}$ items belonging to $s_{3}$, $\ldots$, finally followed by $c_{k}$ items belonging to $s_{k}$ (where, obviously, buckets $s_{j}$ for which $c_{j}$ is equal to zero can be left out). Note that at the end we must have that $\sum_{i=1}^{k} c_{i}=n$. This process takes effort linear in $k$ to initialize the counters, effort linear in $n$ to process the items of $L$, and effort linear in $k$ again to produce the "sorted" result (within the same bucket distinct items may not be sorted yet). Overall the effort is linear in $\max (k, n)$ up to some non-zero multiplicative factor: bucket sort runs in time $O(\max (k, n))$ (and, more precisely, in $\Theta(\max (k, n)))$.

Bubble sort. This is a classical, generic sorting method (and a pretty poor one too for the reasons given below). Let $\ell_{1}, \ell_{2}, \ldots, \ell_{n}$ be the original list in its initial (and most likely unsorted) order. Bubble sort considers all pairs $\ell_{i}, \ell_{i+1}$ going from $i=1$ to $i=2, \ldots$, up to $i=n-1$, swapping ${ }^{1}$ them if they are "out of order" (i.e., if $\ell_{i}>\ell_{i+1}$ ). Once this has been done (at the cost of $n-1$ comparisons and at most $n-1$ swaps, thus "cost" $n-1$ ) the largest item will be at the last place (i.e., $\ell_{n}$ is now the largest item), but the rest, i.e., $\ell_{1}, \ell_{2}, \ldots, \ell_{n-1}$ may still be unsorted. So, the process is repeated for the list $\ell_{1}, \ell_{2}, \ldots, \ell_{n-1}$ (at cost $\left.n-2\right)$, then for the list $\ell_{1}, \ell_{2}, \ldots, \ell_{n-2}$ (at cost $n-3$ ), etc, until it is repeated for the last time for the list $\ell_{1}, \ell_{2}$ (at cost 1), for an overall cost of $\sum_{i=1}^{n-1}(n-i)=O\left(n^{2}\right)$ comparisons and at most the same number of swaps. Note that in bubble sort the sorted list appears in a natural way from right to left (cf. example below).

Though intuitively appealing, bubble sort is inefficient because items are swapped around a lot compared to other methods: during each of the (at most) $n-1$ main iterations larger items are moved as far to the right as possible until they "hit" a larger item, whereas smaller items may move left at most one position per iteration. The method has the (small) advantage, though, that it may notice that during a certain iteration no swaps are made and thus conclude that the list is already sorted. So it runs in time $O\left(n^{2}\right)$ and $\Omega(n)$ : a tight $\Theta$ estimate cannot be given.

When sorting

$688,106,15,880,661,410,464,808,225,798,518,397,613,214,942,239$

using bubble sort the following sequence of lists is produced (with the already sorted part underlined). The example does not show all swaps per main iteration: make sure you understand which swaps are taking place (such as $(688,106),(688,15)$, $(688,880),(880,661),(880,410), \ldots,(880,942),(942,239)$ during the first main iteration (applied to the original list) that leads to the first subresult where only the largest item is at the right place). Note that smaller items (slowly) drift to the left and larger ones (often faster) to the right. And note the final short-cut, because no more swaps were made after 397 reached its destination (i.e., got underlined), so bubble sort concluded that the entire list is sorted.

Selection sort. A better version of bubble sort avoids the huge number of swaps and is referred to as "selection sort": locate the smallest item among $\ell_{1}, \ell_{2}, \ldots, \ell_{n}$ (cost always $n-1$ comparison: not more and not less) and swap (if needed) the result with $\ell_{1}$ (at most a single swap), then locate the smallest item among $\ell_{2}, \ell_{3}, \ldots, \ell_{n}$ (cost precisely $n-2$ comparisons) and swap (if needed) the result with $\ell_{2}$ (at most a single swap), then locate the smallest item among $\ell_{3}, \ell_{4}, \ldots, \ell_{n}$ (cost precisely $n-3$ comparisons) and swap (if needed) the result with $\ell_{3}$ (at most a single swap), etc, for a total cost of precisely $\sum_{i=1}^{n-1}(n-i)$ (thus $\left.\Theta\left(n^{2}\right)\right)$ comparisons and at most $n-1$ swaps. Note that in selection sort the sorted list appears from left to right (cf. example below) if during each main iteration we locate the smallest item. We could have opted for the largest item instead; make sure you understand that it works in the same way, but that it builds the sorted list from right to left. Overall, selection sort works in time $\Theta\left(n^{2}\right)$.

The smallest item in the part of the list that still has to be sorted is indicated in red if it requires a swap and in blue if no swap is required. The $n-\ell$ comparisons required to find the smallest item in the $\ell$-th iteration are not shown.

Insertion sort. This may be the most intuitively appealing of the classical sorting methods, because it is quite similar to the way (many) people proceed when sorting stacks of paper. One begins by observing that, by itself, the sublist consisting of just the first item $\ell_{1}$ is sorted, because it consists of just one item; thus we have that the list looks like $\ell_{1} \| \ell_{2}, \ell_{3}, \ldots, \ell_{n}$ with everything left of the "||" already sorted, and everything right of the "||" not yet inspected and most likely not sorted yet. We now iteratively for $i=2,3, \ldots, n$ take the item $\ell_{i}$ just right of the "||", remove it from the "right part" and insert it at the correct place in the already sorted "left part", thereby increasing the size of the left part from $i-1$ to $i$ and decreasing the size of the right part from $n-i+1$ to $n-i$ : the place where $\ell_{i}$ should be inserted in the left part to keep the left part sorted is easy to find (requiring about $\log _{2}(i)$ comparisons using binary search, as discussed during the previous lecture) because the left part is sorted already. Given the place where to insert, then doing the actual insertion may require shifting (at most $i-1$ items) one position to the right. For
humans sorting stacks of papers, the actual insertion is just a single operation, but inserting an element in an array on a computer is more cumbersome and the effort per iteration is dominated by the potential shift of the already sorted sequence of $i-1$ array entries. This leads to an overall shifting effort $\leq \sum_{i=2}^{n}(i-1)=\frac{(n-1) n}{2}$, i.e., $O\left(n^{2}\right)$. If the shifts would be "for free" (which they are not for computers, but which may be the case if you're manually sorting things), the effort would be $\leq \sum_{i=2}^{n} \log _{2}(i)=\log _{2}(n!)$ (because $n!\leq n^{n}$ this is $O(n \log (n))$ ) for the binary searches. The method runs in time $O\left(n^{2}\right)$ and $\Omega(n \log (n))$; a tight $\Theta$ estimate cannot be given (to see this: use insertion sort to sort an already sorted list).

Note the similarity and difference between selection and insertion sort: they both keep extending a sorted list with a next item, with selection sort spending time locating the next item as the smallest of an unsorted list (and then trivially appending it), but insertion sort spending no time selecting the next item (it just picks it as the first of the unsorted part), then taking advantage of the already sorted part to find where it should go, and ending up spending time shifting other entries to make place for the item to be inserted.

Faster methods. All sorting methods seen so far consider the global implications of partial results. The faster methods that were discussed in the remainder of the lecture extend local properties by combining them until a global result is obtained. At least, that is one way to see it...

Merge sort. When using merge sort to sort a list of $n$ items one optimistically regards the initial list as $n$ sorted sublists each consisting of a single item, and proceeds by systematically reducing the number of sorted sublists by pairing them,
thus doubling their length while making sure that each longer sublist remains sorted.

To analyse the overall effort for general $n$, note that at the $k$-th lowest level merge-step pairs of sorted $2^{k-1}$-item sublists are merged into sorted $2^{k}$-item sublists, at a total cost of fewer than $n$ comparisons for all pairs combined. Because the largest $k$ is the minimal $k \in \mathbf{Z}$ for which $2^{k}>n$, it follows that about $\log _{2}(n)$ levels of merge-steps suffice: with a cost per level that is bounded by $n$, the overall cost is bounded by $n \log _{2}(n)$. Because this is both an upper bound and a lower bound (why?), we find that merge sort runs in time $\Theta(n \log (n))$.

Merge sort, more traditional "divide and conquer" description. Solving a problem using recursion consists of identifying a way to split the problem into a number of similar smaller subproblems in such a way that solutions to the subproblems ("at the next level") can be combined into a solution to the original problem, and by making sure that the smallest possible subproblems (at the "deepest" or lowest level) can easily be solved directly. Using the same observation "recursively" to solve the similar smaller subproblems - unless they are small enough to be solved directly - then results in a solution to the original problem that keeps going deeper and deeper into the recursion by continually splitting problems into smaller subproblems, until the deepest level is reached ("the bottom of the recursion") where splitting is no longer necessary (or possible) because solutions to the subproblems are easily found. At that point the lowest level solutions are combined into solutions to higher level subproblems until ultimately the highest, top level is reached again and the solution to the original problem is found.

The merge sort example above is a somewhat unorthodox non-recursive description of merge sort. It shows what actually happens at the deepest recursion level of merge sort (namely, "merging" of single-item sublists into 2 -item sublists), working its way back up again to larger and larger sublists (at higher and higher levels) that have to be merged. More traditionally merge sort is defined recursively in the following manner (and as suggested by the paragraph above): to merge sort a list of $n$ items, don't do anything if $n \leq 1$ (because the list is already sorted; this is called the bottom of the recursion), otherwise split the list as evenly as possible into a "left" and a "right" sublist (the split does not require any actual work), use merge sort (recursively calling it, twice, namely to the left sublist and to the right sublist) to sort these smaller sublists, and merge the sorted sublists (resulting from the two recursive calls to merge sort) to obtain the overall sorted list. Note that the actual work is performed during the merging.

Thus, accounting just for the lengths of the lists, and denoting by $T(n)$ the cost of merge sort to sort a list of $n$ items, $T(n)=T(n / 2)+T(n / 2)+n=2 T(n / 2)+n$ where $n$ is an estimate for the cost of merging the two sorted sublists of length (about) $n / 2$ (that result from the two recursive calls to merge sort to sort two lists of about $n / 2$ items). It follows (by substituting $n / 2$ for $n$ in $T(n)=2 T(n / 2)+n)$ that $T(n / 2)=$ $2 T(n / 4)+n / 2$ and thus that $T(n)=4 T(n / 4)+2 n$, so that (now substituting $n / 4$ for $n)$ with $T(n / 4)=2 T(n / 8)+n / 4$ it follows that $T(n)=8 T(n / 8)+3 n$, etc., leading more in general to $T(n)=2^{k} T\left(n / 2^{k}\right)+k n$. The minimal $k \in \mathbf{Z}$ for which $n / 2^{k} \leq 1$ leads to $T\left(n / 2^{k}\right)=0$ (because a list of at most one item does not require any work) and thus, for that $k$, it follows that $T(n)=k n$. Because $k$ is approximately $\log _{2}(n)$, we find again that $T(n)=n \log _{2}(n)$.

Although merge sort is much faster than the earlier methods and mostly oblivious of anything with the exception of the number of items to be sorted, arrayimplementation requires additional memory to store the merged array(s).

Quick sort. Like merge sort, quick sort is a recursive sorting method. Where merge sort spends no effort splitting the problem into subproblems and does the actual work in the combination (i.e., the merging) of the subproblem solutions, quick sort uses a diametrically opposite approach: quick sort spends all its effort splitting the problem into subproblems but does not require any work in the combination of the subproblem solutions.

Summarizing, quick sort works by picking a random "separator" item from the list (for which we picked the first item on the list; thus, if the initial list is $\ell_{1}, \ell_{2}, \ldots, \ell_{n}$, the item $\ell_{1}$ would be the separator), puts all items that are at most $\ell_{1}$ in the left part $L_{\text {left }}$ of $L$, and everything larger than $\ell_{1}$ in the right part $L_{\text {right }}$ of $L$. This takes a total of at most $n-1$ comparisons and $n-1$ swaps. The overall sorted list is going to look like "first $L_{\text {left }}$, then $\ell_{1}$, and then $L_{\text {right }}$ " (because everything in $L_{\text {left }}$ is at most $\ell_{1}$ and $\ell_{1}$ is smaller than everything in $L_{\text {right }}$ ), except that we still
need to sort $L_{\text {left }}$ and $L_{\text {right }}$. If we are lucky, $L_{\text {left }}$ and $L_{\text {right }}$ both have a nicely balanced size of about $\frac{n-1}{2}$ items. If we are unlucky, one of them is empty, and the other one contains $n-1$ items. In any case, all non-empty sublists $L_{\text {left }}$ and $L_{\text {right }}$ need to be sorted, which is done in the same way: pick a random item from the first sublist (its first item) and partition the sublist in a left part, separator, and right part (to which, yet again, the same method is applied), and do the same for the second sublist. The overall number of comparisons and swaps will be about $n \log _{2}(n)$ at best (i.e., if splitting into two sublists always results in two sublists of about the same size) and about $\frac{(n-1) n}{2}$ at worst (i.e., if it is always the case that one of the new sublists is empty):

- (best case of quick sort) Denoting by $S(n)$ the cost of quick sort to sort a list of $n$ items, $S(n)=n-1+S(k)+S(n-1-k)$, for some $k$ with $0 \leq k \leq n-1$ and where the $n-1$ term estimates the cost to use the separator to build the two sublists as described above. Optimistically assuming that $k \approx n / 2$, it follows that $S(n) \approx n+2 S(n / 2)$. With the same optimistic assumption also made recursively at all levels, it follows that $S(n / 2) \approx n / 2+2 S(n / 4)$, that $S(n / 4) \approx n / 4+2 S(n / 8)$, etc. (constantly assuming the "best case" where the separator always splits its sublist right in the middle), from which it follows as above (analysis of merge sort) that $S(n) \approx n \log _{2}(n)$.
- (worst case of quick sort) With the same notation, but now assuming that at all levels of the recursion the separator never achieves a non-trivial split (i.e., one sublist is empty, the other has just one fewer item (namely the separator itself) than the list where it came from) it is found that $S(n)=n-1+S(n-$ 1), that $S(n-1)=n-2+S(n-2)$ (thus $S(n)=n-1+n-2+S(n-2))$, that $S(n-2)=n-3+S(n-3)$ (thus $S(n)=n-1+n-2+n-3+S(n-3))$, etc., resulting in (with $S(1)=0) S(n)=\sum_{i=1}^{n-1}(n-i)=\frac{(n-1) n}{2}$ (remember: the sum of an arithmetic sequence is the number of terms (here $n-1$ ) times the sum of the first term (here $n-1$ ) and the last term (here 1 ) divided by two). Note that the worst case can easily be enforced by using quick sort to sort an already sorted list, while always using a sublist's first item as the separator.

It follows that in the worst case quick sort is as bad as our earlier "quadratic" sorting methods (such as insertion sort), but that in the best case it works as fast as merge sort. Because in practice one stops the recursion anyhow as soon as the sublist is short enough (and one uses a simple quadratic method to sort the short sublist) and the chance of nasty behavior of quick sort decreases with longer lists, quick sort works quite well in practice and is often preferred to merge sort. The latter, however, has the advantage that it is guaranteed that unpleasant surprises will not occur. For "manual" sorting (of piles of paper, for instance), merge sort works great (assuming that small piles are done using some more elementary method such as insertion sort).

