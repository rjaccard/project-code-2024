{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.external_database import ExternalDatabase\n",
    "from models.model_rag import RAG\n",
    "from datasets import Features, Value, Sequence\n",
    "\n",
    "# Autoreload \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\affol\\anaconda3\\envs\\Projects\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizerFast'.\n",
      "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-multiset-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42fe3cc63875403ba4f79845c4cf2d43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d43cf7a03664b62b56b47a6ff955d10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ext_db = ExternalDatabase()\n",
    "\n",
    "# ============= Create a dataset =============\n",
    "# Create a dataset from a csv file\n",
    "dataset = ext_db.create_dataset(\"documents/my_dataset.csv\")\n",
    "\n",
    "# Split the documents into chunks\n",
    "dataset = dataset.map(ext_db.split_documents, batched=True, num_proc=None)\n",
    "\n",
    "# Compute the embeddings of the chunks\n",
    "new_features = Features(\n",
    "    {\"text\": Value(\"string\"), \"title\": Value(\"string\"), \"embeddings\": Sequence(Value(\"float32\"))}\n",
    ")  \n",
    "dataset = dataset.map(ext_db.compute_embeddings, batched=True, batch_size=16, features=new_features)\n",
    "\n",
    "# Save the dataset\n",
    "ext_db.save_dataset(dataset, \"datasets/rag/rag_dataset\")\n",
    "\n",
    "# ============= Create the FAISS index =============\n",
    "dataset = ext_db.index_dataset(dataset, \n",
    "                               path_index='datasets/rag/rag_dataset_index.faiss', \n",
    "                               embedding_dim=768, \n",
    "                               nb_links=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Predictions of the model for the samples from the evaluation_set will be saved under the path specified by the predictions_path parameter. If this path already exists, the script will use saved predictions to calculate metrics. Add --recalculate parameter to force the script to perform inference from scratch. An example e2e evaluation run could look as follows:',\n",
       " 'a pizza']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_name_or_path = \"facebook/rag-sequence-base\"\n",
    "question_encoder_name_or_path =\"facebook/dpr-question_encoder-single-nq-base\" \n",
    "generator_name_or_path = \"google/flan-t5-small\"\n",
    "\n",
    "rag_save_dir = \"checkpoints/rag_model/\"\n",
    "pretrained_model = rag_save_dir\n",
    "path_dataset = \"datasets/rag/rag_dataset\"\n",
    "path_index = \"datasets/rag/rag_dataset_index.faiss\"\n",
    "\n",
    "kwargs = {\n",
    "    \"rag_name_or_path\": rag_name_or_path,\n",
    "    \"question_encoder_name_or_path\": question_encoder_name_or_path,\n",
    "    \"generator_name_or_path\": generator_name_or_path,\n",
    "    \"dataset_path\": path_dataset,\n",
    "    \"index_path\": path_index,\n",
    "    \"rag_save_dir\": rag_save_dir\n",
    "}\n",
    "\n",
    "rag = RAG(pretrained_model, **kwargs)\n",
    "\n",
    "input_questions = [\"What is Linear Regression ?\", \"What is the recipe of pizza ?\"]\n",
    "rag.prediction_step(input_questions, max_new_tokens=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rag_name_or_path': 'facebook/rag-token-nq', 'generator_name_or_path': 'google/flan-t5-small', 'question_encoder_name_or_path': 'facebook/dpr-question_encoder-single-nq-base', 'dataset_path': 'datasets/rag/rag_dataset', 'index_path': 'datasets/rag/rag_dataset_index.faiss', 'rag_save_dir': 'checkpoints/rag_model/'}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'question': 'What is Linear Regression ?',\n",
       "  'choices': ['A', 'B', 'C', 'D'],\n",
       "  'answer': 'Blablabla',\n",
       "  'preds': 'a'},\n",
       " {'question': 'What is FFT ?',\n",
       "  'choices': ['A', 'B', 'C', 'D'],\n",
       "  'answer': 'Blablabla',\n",
       "  'preds': 'a'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.model_dpo import AutoDPOModelForSeq2SeqLM\n",
    "from transformers import AutoTokenizer\n",
    "import yaml \n",
    "\n",
    "# Parameters for evaluation\n",
    "main_config = {}\n",
    "with open(\"main_config.yaml\") as f:\n",
    "    try:\n",
    "        main_config = yaml.safe_load(f)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading main_config.yaml: {e}! Please check the file format.\")\n",
    "\n",
    "model_class = AutoDPOModelForSeq2SeqLM\n",
    "rag_policy_model_path = main_config[\"rag_policy_model_path\"]\n",
    "rag_model_args = main_config.get(\"rag_model_args\", {})\n",
    "print(rag_model_args, end=\"\\n\\n\")\n",
    "\n",
    "# Load model \n",
    "model = AutoDPOModelForSeq2SeqLM.from_pretrained(rag_policy_model_path, \n",
    "                                                 **rag_model_args)\n",
    "tokenizer = AutoTokenizer.from_pretrained(rag_policy_model_path)\n",
    "\n",
    "# Test generation\n",
    "input_questions = [{\n",
    "        \"question\": \"What is Linear Regression ?\",\n",
    "        \"choices\": [\"A\", \"B\", \"C\", \"D\"],\n",
    "        \"answer\": \"Blablabla\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is FFT ?\",\n",
    "        \"choices\": [\"A\", \"B\", \"C\", \"D\"],\n",
    "        \"answer\": \"Blablabla\",\n",
    "    }\n",
    "]\n",
    "\n",
    "model.prediction_step_mcqa(input_questions, tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
