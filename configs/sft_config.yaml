model:
  model_name: "google/flan-t5-large"
  model_name_dev: "google/flan-t5-small"
  learning_rate: 3.0e-4
  max_length: 512
  num_epochs: 2
  seed: 42
  per_device_eval_batch: 4
  weight_decay: 0.01
  peft:
    rank: 64
    lora_alpha: 64
    lora_dropout: 0.05
    learning_rate: 1.0e-4
    num_epochs: 2


data:
  dataset_name: "meta-math/MetaMathQA"
  
wandb:
  project_name: "flan-t5-sft"
